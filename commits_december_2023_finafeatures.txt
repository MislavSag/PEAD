
+ebed8568e4334db95d9badd2016f26a7d19c7c3a unknown Tue Nov 21 23:13:15 2023 +0100 add ica
+diff --git a/image.def b/image.def
+index 9033da5..03e6f96 100644
+--- a/image.def
++++ b/image.def
+@@ -34,6 +34,7 @@ From: r-base:4.3.0
+   R --slave -e 'install.packages("dbarts")'
+   R --slave -e 'install.packages("nnet")'
+   R --slave -e 'install.packages("praznik")'
++  R --slave -e 'install.packages("fastICA")'
+   R --slave -e 'install.packages("remotes")'
+   R --slave -e 'remotes::install_github("mlr-org/mlr3extralearners")'
+   R --slave -e 'install.packages("lightgbm")'
+
+12942634f3044fa018e57f26a1ba0117000f158c unknown Tue Nov 21 22:13:56 2023 +0100 fix nnet, fix glmnet and set test period to 1
+diff --git a/results_light.R b/results_light.R
+index 7bcf52c..4718ee3 100644
+--- a/results_light.R
++++ b/results_light.R
+@@ -138,7 +138,6 @@ unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c(
+ unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c("task", "cv")]
+ 
+ # accuracy by ids
+-pr = PortfolioRet$new()
+ measures = function(t, res) {
+   list(acc   = mlr3measures::acc(t, res),
+        fbeta = mlr3measures::fbeta(t, res, positive = "1"),
+@@ -182,7 +181,6 @@ predictions_ensemble[, (cols_sign_response_neg) := lapply(sign_response_seq, fun
+ 
+ predictions_ensemble[median_response > 0 & sd_response < 0.15, .(tr = truth_sign, res = 1)][, sum(tr == res) / length(tr)]
+ 
+-
+ # check only sign ensamble performance
+ res = lapply(cols_sign_response_pos[1:5], function(x) {
+   print(x)
+diff --git a/run_job.R b/run_job.R
+index f58b330..4d1c2ce 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -121,16 +121,16 @@ ids_done = findDone(reg=reg)
+ # if (nrow(ids_done) == 0) {
+ #
+ # }
+-# resources = list(ncpus = 4) # this shouldnt be important
+-# jc = makeJobCollection(ids = NULL,
+-#                        resources = resources,
+-#                        reg = reg)
+-
+ resources = list(ncpus = 4) # this shouldnt be important
+-jc = makeJobCollection(ids = ids_not_done,
++jc = makeJobCollection(ids = NULL,
+                        resources = resources,
+                        reg = reg)
+ 
++# resources = list(ncpus = 4) # this shouldnt be important
++# jc = makeJobCollection(ids = ids_not_done,
++#                        resources = resources,
++#                        reg = reg)
++
+ 
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+
+f1da42a171148219c7213da96e102bbcf836a079 unknown Tue Nov 21 22:12:38 2023 +0100 fix nnet, fix glmnet and set test period to 1
+diff --git a/run_job.R b/run_job.R
+index 213d1a0..f58b330 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -112,7 +112,7 @@ reg = loadRegistry("experiments")
+ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+ # i = 1663L
+ 
+-# extract not  done ids
++# extract not done ids
+ ids_not_done = findNotDone(reg=reg)
+ ids_done = findDone(reg=reg)
+ (nrow(ids_not_done) + nrow(ids_done)) == 8866
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index bdf93e2..05de47b 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -250,8 +250,9 @@ nested_cv_split_week = function(task,
+ }
+ 
+ custom_cvs = list()
+-custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 96, 13, 5, 1, 1)
+-custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 144, 13, 5, 1, 1)
++# custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 96, 13, 1, 1, 1)
++# custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 144, 13, 1, 1, 1)
++custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 200, 15, 1, 1, 1)
+ 
+ 
+ # ADD PIPELINES -----------------------------------------------------------
+@@ -293,7 +294,8 @@ mlr_measures$add("portfolio_ret", PortfolioRet)
+ # graph template
+ gr = gunion(list(
+   po("nop", id = "nop_union_pca"),
+-  po("pca", center = FALSE, rank. = 100)
++  po("pca", center = FALSE, rank. = 100),
++  po("ica", n.comp = 100)
+ )) %>>% po("featureunion")
+ graph_template =
+   po("subsample") %>>% # uncomment this for hyperparameter tuning
+@@ -527,12 +529,12 @@ search_space_kknn = ps(
+ 
+ # nnet graph
+ graph_nnet = graph_template %>>%
+-  po("learner", learner = lrn("regr.nnet", MaxNWts = 40000))
++  po("learner", learner = lrn("regr.nnet", MaxNWts = 50000))
+ graph_nnet = as_learner(graph_nnet)
+ as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
+ search_space_nnet = search_space_template$clone()
+ search_space_nnet$add(
+-  ps(regr.nnet.size  = p_int(lower = 5, upper = 30),
++  ps(regr.nnet.size  = p_int(lower = 2, upper = 25),
+      regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
+      regr.nnet.maxit = p_int(lower = 50, upper = 500))
+ )
+@@ -567,7 +569,7 @@ search_space_glmnet = ps(
+   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+   # learner
+   regr.glmnet.s     = p_int(lower = 5, upper = 30),
+-  regr.glmnet.alpha = p_dbl(lower = 1e-4, upper = 1e4, logscale = TRUE)
++  regr.glmnet.alpha = p_dbl(lower = 1e-4, upper = 1, logscale = TRUE)
+ )
+ 
+ 
+@@ -982,7 +984,7 @@ sh_file = sprintf("
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=10GB
++#PBS -l mem=12GB
+ #PBS -J 1-%d
+ #PBS -o experiments/logs
+ #PBS -j oe
+
+c2cd04d0483da5b6ee50a9815d48038cf9a372fd unknown Tue Nov 21 11:48:10 2023 +0100 bug fix
+diff --git a/run_job.R b/run_job.R
+index 40ea4fc..213d1a0 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -113,7 +113,7 @@ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+ # i = 1663L
+ 
+ # extract not  done ids
+-# ids_not_done = findNotDone(reg=reg)
++ids_not_done = findNotDone(reg=reg)
+ ids_done = findDone(reg=reg)
+ (nrow(ids_not_done) + nrow(ids_done)) == 8866
+ 
+
+3284e2a96f94a5e3a5c921d0bcf9e4c174e683c2 unknown Tue Nov 21 11:14:27 2023 +0100 increase ram and check onw more time run_job
+diff --git a/run_job.R b/run_job.R
+index 876a46c..40ea4fc 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -105,6 +105,7 @@ UpdateBuffer = R6Class(
+ 
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
++# reg = loadRegistry("F:/H4week")
+ reg = loadRegistry("experiments")
+ 
+ # extract integer
+@@ -113,13 +114,24 @@ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+ 
+ # extract not  done ids
+ # ids_not_done = findNotDone(reg=reg)
++ids_done = findDone(reg=reg)
++(nrow(ids_not_done) + nrow(ids_done)) == 8866
+ 
+ # create job collection
++# if (nrow(ids_done) == 0) {
++#
++# }
++# resources = list(ncpus = 4) # this shouldnt be important
++# jc = makeJobCollection(ids = NULL,
++#                        resources = resources,
++#                        reg = reg)
++
+ resources = list(ncpus = 4) # this shouldnt be important
+-jc = makeJobCollection(ids = NULL,
++jc = makeJobCollection(ids = ids_not_done,
+                        resources = resources,
+                        reg = reg)
+ 
++
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+diff --git a/run_ml.sh b/run_ml.sh
+index 0b76fdd..df7a1eb 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -2,8 +2,8 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=5GB
+-#PBS -J 1-2
++#PBS -l mem=15GB
++#PBS -J 1-434
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index 1bb5b38..bdf93e2 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -953,6 +953,7 @@ designs_l = lapply(custom_cvs, function(cv_) {
+     # TEST
+   })
+   designs_cv = do.call(rbind, designs_cv_l)
++  designs_cv
+ })
+ designs = do.call(rbind, designs_l)
+ 
+
+faf878c0225333dcf6e2d6f3ddf33401c553d9c0 unknown Sun Nov 19 14:12:36 2023 +0100 ready for new run
+diff --git a/run_job.R b/run_job.R
+index cfac950..876a46c 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -107,19 +107,19 @@ UpdateBuffer = R6Class(
+ # load registry
+ reg = loadRegistry("experiments")
+ 
++# extract integer
++i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
++# i = 1663L
++
+ # extract not  done ids
+-ids_not_done = findNotDone(reg=reg)
++# ids_not_done = findNotDone(reg=reg)
+ 
+ # create job collection
+ resources = list(ncpus = 4) # this shouldnt be important
+-jc = makeJobCollection(ids_not_done,
++jc = makeJobCollection(ids = NULL,
+                        resources = resources,
+                        reg = reg)
+ 
+-# extract integer
+-i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+-# i = 2L
+-
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index 2e9c64d..1bb5b38 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -414,8 +414,8 @@ search_space_xgboost = ps(
+     }
+   ),
+   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  winsorizesimple.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimple.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+   # scaling
+   scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
+   # filters
+@@ -495,8 +495,8 @@ search_space_kknn = ps(
+     }
+   ),
+   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  winsorizesimple.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimple.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+   # scaling
+   scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
+   # filters
+@@ -557,8 +557,8 @@ search_space_glmnet = ps(
+     }
+   ),
+   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  winsorizesimple.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimple.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+   # scaling
+   scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
+   # filters
+@@ -617,8 +617,8 @@ graph_template =
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+   po("fixfactors", id = "fixfactors") %>>%
+-  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+-  po("winsorizesimplegroup", group_var = "weekid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  # po("winsorizesimplegroup", group_var = "weekid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+   po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+   po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+   # scale branch
+@@ -661,8 +661,8 @@ search_space_template = ps(
+     }
+   ),
+   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  winsorizesimple.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimple.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+   # scaling
+   scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
+   # filters
+diff --git a/test.R b/test.R
+index 41d3959..ef350a5 100644
+--- a/test.R
++++ b/test.R
+@@ -106,10 +106,10 @@ UpdateBuffer = R6Class(
+ 
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
+-reg = loadRegistry("experiments")
++reg = loadRegistry("experimentstest")
+ 
+ # extract not  done ids
+-ids_not_done = findNotDone(reg=reg)
++# ids_not_done = findNotDone(reg=reg)
+ 
+ # create job collection
+ resources = list(ncpus = 4) # this shouldnt be important
+@@ -118,7 +118,7 @@ jc = makeJobCollection(ids_not_done,
+                        reg = reg)
+ 
+ # extract integer
+-i = 1663L
++i = 1L
+ 
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+@@ -133,9 +133,11 @@ cat("Get Job \n")
+ #   seed = 1 + jc$jobs[i]$job.id,
+ #   resources = jc$resources
+ # )
+-job = batchtools:::getJob(jc, i)
++job = batchtools:::getJob.JobCollection(jc, i)
+ id = job$id
+ 
++batchtools:::getSeed()
++
+ print(job)
+ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+ 
+
+5481fcfa8c56933b1e309b25641f740aebdc0526 unknown Sat Nov 18 15:37:51 2023 +0100 test script
+diff --git a/test.R b/test.R
+index 7e5dd2b..41d3959 100644
+--- a/test.R
++++ b/test.R
+@@ -118,7 +118,7 @@ jc = makeJobCollection(ids_not_done,
+                        reg = reg)
+ 
+ # extract integer
+-i = 1L
++i = 1663L
+ 
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+
+3f9f927d5d858fb1e014de1ee1e1251c7cae1fa3 unknown Fri Nov 17 15:13:33 2023 +0100 change to winsorize simple and increase cv sets
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-357444914953441.rds b/gausscov_f1/gausscov_f1-taskRetWeek-357444914953441.rds
+new file mode 100644
+index 0000000..81010de
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-357444914953441.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-388651697727097.rds b/gausscov_f1/gausscov_f1-taskRetWeek-388651697727097.rds
+new file mode 100644
+index 0000000..81da142
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-388651697727097.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-530662705677135.rds b/gausscov_f1/gausscov_f1-taskRetWeek-530662705677135.rds
+new file mode 100644
+index 0000000..ccfed86
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-530662705677135.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-619651364778988.rds b/gausscov_f1/gausscov_f1-taskRetWeek-619651364778988.rds
+new file mode 100644
+index 0000000..2f80d03
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-619651364778988.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-743340475724469.rds b/gausscov_f1/gausscov_f1-taskRetWeek-743340475724469.rds
+new file mode 100644
+index 0000000..b72f183
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-743340475724469.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-997078120701145.rds b/gausscov_f1/gausscov_f1-taskRetWeek-997078120701145.rds
+new file mode 100644
+index 0000000..f689f64
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-997078120701145.rds differ
+diff --git a/mlr3_winsorizationsimple.R b/mlr3_winsorizationsimple.R
+index a9ef8e0..d32311e 100644
+--- a/mlr3_winsorizationsimple.R
++++ b/mlr3_winsorizationsimple.R
+@@ -50,8 +50,8 @@ PipeOpWinsorizeSimple = R6::R6Class(
+     },
+ 
+     .transform_dt  = function(dt, levels) {
+-      dt = dt[, Map(function(a, b) ifelse(a < b, b, a), .SD, self$state$minvals)]
+-      dt = dt[, Map(function(a, b) ifelse(a > b, b, a), .SD, self$state$maxvals)]
++      dt = dt[, Map(function(a, b) fifelse(a < b, b, a), .SD, self$state$minvals)]
++      dt = dt[, Map(function(a, b) fifelse(a > b, b, a), .SD, self$state$maxvals)]
+       dt
+     }
+   )
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index 6740e53..2e9c64d 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -128,6 +128,15 @@ DT[, .(symbol, date, weekid, yearmonthid)]
+ print("This was the problem. Solved.")
+ 
+ 
++
++# SUMMARY STATISTICS ------------------------------------------------------
++#
++# nas = colSums(is.na(DT))
++# round(nas / nrow(DT) * 100, 2)
++# nas_sample = colSums(is.na(DT[1:3000]))
++# round(nas_sample / 3000 * 100, 2)
++
++
+ # TASKS -------------------------------------------------------------------
+ print("Tasks")
+ 
+@@ -240,11 +249,9 @@ nested_cv_split_week = function(task,
+   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+ }
+ 
+-# generate cv's
+ custom_cvs = list()
+-custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 48, 12, 1, 1, 1)
+-custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 96, 12, 1, 1, 1)
+-custom_cvs[[3]] = nested_cv_split_week(task_ret_week, 144, 12, 1, 1, 1)
++custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 96, 13, 5, 1, 1)
++custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 144, 13, 5, 1, 1)
+ 
+ 
+ # ADD PIPELINES -----------------------------------------------------------
+@@ -294,8 +301,8 @@ graph_template =
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+   po("fixfactors", id = "fixfactors") %>>%
+-  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+-  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  # po("winsorizesimplegroup", group_var = "weekid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+   po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+   po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+   # po("uniformization") %>>%
+@@ -349,8 +356,10 @@ search_space_template = ps(
+     }
+   ),
+   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  # winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  winsorizesimple.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimple.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+   # scaling
+   scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
+   # filters
+@@ -609,7 +618,7 @@ graph_template =
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+   po("fixfactors", id = "fixfactors") %>>%
+   # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+-  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimplegroup", group_var = "weekid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+   po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+   po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+   # scale branch
+@@ -907,6 +916,41 @@ designs_l = lapply(custom_cvs, function(cv_) {
+                       at_kknn, at_gbm, at_rsm, at_bart, at_catboost, at_glmnet),
+       resamplings = customo_
+     )
++
++    # TEST --------------------------------------------------------------------
++    # bmr = benchmark(design[1], store_models = FALSE)
++    #
++    # design$learner[[1]]
++    # pops = function(x, pop, ...) {
++    #   pos = po(pop, ...)
++    #   x = pos$train(list(x))[[1]]
++    #   cat("Rows ", x$nrow, "\nColumns ", x$ncol, "\n")
++    #   return(x)
++    # }
++    # x = task_ret_week$clone()
++    # x$filter(customo_$instance$train[[1]])
++    # x$nrow
++    # x$ncol
++    # x = pops(x, "subsample", frac = 0.3)
++    # x = pops(x, "dropnacol", cutoff = 0.05)
++    # x = pops(x, "dropna")
++    # x = pops(x, "removeconstants")
++    # x = pops(x, "fixfactors")
++    # # x = pops(x, "winsorizesimple", probs_low = 0.2, probs_high = 0.8, na.rm = TRUE)
++    # x = pops(x, "winsorizesimplegroup", group_var = "yearmonthid", probs_low = 0.1, probs_high = 0.9, na.rm = TRUE)
++    # any(is.na(x$data()))
++    # x = pops(x, "removeconstants")
++    # x = pops(x, "dropcorr", cutoff = 0.8)
++    # any(is.na(x$data()))
++    # x$data()[, 1:10]
++    # x$data(1:10)[, 1:10]
++    # x = pops(x, "uniformization")
++    # x = pops(x, "dropna")
++    # x$data()[, 1:6]
++    # x = pops(x, "filter", filter = flt("jmi"), filter.frac = 0.05)
++    # x = pops(x, "filter", filter = flt("relief"), filter.frac = 0.05)
++    # x = pops(x, "filter", filter = flt("gausscov_f1st"), filter.frac = 0.05)
++    # TEST
+   })
+   designs_cv = do.call(rbind, designs_cv_l)
+ })
+
+f04627c08baab850ee43c7fbc99db7c64ce6a97e unknown Fri Nov 17 08:57:41 2023 +0100 test script
+diff --git a/run_ml.sh b/run_ml.sh
+index 5849e99..0b76fdd 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -1,14 +1,13 @@
+-
+ #!/bin/bash
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+ #PBS -l mem=5GB
+-#PBS -J 1
++#PBS -J 1-2
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
+ 
+-# 1-2516
++
+diff --git a/test.R b/test.R
+index 60d3738..7e5dd2b 100644
+--- a/test.R
++++ b/test.R
+@@ -1,3 +1,4 @@
++# 1-2516
+ options(warn = -1)
+ library(data.table)
+ library(gausscov)
+
+d89bd2655e6d1fdd0c3394a767fb209f764a4f08 unknown Fri Nov 17 08:56:06 2023 +0100 test script
+diff --git a/run_job.R b/run_job.R
+index 91857cd..cfac950 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -135,7 +135,7 @@ cat("Get Job \n")
+ #   resources = jc$resources
+ # )
+ job = batchtools:::getJob(jc, i)
+-id =
++id = job$id
+ 
+ 
+ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+diff --git a/run_ml.sh b/run_ml.sh
+index 57bd989..5849e99 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -4,9 +4,11 @@
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+ #PBS -l mem=5GB
+-#PBS -J 1-2516
++#PBS -J 1
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
++
++# 1-2516
+
+c5ddab4ef9e3a39ee382aa57924f4495b5bc18ad unknown Fri Nov 17 08:35:42 2023 +0100 test script
+diff --git a/test.R b/test.R
+index 1212f0e..60d3738 100644
+--- a/test.R
++++ b/test.R
+@@ -142,4 +142,9 @@ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+ cat("Execute Job")
+ gc(reset = TRUE)
+ update$started = batchtools:::ustamp()
++
++print("Job values")
++print(job$pars)
++
++
+ # result = execJob(job)
+
+1e0ec68222e045c1c325b5f59b923bd3a1983484 unknown Thu Nov 16 17:29:54 2023 +0100 test script
+diff --git a/test.R b/test.R
+index 15f2145..1212f0e 100644
+--- a/test.R
++++ b/test.R
+@@ -142,4 +142,4 @@ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+ cat("Execute Job")
+ gc(reset = TRUE)
+ update$started = batchtools:::ustamp()
+-result = execJob(job)
++# result = execJob(job)
+
+975415c685e1144ae2c923ce3773611bb94d77bd unknown Thu Nov 16 17:29:28 2023 +0100 test script
+diff --git a/.gitignore b/.gitignore
+index 2b71018..a216673 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -12,3 +12,4 @@ experiment
+ predictions
+ experiments.zip
+ pead_experiment.zip
++experimentstest
+diff --git a/run_job.R b/run_job.R
+index b27fee7..91857cd 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -118,7 +118,7 @@ jc = makeJobCollection(ids_not_done,
+ 
+ # extract integer
+ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+-# i = 10L
++# i = 2L
+ 
+ # start buffer
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+@@ -126,16 +126,17 @@ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_ch
+ 
+ # get job
+ cat("Get Job \n")
+-job = batchtools:::Job$new(
+-  file.dir = jc$file.dir,
+-  reader = batchtools:::RDSReader$new(FALSE),
+-  id = jc$jobs[i]$job.id,
+-  job.pars = jc$jobs[i]$job.pars[[1L]],
+-  seed = 1 + jc$jobs[i]$job.id,
+-  resources = jc$resources
+-)
+-# job = batchtools:::getJob(jc, i)
+-id = job$id
++# job = batchtools:::Job$new(
++#   file.dir = jc$file.dir,
++#   reader = batchtools:::RDSReader$new(FALSE),
++#   id = jc$jobs[i]$job.id,
++#   job.pars = jc$jobs[i]$job.pars[[1L]],
++#   seed = 1 + jc$jobs[i]$job.id,
++#   resources = jc$resources
++# )
++job = batchtools:::getJob(jc, i)
++id =
++
+ 
+ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+ 
+@@ -145,7 +146,7 @@ gc(reset = TRUE)
+ update$started = batchtools:::ustamp()
+ result = execJob(job)
+ 
+-# save ojb
++# save job
+ writeRDS(result, file = getResultFiles(jc, id), compress = jc$compress)
+ 
+ # memory usage
+diff --git a/run_ml.sh b/run_ml.sh
+index 7f230eb..57bd989 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -10,4 +10,3 @@
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
+-
+diff --git a/test.R b/test.R
+index e502587..15f2145 100644
+--- a/test.R
++++ b/test.R
+@@ -1,63 +1,145 @@
++options(warn = -1)
++library(data.table)
++library(gausscov)
++library(paradox)
++library(mlr3)
++library(mlr3pipelines)
++library(mlr3viz)
++library(mlr3tuning)
++library(mlr3misc)
++library(future)
++library(future.apply)
++library(mlr3extralearners)
+ library(batchtools)
+ library(mlr3batchmark)
++library(checkmate)
++library(stringi)
++library(R6)
++library(brew)
+ 
++
++
++# UTILS -------------------------------------------------------------------
++# utils functions
++dir = function(reg, what) {
++  fs::path(fs::path_expand(reg$file.dir), what)
++}
++getResultFiles = function(reg, ids) {
++  fs::path(dir(reg, "results"), sprintf("%i.rds", if (is.atomic(ids)) ids else ids$job.id))
++}
++waitForFile = function(fn, timeout = 0, must.work = TRUE) {
++  if (timeout == 0 || fs::file_exists(fn))
++    return(TRUE)
++  "!DEBUG [waitForFile]: `fn` not found via 'file.exists()'"
++  timeout = timeout + Sys.time()
++  path = fs::path_dir(fn)
++  repeat {
++    Sys.sleep(0.5)
++    if (basename(fn) %chin% list.files(path, all.files = TRUE))
++      return(TRUE)
++    if (Sys.time() > timeout) {
++      if (must.work)
++        stopf("Timeout while waiting for file '%s'",
++              fn)
++      return(FALSE)
++    }
++  }
++}
++writeRDS = function (object, file, compress = "gzip") {
++  batchtools:::file_remove(file)
++  saveRDS(object, file = file, version = 2L, compress = compress)
++  waitForFile(file, 300)
++  invisible(TRUE)
++}
++UpdateBuffer = R6Class(
++  "UpdateBuffer",
++  cloneable = FALSE,
++  public = list(
++    updates = NULL,
++    next.update = NA_real_,
++    initialize = function(ids) {
++      self$updates = data.table(
++        job.id = ids,
++        started = NA_real_,
++        done = NA_real_,
++        error = NA_character_,
++        mem.used = NA_real_,
++        written = FALSE,
++        key = "job.id"
++      )
++      self$next.update = Sys.time() + runif(1L, 60, 300)
++    },
++
++    add = function(i, x) {
++      set(self$updates, i, names(x), x)
++    },
++
++    save = function(jc) {
++      i = self$updates[!is.na(started) & (!written), which = TRUE]
++      if (length(i) > 0L) {
++        first.id = self$updates$job.id[i[1L]]
++        writeRDS(
++          self$updates[i,!"written"],
++          file = fs::path(
++            jc$file.dir,
++            "updates",
++            sprintf("%s-%i.rds", jc$job.hash, first.id)
++          ),
++          compress = jc$compress
++        )
++        set(self$updates, i, "written", TRUE)
++      }
++    },
++
++    flush = function(jc) {
++      now = Sys.time()
++      if (now > self$next.update) {
++        self$save(jc)
++        self$next.update = now + runif(1L, 60, 300)
++      }
++    }
++
++  )
++)
++
++
++# RUN JOB -----------------------------------------------------------------
+ # load registry
+ reg = loadRegistry("experiments")
+ 
+-# get jobs
+-job_table = getJobTable(reg = reg)
+-print(job_table)
+-
+-
+-
+-# # featureless baseline
+-# lrn_baseline = lrn("classif.featureless", id = "featureless")
+-#
+-# # logistic regression pipeline
+-# lrn_lr = lrn("classif.log_reg")
+-# lrn_lr = as_learner(ppl("robustify", learner = lrn_lr) %>>% lrn_lr)
+-# lrn_lr$id = "logreg"
+-# lrn_lr$fallback = lrn_baseline
+-# lrn_lr$encapsulate = c(train = "try", predict = "try")
+-#
+-# # random forest pipeline
+-# lrn_rf = lrn("classif.ranger")
+-# lrn_rf = as_learner(ppl("robustify", learner = lrn_rf) %>>% lrn_rf)
+-# lrn_rf$id = "ranger"
+-# lrn_rf$fallback = lrn_baseline
+-# lrn_rf$encapsulate = c(train = "try", predict = "try")
+-#
+-# learners = list(lrn_lr, lrn_rf, lrn_baseline)
+-#
+-# library(mlr3oml)
+-#
+-# otask_collection = ocl(id = 99)
+-#
+-# binary_cc18 = list_oml_tasks(
+-#   limit = 6,
+-#   task_id = otask_collection$task_ids,
+-#   number_classes = 2
+-# )
+-#
+-# # load tasks as a list
+-# otasks = lapply(binary_cc18$task_id, otsk)
+-#
+-# # convert to mlr3 tasks and resamplings
+-# tasks = as_tasks(otasks)
+-# resamplings = as_resamplings(otasks)
+-#
+-# large_design = benchmark_grid(tasks, learners, resamplings,
+-#                               paired = TRUE)
+-#
+-# library(batchtools)
+-#
+-# # create registry
+-# reg = makeExperimentRegistry(
+-#   file.dir = "./experiments",
+-#   seed = 1,
+-#   packages = "mlr3verse"
++# extract not  done ids
++ids_not_done = findNotDone(reg=reg)
++
++# create job collection
++resources = list(ncpus = 4) # this shouldnt be important
++jc = makeJobCollection(ids_not_done,
++                       resources = resources,
++                       reg = reg)
++
++# extract integer
++i = 1L
++
++# start buffer
++buf = UpdateBuffer$new(jc$jobs$job.id)
++update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
++
++cat("Get Job \n")
++# job = batchtools:::Job$new(
++#   file.dir = jc$file.dir,
++#   reader = batchtools:::RDSReader$new(FALSE),
++#   id = jc$jobs[i]$job.id,
++#   job.pars = jc$jobs[i]$job.pars[[1L]],
++#   seed = 1 + jc$jobs[i]$job.id,
++#   resources = jc$resources
+ # )
+-# library(mlr3batchmark)
+-# batchmark(large_design, reg = reg)
+-#
+-# job_table = getJobTable(reg = reg)
++job = batchtools:::getJob(jc, i)
++id = job$id
++
++print(job)
++cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
++
++# execute job
++cat("Execute Job")
++gc(reset = TRUE)
++update$started = batchtools:::ustamp()
++result = execJob(job)
+
+4f209823e52af1b58664a5a2e94b9b94bf1b6c42 unknown Thu Nov 16 14:19:50 2023 +0100 bug fix
+diff --git a/run_job.R b/run_job.R
+index 22b6e17..b27fee7 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -125,13 +125,13 @@ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+ 
+ # get job
+-cat("Get Job")
++cat("Get Job \n")
+ job = batchtools:::Job$new(
+   file.dir = jc$file.dir,
+   reader = batchtools:::RDSReader$new(FALSE),
+   id = jc$jobs[i]$job.id,
+   job.pars = jc$jobs[i]$job.pars[[1L]],
+-  seed = 1 + jc$jobs[i],
++  seed = 1 + jc$jobs[i]$job.id,
+   resources = jc$resources
+ )
+ # job = batchtools:::getJob(jc, i)
+
+c972b947fba5c6558420b7313e31733513be9c4c unknown Thu Nov 16 12:55:14 2023 +0100 bug fix
+diff --git a/run_job.R b/run_job.R
+index 3d853be..22b6e17 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -128,7 +128,7 @@ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_ch
+ cat("Get Job")
+ job = batchtools:::Job$new(
+   file.dir = jc$file.dir,
+-  reader = reader,
++  reader = batchtools:::RDSReader$new(FALSE),
+   id = jc$jobs[i]$job.id,
+   job.pars = jc$jobs[i]$job.pars[[1L]],
+   seed = 1 + jc$jobs[i],
+
+47e1411b2b69fda8efdb655e63b8454c67187cb5 unknown Thu Nov 16 12:23:50 2023 +0100 bug fix
+diff --git a/run_job.R b/run_job.R
+index 8abe152..3d853be 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -125,18 +125,22 @@ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+ 
+ # get job
+-job = Job$new(file.dir = jc$file.dir,
+-              reader = reader,
+-              id = jc$jobs[i]$job.id,
+-              job.pars = jc$jobs[i]$job.pars[[1L]],
+-              seed = 1 + jc$jobs[i],
+-              resources = jc$resources)
++cat("Get Job")
++job = batchtools:::Job$new(
++  file.dir = jc$file.dir,
++  reader = reader,
++  id = jc$jobs[i]$job.id,
++  job.pars = jc$jobs[i]$job.pars[[1L]],
++  seed = 1 + jc$jobs[i],
++  resources = jc$resources
++)
+ # job = batchtools:::getJob(jc, i)
+ id = job$id
+ 
+ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+ 
+ # execute job
++cat("Execute Job")
+ gc(reset = TRUE)
+ update$started = batchtools:::ustamp()
+ result = execJob(job)
+
+f0bf6e48c38fbc3ecbd1a068a27cc196da180b0c unknown Thu Nov 16 11:56:54 2023 +0100 bug fix
+diff --git a/paper_trading.R b/paper_trading.R
+index c27b227..a7166f1 100644
+--- a/paper_trading.R
++++ b/paper_trading.R
+@@ -47,6 +47,9 @@ snakeToCamel <- function(snake_str) {
+   return(camel_case_str)
+ }
+ 
++# path to save output
++mlr3_save_path = "D:/paper"
++
+ 
+ # PREPARE DATA ------------------------------------------------------------
+ print("Prepare data")
+@@ -737,7 +740,7 @@ set_threads(graph_glmnet, n = threads)
+ 
+ # DESIGNS -----------------------------------------------------------------
+ # create designs
+-designs_l = lapply(custom_cvs, function(cv_) {
++lapply(custom_cvs, function(cv_) {
+   # debug
+   # cv_ = custom_cvs[[1]]
+ 
+@@ -905,13 +908,12 @@ designs_l = lapply(custom_cvs, function(cv_) {
+                     at_kknn, at_gbm, at_rsm, at_bart, at_catboost, at_glmnet),
+     resamplings = customo_
+   )
+-})
+-designs = do.call(rbind, designs_l)
+ 
+-# benchmark
+-bmr = benchmark(designs, store_models = FALSE)
++  # benchmark
++  bmr = benchmark(design, store_models = FALSE)
+ 
+-# save locally and to list
+-time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+-file_name = paste0("cv-", cv_$custom_inner$iters, "-", i, "-", time_, ".rds")
+-saveRDS(bmr, file.path(mlr3_save_path, file_name))
++  # save locally and to list
++  time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
++  file_name = paste0("paper-pead", cv_$custom_inner$iters, "-", time_, ".rds")
++  saveRDS(bmr, file.path(mlr3_save_path, file_name))
++})
+diff --git a/run_job.R b/run_job.R
+index f14be9a..8abe152 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -124,10 +124,6 @@ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+ 
+-
+-batchtools:::getSeed(jc$seed, row$job.id)
+-
+-
+ # get job
+ job = Job$new(file.dir = jc$file.dir,
+               reader = reader,
+
+9268a23e6961f7e25c8f0696e3233da7b110f791 unknown Thu Nov 16 10:03:23 2023 +0100 try to fix error with seeds
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-078122910888514.rds b/gausscov_f1/gausscov_f1-taskRetWeek-078122910888514.rds
+new file mode 100644
+index 0000000..5ed69d6
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-078122910888514.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-150223369419727.rds b/gausscov_f1/gausscov_f1-taskRetWeek-150223369419727.rds
+new file mode 100644
+index 0000000..f54b047
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-150223369419727.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-254348429132733.rds b/gausscov_f1/gausscov_f1-taskRetWeek-254348429132733.rds
+new file mode 100644
+index 0000000..4a05f77
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-254348429132733.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-382827425475386.rds b/gausscov_f1/gausscov_f1-taskRetWeek-382827425475386.rds
+new file mode 100644
+index 0000000..a9e683a
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-382827425475386.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-720203938907062.rds b/gausscov_f1/gausscov_f1-taskRetWeek-720203938907062.rds
+new file mode 100644
+index 0000000..b1499c6
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-720203938907062.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-829725293686005.rds b/gausscov_f1/gausscov_f1-taskRetWeek-829725293686005.rds
+new file mode 100644
+index 0000000..2546e00
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-829725293686005.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-836430097611408.rds b/gausscov_f1/gausscov_f1-taskRetWeek-836430097611408.rds
+new file mode 100644
+index 0000000..3f0660a
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-836430097611408.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-842355124427349.rds b/gausscov_f1/gausscov_f1-taskRetWeek-842355124427349.rds
+new file mode 100644
+index 0000000..d133398
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-842355124427349.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-925006275633950.rds b/gausscov_f1/gausscov_f1-taskRetWeek-925006275633950.rds
+new file mode 100644
+index 0000000..a2bfee3
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-925006275633950.rds differ
+diff --git a/gausscov_f1/gausscov_f1-taskRetWeek-975520658342413.rds b/gausscov_f1/gausscov_f1-taskRetWeek-975520658342413.rds
+new file mode 100644
+index 0000000..dd5de06
+Binary files /dev/null and b/gausscov_f1/gausscov_f1-taskRetWeek-975520658342413.rds differ
+diff --git a/paper_trading.R b/paper_trading.R
+index d229d2a..c27b227 100644
+--- a/paper_trading.R
++++ b/paper_trading.R
+@@ -11,6 +11,7 @@ library(future)
+ library(future.apply)
+ library(batchtools)
+ library(mlr3batchmark)
++library(AzureStor)
+ 
+ 
+ # SETUP -------------------------------------------------------------------
+@@ -19,6 +20,14 @@ monnb <- function(d) {
+   lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
+   lt$year*12 + lt$mon }
+ mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
++diff_in_weeks = function(d1, d2) difftime(d2, d1, units = "weeks") # weeks
++
++# weeknb <- function(d) {
++#   as.numeric(difftime(as.Date(d), as.Date("1900-01-01"), units = "weeks"))
++# }
++# weekdf <- function(d1, d2) {
++#   weeknb(d2) - weeknb(d1)
++# }
+ 
+ # snake to camel
+ snakeToCamel <- function(snake_str) {
+@@ -43,7 +52,7 @@ snakeToCamel <- function(snake_str) {
+ print("Prepare data")
+ 
+ # read predictors
+-data_tbl = fread("D:/features/pead-predictors-20231031.csv")
++data_tbl <- fread("D:/features/pead-predictors-20231031.csv")
+ 
+ # convert tibble to data.table
+ DT = as.data.table(data_tbl)
+@@ -51,9 +60,11 @@ DT = as.data.table(data_tbl)
+ # create group variable
+ DT[, date_rolling := as.IDate(date_rolling)]
+ DT[, yearmonthid := round(date_rolling, digits = "month")]
+-DT[, .(date, date_rolling, yearmonthid)]
++DT[, weekid := round(date_rolling, digits = "week")]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
+ DT[, yearmonthid := as.integer(yearmonthid)]
+-DT[, .(date, date_rolling, yearmonthid)]
++DT[, weekid := as.integer(weekid)]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
+ 
+ # remove industry and sector vars
+ DT[, `:=`(industry = NULL, sector = NULL)]
+@@ -62,7 +73,7 @@ DT[, `:=`(industry = NULL, sector = NULL)]
+ cols_non_features <- c("symbol", "date", "time", "right_time",
+                        "bmo_return", "amc_return",
+                        "open", "high", "low", "close", "volume", "returns",
+-                       "yearmonthid", "date_rolling"
++                       "yearmonthid", "weekid", "date_rolling"
+ )
+ targets <- c(colnames(DT)[grep("ret_excess", colnames(DT))])
+ cols_features <- setdiff(colnames(DT), c(cols_non_features, targets))
+@@ -94,20 +105,33 @@ factor_cols = as.matrix(factor_cols)[1, ]
+ factor_cols = factor_cols[factor_cols <= 100]
+ DT = DT[, (names(factor_cols)) := lapply(.SD, as.factor), .SD = names(factor_cols)]
+ 
++# remove observations with missing target
++# if we want to keep as much data as possible an use only one predicitn horizont
++# we can skeep this step
++DT = na.omit(DT, cols = "retExcessStand5")
++
+ # change IDate to date, because of error
++# Assertion on 'feature types' failed: Must be a subset of
++# {'logical','integer','numeric','character','factor','ordered','POSIXct'},
++# but has additional elements {'IDate'}.
+ DT[, date := as.POSIXct(date, tz = "UTC")]
++# DT[, .(symbol,date, date_rolling, yearmonthid)]
+ 
+ # sort
+-DT = DT[order(yearmonthid)]
+-
+-
++# this returns error on HPC. Some problem with memory
++# setorder(DT, date)
++print("This was the problem")
++# DT = DT[order(date)] # DOESNT WORK TOO
++DT = DT[order(yearmonthid, weekid)]
++DT[, .(symbol, date, weekid, yearmonthid)]
++print("This was the problem. Solved.")
+ 
+ 
+ # TASKS -------------------------------------------------------------------
+ print("Tasks")
+ 
+ # id coluns we always keep
+-id_cols = c("symbol", "date", "yearmonthid")
++id_cols = c("symbol", "date", "yearmonthid", "weekid")
+ 
+ # convert date to PosixCt because it is requireed by mlr3
+ DT[, date := as.POSIXct(date, tz = "UTC")]
+@@ -127,28 +151,37 @@ task_ret_week$col_roles$feature = setdiff(task_ret_week$col_roles$feature,
+ # CROSS VALIDATIONS -------------------------------------------------------
+ print("Cross validations")
+ 
+-# create train, tune and test set
+-nested_cv_split = function(task,
+-                           train_length = 12,
+-                           tune_length = 1,
+-                           gap_tune = 0,
+-                           id = task$id) {
++# cv split function
++nested_cv_split_week = function(task,
++                                train_length = 50,
++                                tune_length = 10,
++                                test_length = 1,
++                                gap_tune = 1,
++                                gap_test = 1,
++                                id = task$id) {
++
++  # # debug
++  # train_length = 144
++  # tune_length = 12
++  # test_length = 1
++  # gap_tune = 1
++  # gap_test = 1
+ 
+   # get year month id data
+   # task = task_ret_week$clone()
+   task_ = task$clone()
+-  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+-                                    rows = 1:task_$nrow)
+-  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+-  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
++  weekid_ = task_$backend$data(cols = c("weekid", "..row_id"),
++                               rows = 1:task_$nrow)
++  stopifnot(all(task_$row_ids == weekid_$`..row_id`))
++  groups_v = weekid_[, unlist(unique(weekid))]
+ 
+   # create cusom CV's for inner and outer sampling
+   custom_inner = rsmp("custom", id = task$id)
+   custom_outer = rsmp("custom", id = task$id)
+ 
+   # util vars
+-  start_folds = 1:(length(groups_v)-train_length-tune_length-gap_test)
+-  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
++  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test-gap_tune)
++  get_row_ids = function(mid) unlist(weekid_[weekid %in% mid, 2], use.names = FALSE)
+ 
+   # create train data
+   train_groups <- lapply(start_folds,
+@@ -162,19 +195,19 @@ nested_cv_split = function(task,
+ 
+   # test train and tune
+   test_1 = vapply(seq_along(train_groups), function(i) {
+-    mondf(
++    diff_in_weeks(
+       tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+     )
+   }, FUN.VALUE = numeric(1L))
+-  stopifnot(all(test_1 == (1+gap_tune)))
++  stopifnot(all(as.integer(test_1) %in% c(1-gap_tune+1, 1+gap_tune, 1+gap_tune+1)))
+   # test_2 = vapply(seq_along(train_groups), function(i) {
+   #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+   # }, FUN.VALUE = numeric(1L))
+   # stopifnot(all(test_2 > ))
+ 
+   # create test sets
+-  insample_length = train_length + gap_tune +  tune_length + gap_test
++  insample_length = train_length + gap_tune + tune_length + gap_test
+   test_groups <- lapply(start_folds,
+                         function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
+   test_sets <- lapply(test_groups, get_row_ids)
+@@ -186,7 +219,7 @@ nested_cv_split = function(task,
+       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+     )
+   }, FUN.VALUE = numeric(1L))
+-  stopifnot(all(test_1 == 1 + gap_test))
++  stopifnot(all(as.integer(test_1) %in% c(1 + gap_test - 1, 1 + gap_test, 1 + gap_test + 1)))
+   # test_4 = vapply(seq_along(train_groups), function(i) {
+   #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+   # }, FUN.VALUE = numeric(1L))
+@@ -207,54 +240,10 @@ nested_cv_split = function(task,
+ }
+ 
+ # generate cv's
+-train_sets = seq(12, 12 * 3, 12)
+-gap_sets = c(0)
+-mat = cbind(train = train_sets)
+-expanded_list  = lapply(gap_sets, function(v) {
+-  cbind.data.frame(mat, gap = v)
+-})
+-cv_param_grid = rbindlist(expanded_list)
+-cv_param_grid[ ,tune := 3]
+ custom_cvs = list()
+-for (i in 1:nrow(cv_param_grid)) {
+-  print(i)
+-  param_ = cv_param_grid[i]
+-  if (param_$gap == 0) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_week,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-  } else if (param_$gap == 1) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_month,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  } else if (param_$gap == 2) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  } else if (param_$gap == 3) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  }
+-}
+-
+-# test
+-length(custom_cvs) == nrow(cv_param_grid)
++custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 48, 12, 1, 1, 1)
++custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 96, 12, 1, 1, 1)
++custom_cvs[[3]] = nested_cv_split_week(task_ret_week, 144, 12, 1, 1, 1)
+ 
+ 
+ # ADD PIPELINES -----------------------------------------------------------
+@@ -292,10 +281,14 @@ mlr_measures$add("adjloss2", AdjLoss2)
+ mlr_measures$add("portfolio_ret", PortfolioRet)
+ 
+ 
+-# GRAPH V2 ----------------------------------------------------------------
++# LEARNERS ----------------------------------------------------------------
+ # graph template
++gr = gunion(list(
++  po("nop", id = "nop_union_pca"),
++  po("pca", center = FALSE, rank. = 100)
++)) %>>% po("featureunion")
+ graph_template =
+-  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("subsample") %>>% # uncomment this for hyperparameter tuning
+   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+@@ -312,13 +305,14 @@ graph_template =
+   )) %>>%
+   po("unbranch", id = "scale_unbranch") %>>%
+   po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
+   # filters
+   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+               po("filter", filter = flt("relief"), filter.frac = 0.05),
+               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
+   )) %>>%
+-  # po("nop", id = "nop_filter"))) %>>%
+   po("unbranch", id = "filter_unbranch") %>>%
+   # modelmatrix
+   po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
+@@ -332,7 +326,7 @@ graph_template =
+ graph_template$param_set
+ search_space_template = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -388,7 +382,7 @@ graph_xgboost = as_learner(graph_xgboost)
+ as.data.table(graph_xgboost$param_set)[grep("depth", id), .(id, class, lower, upper, levels)]
+ search_space_xgboost = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -477,17 +471,7 @@ graph_kknn = as_learner(graph_kknn)
+ as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
+ search_space_kknn = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+-  # preprocessing
+-  # dropnacol.affect_columns = p_fct(
+-  #   levels = c("0.01", "0.05", "0.10"),
+-  #   trafo = function(x, param_set) {
+-  #     switch(x,
+-  #            "0.01" = 0.01,
+-  #            "0.05" = 0.05,
+-  #            "0.10" = 0.1)
+-  #   }
+-  # ),
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   dropcorr.cutoff = p_fct(
+     levels = c("0.80", "0.90", "0.95", "0.99"),
+     trafo = function(x, param_set) {
+@@ -543,6 +527,39 @@ search_space_nnet$add(
+      regr.nnet.maxit = p_int(lower = 50, upper = 500))
+ )
+ 
++# glmnet graph
++graph_glmnet = graph_template %>>%
++  po("learner", learner = lrn("regr.glmnet"))
++graph_glmnet = as_learner(graph_glmnet)
++as.data.table(graph_glmnet$param_set)[, .(id, class, lower, upper, levels)]
++search_space_glmnet = ps(
++  # subsample for hyperband
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.glmnet.s     = p_int(lower = 5, upper = 30),
++  regr.glmnet.alpha = p_dbl(lower = 1e-4, upper = 1e4, logscale = TRUE)
++)
++
+ 
+ ### THIS LEARNER UNSTABLE ####
+ # ksvm graph
+@@ -585,7 +602,7 @@ search_space_nnet$add(
+ # lightgbm graph
+ # [LightGBM] [Fatal] Do not support special JSON characters in feature name.
+ graph_template =
+-  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("subsample") %>>% # uncomment this for hyperparameter tuning
+   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+@@ -601,6 +618,8 @@ graph_template =
+   )) %>>%
+   po("unbranch", id = "scale_unbranch") %>>%
+   po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
+   # filters
+   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+@@ -610,7 +629,7 @@ graph_template =
+   po("unbranch", id = "filter_unbranch")
+ search_space_template = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -713,9 +732,11 @@ set_threads(graph_earth, n = threads)
+ set_threads(graph_gbm, n = threads)
+ set_threads(graph_rsm, n = threads)
+ set_threads(graph_catboost, n = threads)
++set_threads(graph_glmnet, n = threads)
+ 
+ 
+ # DESIGNS -----------------------------------------------------------------
++# create designs
+ designs_l = lapply(custom_cvs, function(cv_) {
+   # debug
+   # cv_ = custom_cvs[[1]]
+@@ -723,187 +744,174 @@ designs_l = lapply(custom_cvs, function(cv_) {
+   # get cv inner object
+   cv_inner = cv_$custom_inner
+   cv_outer = cv_$custom_outer
+-  cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
+-
+-  # choose task_
+-  print(cv_inner$id)
+-  if (cv_inner$id == "taskRetWeek") {
+-    task_ = task_ret_week$clone()
+-  } else if (cv_inner$id == "taskRetMonth") {
+-    task_ = task_ret_month$clone()
+-  } else if (cv_inner$id == "taskRetMonth2") {
+-    task_ = task_ret_month2$clone()
+-  } else if (cv_inner$id == "taskRetQuarter") {
+-    task_ = task_ret_quarter$clone()
+-  }
+-
+-  designs_cv_l = lapply(1:cv_inner$iters, function(i) { # 1:cv_inner$iters
+-    # debug
+-    # i = 1
+-
+-    # choose task_
+-    print(cv_inner$id)
+-    if (cv_inner$id == "taskRetWeek") {
+-      task_ = task_ret_week$clone()
+-    } else if (cv_inner$id == "taskRetMonth") {
+-      task_ = task_ret_month$clone()
+-    } else if (cv_inner$id == "taskRetMonth2") {
+-      task_ = task_ret_month2$clone()
+-    } else if (cv_inner$id == "taskRetQuarter") {
+-      task_ = task_ret_quarter$clone()
+-    }
+ 
+-    # inner resampling
+-    custom_ = rsmp("custom")
+-    custom_$id = paste0("custom_", cv_inner$iters, "_", i)
+-    custom_$instantiate(task_ret_week,
+-                        list(cv_inner$train_set(i)),
+-                        list(cv_inner$test_set(i)))
+-
+-    # objects for all autotuners
+-    measure_ = msr("portfolio_ret")
+-    # tuner_   = tnr("hyperband", eta = 4)
+-    tuner_   = tnr("mbo")
+-    term_evals = 20
+-
+-    # auto tuner rf
+-    at_rf = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_rf,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_rf,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # get last
++  i = cv_inner$iters
++
++  # inner resampling
++  custom_ = rsmp("custom")
++  custom_$id = paste0("custom_", cv_inner$iters, "_", i)
++  custom_$instantiate(task_ret_week,
++                      list(cv_inner$train_set(i)),
++                      list(cv_inner$test_set(i)))
++
++  # objects for all autotuners
++  measure_ = msr("portfolio_ret")
++  tuner_   = tnr("hyperband", eta = 4)
++  # tuner_   = tnr("mbo")
++  # term_evals = 20
++
++  # auto tuner rf
++  at_rf = auto_tuner(
++    tuner = tuner_,
++    learner = graph_rf,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_rf,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner xgboost
+-    at_xgboost = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_xgboost,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_xgboost,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner xgboost
++  at_xgboost = auto_tuner(
++    tuner = tuner_,
++    learner = graph_xgboost,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_xgboost,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner BART
+-    at_bart = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_bart,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_bart,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner BART
++  at_bart = auto_tuner(
++    tuner = tuner_,
++    learner = graph_bart,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_bart,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner nnet
+-    at_nnet = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_nnet,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_nnet,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner nnet
++  at_nnet = auto_tuner(
++    tuner = tuner_,
++    learner = graph_nnet,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_nnet,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner lightgbm
+-    at_lightgbm = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_lightgbm,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_lightgbm,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner lightgbm
++  at_lightgbm = auto_tuner(
++    tuner = tuner_,
++    learner = graph_lightgbm,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_lightgbm,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner earth
+-    at_earth = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_earth,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_earth,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner earth
++  at_earth = auto_tuner(
++    tuner = tuner_,
++    learner = graph_earth,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_earth,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner kknn
+-    at_kknn = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_kknn,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_kknn,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner kknn
++  at_kknn = auto_tuner(
++    tuner = tuner_,
++    learner = graph_kknn,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_kknn,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner gbm
+-    at_gbm = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_gbm,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_gbm,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner gbm
++  at_gbm = auto_tuner(
++    tuner = tuner_,
++    learner = graph_gbm,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_gbm,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner rsm
+-    at_rsm = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_rsm,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_rsm,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner rsm
++  at_rsm = auto_tuner(
++    tuner = tuner_,
++    learner = graph_rsm,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_rsm,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner rsm
+-    at_bart = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_bart,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_bart,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner rsm
++  at_bart = auto_tuner(
++    tuner = tuner_,
++    learner = graph_bart,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_bart,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # auto tuner catboost
+-    at_catboost = auto_tuner(
+-      tuner = tuner_,
+-      learner = graph_catboost,
+-      resampling = custom_,
+-      measure = measure_,
+-      search_space = search_space_catboost,
+-      # terminator = trm("none")
+-      term_evals = term_evals
+-    )
++  # auto tuner catboost
++  at_catboost = auto_tuner(
++    tuner = tuner_,
++    learner = graph_catboost,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_catboost,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
+ 
+-    # outer resampling
+-    customo_ = rsmp("custom")
+-    customo_$id = paste0("custom_", cv_inner$iters, "_", i)
+-    customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
+-
+-    # nested CV for one round
+-    design = benchmark_grid(
+-      tasks = task_,
+-      learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth,
+-                      at_kknn, at_gbm, at_rsm, at_bart, at_catboost),
+-      resamplings = customo_
+-    )
+-  })
+-  designs_cv = do.call(rbind, designs_cv_l)
++  # auto tuner glmnet
++  at_glmnet = auto_tuner(
++    tuner = tuner_,
++    learner = graph_glmnet,
++    resampling = custom_,
++    measure = measure_,
++    search_space = search_space_glmnet,
++    terminator = trm("none")
++    # term_evals = term_evals
++  )
++
++  # outer resampling
++  customo_ = rsmp("custom")
++  customo_$id = paste0("custom_", cv_inner$iters, "_", i)
++  customo_$instantiate(task_ret_week, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
++
++  # nested CV for one round
++  design = benchmark_grid(
++    tasks = task_ret_week,
++    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth,
++                    at_kknn, at_gbm, at_rsm, at_bart, at_catboost, at_glmnet),
++    resamplings = customo_
++  )
+ })
+ designs = do.call(rbind, designs_l)
+ 
++# benchmark
++bmr = benchmark(designs, store_models = FALSE)
+ 
+-
+-
++# save locally and to list
++time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
++file_name = paste0("cv-", cv_$custom_inner$iters, "-", i, "-", time_, ".rds")
++saveRDS(bmr, file.path(mlr3_save_path, file_name))
+diff --git a/run_job.R b/run_job.R
+index 943f86f..f14be9a 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -106,14 +106,15 @@ UpdateBuffer = R6Class(
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
+ reg = loadRegistry("experiments")
+-# reg = loadRegistry("F:/H4-v9")
+ 
+ # extract not  done ids
+ ids_not_done = findNotDone(reg=reg)
+ 
+ # create job collection
+ resources = list(ncpus = 4) # this shouldnt be important
+-jc = makeJobCollection(ids_not_done, resources = resources, reg = reg)
++jc = makeJobCollection(ids_not_done,
++                       resources = resources,
++                       reg = reg)
+ 
+ # extract integer
+ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+@@ -123,8 +124,18 @@ i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+ buf = UpdateBuffer$new(jc$jobs$job.id)
+ update = list(started = batchtools:::ustamp(), done = NA_integer_, error = NA_character_, mem.used = NA_real_)
+ 
++
++batchtools:::getSeed(jc$seed, row$job.id)
++
++
+ # get job
+-job = batchtools:::getJob(jc, i)
++job = Job$new(file.dir = jc$file.dir,
++              reader = reader,
++              id = jc$jobs[i]$job.id,
++              job.pars = jc$jobs[i]$job.pars[[1L]],
++              seed = 1 + jc$jobs[i],
++              resources = jc$resources)
++# job = batchtools:::getJob(jc, i)
+ id = job$id
+ 
+ cat("CHANGE JOB ID MANNUALY", nrow(jc$jobs), "!!!")
+diff --git a/run_ml.sh b/run_ml.sh
+index ea9222a..7f230eb 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,8 +3,8 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=40GB
+-#PBS -J 1-2529
++#PBS -l mem=5GB
++#PBS -J 1-2516
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+
+12c7667719eef79421f03f36c8b0f1d685e41a8e unknown Wed Nov 15 12:07:24 2023 +0100 bug fix
+diff --git a/run_ml.sh b/run_ml.sh
+index 54ac96c..ea9222a 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -11,5 +11,3 @@
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
+ 
+-15552 - 13023
+-
+
+23d0ec0ab2e8d517a46e27c983cb3d9050a7e2c4 unknown Wed Nov 15 12:04:42 2023 +0100 control for error
+diff --git a/results_light.R b/results_light.R
+index 6000058..7bcf52c 100644
+--- a/results_light.R
++++ b/results_light.R
+@@ -78,7 +78,7 @@ if (!fs::dir_exists("predictions")) fs::dir_create("predictions")
+ saveRDS(predictions, file_name)
+ 
+ # import tasks
+-tasks_files = dir_ls("F:/H4/problems")
++tasks_files = dir_ls(fs::path(PATH, "problems"))
+ tasks = lapply(tasks_files, readRDS)
+ names(tasks) = lapply(tasks, function(t) t$data$id)
+ tasks
+@@ -138,6 +138,7 @@ unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c(
+ unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c("task", "cv")]
+ 
+ # accuracy by ids
++pr = PortfolioRet$new()
+ measures = function(t, res) {
+   list(acc   = mlr3measures::acc(t, res),
+        fbeta = mlr3measures::fbeta(t, res, positive = "1"),
+@@ -179,9 +180,13 @@ predictions_ensemble[, (cols_sign_response_neg) := lapply(sign_response_seq, fun
+ # cols_ = colnames(predictions_dt_ensemble)[24:ncol(predictions_dt_ensemble)]
+ # predictions_dt_ensemble[, lapply(.SD, function(x) sum(x == TRUE)), .SDcols = cols_]
+ 
++predictions_ensemble[median_response > 0 & sd_response < 0.15, .(tr = truth_sign, res = 1)][, sum(tr == res) / length(tr)]
++
++
+ # check only sign ensamble performance
+-res = lapply(cols_sign_response_pos, function(x) {
+-  predictions_ensemble[get(x) == TRUE][
++res = lapply(cols_sign_response_pos[1:5], function(x) {
++  print(x)
++  predictions_ensemble[get(x) == TRUE & sd_response < 0.3][
+     , mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = c("task")]
+ })
+ names(res) = cols_sign_response_pos
+@@ -210,10 +215,12 @@ lapply(unique(predictions_ensemble$task), function(x) {
+   # x = "taskRetWeek"
+ 
+   # prepare data
++  # y = predictions_ensemble[median_response > 0 & sd_response < 0.15]
++  # y = y[, response_sign_sign_pos16 := TRUE]
+   y = predictions_ensemble[task == x]
+   y = na.omit(y)
+   cols = colnames(y)[grep("response_sign", colnames(y))]
+-  cols = c("symbol", "date", "epsDiff", cols)
++  cols = c("symbol", "date", "epsDiff", "mean_response", "sd_response", cols)
+   y = y[, ..cols]
+   y = unique(y)
+ 
+diff --git a/run_job.R b/run_job.R
+index 78da180..943f86f 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -138,8 +138,13 @@ result = execJob(job)
+ writeRDS(result, file = getResultFiles(jc, id), compress = jc$compress)
+ 
+ # memory usage
+-memory.mult = c(if (.Machine$sizeof.pointer == 4L) 28L else 56L, 8L)
+-memory_used = sum(gc()[, 1L] * memory.mult) / 1000000L
++tryCatch({
++  memory.mult = c(if (.Machine$sizeof.pointer == 4L) 28L else 56L, 8L)
++  gc_info <- gc(verbose = FALSE)
++  memory_used = sum(gc_info[, 1L] * memory.mult) / 1000000L
++}, error = function(e) {
++  memory_used <- 1000  # Set to NA or some default value in case of error
++})
+ 
+ # updates
+ update$done = batchtools:::ustamp()
+diff --git a/run_ml.sh b/run_ml.sh
+index 4318f5c..54ac96c 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -4,9 +4,12 @@
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+ #PBS -l mem=40GB
+-#PBS -J 1-2584
++#PBS -J 1-2529
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
++
++15552 - 13023
++
+
+4849b3fde3137d87a35c2ce0161515d29f655349 unknown Tue Nov 14 08:44:27 2023 +0100 increase memory
+diff --git a/run_ml.sh b/run_ml.sh
+index 5171ec6..4318f5c 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,8 +3,8 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=30GB
+-#PBS -J 1-2802
++#PBS -l mem=40GB
++#PBS -J 1-2584
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+
+349fc73890c57b72611bb2a13f50612c2cce197f unknown Mon Nov 13 14:44:19 2023 +0100 increase memory
+diff --git a/results_light.R b/results_light.R
+index 24d22dd..6000058 100644
+--- a/results_light.R
++++ b/results_light.R
+@@ -38,7 +38,7 @@ rbind(ids_notdone, ids_done[job.id %in% results_files])
+ plan("multisession", workers = 4L)
+ start_time = Sys.time()
+ results = future_lapply(ids_done[, job.id], function(id_) {
+-  # id_ = 100
++  # id_ = 2110
+   print(id_)
+   # bmr object
+   bmrs = reduceResultsBatchmark(id_, store_backends = FALSE, reg = reg)
+diff --git a/run_ml.sh b/run_ml.sh
+index ff9dd78..5171ec6 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,8 +3,8 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=20GB
+-#PBS -J 1-3340
++#PBS -l mem=30GB
++#PBS -J 1-2802
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+
+4eee2a267d1cd9c6f5f6c798ce420829cc10bd87 unknown Mon Nov 13 00:16:46 2023 +0100 increase memory
+diff --git a/run_ml.sh b/run_ml.sh
+index 1a52543..ff9dd78 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,8 +3,8 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=15GB
+-#PBS -J 1-3139
++#PBS -l mem=20GB
++#PBS -J 1-3340
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+
+5b6668f7b1e16808929fbf997d84b1b4873601ba unknown Sat Nov 11 16:37:28 2023 +0100 final job
+diff --git a/run_ml.sh b/run_ml.sh
+index 07d4c26..1a52543 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,11 +3,10 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=12GB
+-#PBS -J 1-5000
++#PBS -l mem=15GB
++#PBS -J 1-3139
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_job.R
+-
+
+ef6a20db49aa7a83710cfdf2bb1a8e1a338589b4 unknown Fri Nov 10 13:50:30 2023 +0100 increase memory
+diff --git a/results_light.R b/results_light.R
+index 55d4010..24d22dd 100644
+--- a/results_light.R
++++ b/results_light.R
+@@ -13,15 +13,18 @@ blob_key = "0M4WRlV0/1b6b3ZpFKJvevg4xbC/gaNBcdtVZW+zOZcRi0ZLfOm1v/j2FZ4v+o8lycJL
+ endpoint = "https://snpmarketdata.blob.core.windows.net/"
+ BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+ 
++# globals
++PATH = "F:/H4week"
++
+ # load registry
+-reg = loadRegistry("F:/H4v2", work.dir="F:/H4v2")
++reg = loadRegistry(PATH, work.dir=PATH)
+ 
+ # used memory
+ reg$status[!is.na(mem.used)]
+ reg$status[, max(mem.used, na.rm = TRUE)]
+ 
+ # done jobs
+-results_files = fs::path_ext_remove(fs::path_file(dir_ls("F:/H4/results")))
++results_files = fs::path_ext_remove(fs::path_file(dir_ls(fs::path(PATH, "results"))))
+ ids_done = findDone(reg=reg)
+ ids_done = ids_done[job.id %in% results_files]
+ ids_notdone = findNotDone(reg=reg)
+@@ -35,7 +38,7 @@ rbind(ids_notdone, ids_done[job.id %in% results_files])
+ plan("multisession", workers = 4L)
+ start_time = Sys.time()
+ results = future_lapply(ids_done[, job.id], function(id_) {
+-  # id_ = 6819
++  # id_ = 100
+   print(id_)
+   # bmr object
+   bmrs = reduceResultsBatchmark(id_, store_backends = FALSE, reg = reg)
+@@ -193,11 +196,11 @@ names(res) = cols_sign_response_pos
+ res
+ 
+ # check only sign ensamble performance
+-res = lapply(cols_sign_response_neg, function(x) {
++res = lapply(cols_sign_response_neg[1:5], function(x) { # TODO: REMOVE indexing later
+   predictions_ensemble[get(x) == TRUE][
+     , mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = c("task")]
+ })
+-names(res) = cols_sign_response_neg
++names(res) = cols_sign_response_neg[1:5]
+ res
+ 
+ # save to azure for QC backtest
+diff --git a/run_ml.sh b/run_ml.sh
+index 2a7e78b..07d4c26 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -3,7 +3,7 @@
+ 
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+-#PBS -l mem=10GB
++#PBS -l mem=12GB
+ #PBS -J 1-5000
+ #PBS -o experiments/logs
+ #PBS -j oe
+
+d10211c9fdfa808868023dce35b9375363d9bc5e unknown Thu Nov 9 09:30:31 2023 +0100 experiments
+diff --git a/run_job.R b/run_job.R
+index 037d4ba..78da180 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -105,7 +105,7 @@ UpdateBuffer = R6Class(
+ 
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
+-reg = loadRegistry("experiment")
++reg = loadRegistry("experiments")
+ # reg = loadRegistry("F:/H4-v9")
+ 
+ # extract not  done ids
+
+727fae44447c6b7e21c87a5189ab5f74d0cc31bc unknown Thu Nov 9 09:14:11 2023 +0100 decrease array size
+diff --git a/run_ml.sh b/run_ml.sh
+index 9663488..2a7e78b 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -4,7 +4,7 @@
+ #PBS -N PEAD
+ #PBS -l ncpus=4
+ #PBS -l mem=10GB
+-#PBS -J 1-14256
++#PBS -J 1-5000
+ #PBS -o experiments/logs
+ #PBS -j oe
+ 
+
+5c907aef777bdd80febad67eb6b7342f1dfdb975 unknown Wed Nov 8 18:45:20 2023 +0100 increase memory for preparation
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index 0dc205c..f9eedbc 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -1,7 +1,7 @@
+ #!/bin/bash
+ 
+ #PBS -N PEADPREPARE
+-#PBS -l mem=50GB
++#PBS -l mem=55GB
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+52bee59d3638c677dc3b4f6f1886542d04187720 unknown Wed Nov 8 16:01:46 2023 +0100 increase memory for preparation
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index 6a90510..0dc205c 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -1,7 +1,7 @@
+ #!/bin/bash
+ 
+ #PBS -N PEADPREPARE
+-#PBS -l mem=45GB
++#PBS -l mem=50GB
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+76c26ad4d31ade41939a795aa9de3aae11cd3110 unknown Wed Nov 8 16:01:31 2023 +0100 increase memory for preparation
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index fa8db06..6a90510 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -1,7 +1,7 @@
+ #!/bin/bash
+ 
+ #PBS -N PEADPREPARE
+-#PBS -l mem=40GB
++#PBS -l mem=45GB
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+2a96c4339f1a45130d9ca7a0a501471c67f21f0e unknown Wed Nov 8 13:34:32 2023 +0100 change image def
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index 7fdf83c..fa8db06 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -1,7 +1,7 @@
+ #!/bin/bash
+ 
+ #PBS -N PEADPREPARE
+-#PBS -l mem=35GB
++#PBS -l mem=40GB
+ 
+ cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+2505d72317291601cc130174643d7f468f9f3821 unknown Wed Nov 8 13:34:22 2023 +0100 change image def
+diff --git a/image.def b/image.def
+index 5d18d6c..9033da5 100644
+--- a/image.def
++++ b/image.def
+@@ -43,7 +43,7 @@ From: r-base:4.3.0
+   R --slave -e 'install.packages("gbm")'
+   R --slave -e 'install.packages("rsm")'
+   R --slave -e 'install.packages("glmnet")'
+-  R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.1/catboost-R-Linux-1.2.2.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
++  R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.2/catboost-R-Linux-1.2.2.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
+   R --slave -e 'install.packages("torch")'
+   R --slave -e 'torch::install_torch()'
+   R --slave -e 'remotes::install_github("mlr-org/mlr3torch")'
+
+0fc954b123b39426aaf46d17dd4d7a5d82577bc4 unknown Wed Nov 8 12:25:58 2023 +0100 run padobran on padobran
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index 316c4c5..7fdf83c 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -3,5 +3,5 @@
+ #PBS -N PEADPREPARE
+ #PBS -l mem=35GB
+ 
+-cd ${sPBS_O_WORKDIR}
++cd ${PBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+8c0e0d33c87a87eca4cf3c02727b60d24c498fcf unknown Wed Nov 8 12:15:54 2023 +0100 run padobran on padobran
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index d6b6b48..6740e53 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -282,7 +282,7 @@ mlr_measures$add("adjloss2", AdjLoss2)
+ mlr_measures$add("portfolio_ret", PortfolioRet)
+ 
+ 
+-# GRAPH V2 ----------------------------------------------------------------
++# LEARNERS ----------------------------------------------------------------
+ # graph template
+ gr = gunion(list(
+   po("nop", id = "nop_union_pca"),
+@@ -327,7 +327,7 @@ graph_template =
+ graph_template$param_set
+ search_space_template = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -383,7 +383,7 @@ graph_xgboost = as_learner(graph_xgboost)
+ as.data.table(graph_xgboost$param_set)[grep("depth", id), .(id, class, lower, upper, levels)]
+ search_space_xgboost = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -472,7 +472,7 @@ graph_kknn = as_learner(graph_kknn)
+ as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
+ search_space_kknn = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   dropcorr.cutoff = p_fct(
+     levels = c("0.80", "0.90", "0.95", "0.99"),
+     trafo = function(x, param_set) {
+@@ -536,6 +536,7 @@ as.data.table(graph_glmnet$param_set)[, .(id, class, lower, upper, levels)]
+ search_space_glmnet = ps(
+   # subsample for hyperband
+   subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++
+   dropcorr.cutoff = p_fct(
+     levels = c("0.80", "0.90", "0.95", "0.99"),
+     trafo = function(x, param_set) {
+@@ -602,7 +603,7 @@ search_space_glmnet = ps(
+ # lightgbm graph
+ # [LightGBM] [Fatal] Do not support special JSON characters in feature name.
+ graph_template =
+-  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("subsample") %>>% # uncomment this for hyperparameter tuning
+   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+@@ -629,7 +630,7 @@ graph_template =
+   po("unbranch", id = "filter_unbranch")
+ search_space_template = ps(
+   # subsample for hyperband
+-  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+   # preprocessing
+   # dropnacol.affect_columns = p_fct(
+   #   levels = c("0.01", "0.05", "0.10"),
+@@ -948,28 +949,28 @@ sh_file_name = "run_ml.sh"
+ file.create(sh_file_name)
+ writeLines(sh_file, sh_file_name)
+ 
+-# send zipped file to Azure
+-folder_to_zip <- "C:/Users/Mislav/Documents/GitHub/PEAD/experiments"
+-output_zip_file <- "experiments.zip"
+-getDirSize <- function(dir) {
+-  # List all files in the directory and its subdirectories
+-  files <- list.files(dir, recursive = TRUE, full.names = TRUE)
+-
+-  # Get the size of each file
+-  file_sizes <- sapply(files, function(x) file.info(x)$size)
+-
+-  # Calculate the total size
+-  total_size <- sum(file_sizes, na.rm = TRUE)
+-
+-  return(total_size)
+-}
+-dir_size <- getDirSize(folder_to_zip)
+-dir_size_gb <- dir_size / 2^30
+-print(paste("Directory size:", formatC(dir_size_gb, format = "f", digits = 2), "GB"))
+-utils::zip(zipfile = output_zip_file, files = folder_to_zip, flags = "-r")
+-
+-blob_key = readLines('./blob_key.txt')
+-endpoint = "https://snpmarketdata.blob.core.windows.net/"
+-BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+-cont = storage_container(BLOBENDPOINT, "jphd")
+-storage_upload(cont, output_zip_file, "pead_experiment.zip")
++# # send zipped file to Azure
++# folder_to_zip <- "C:/Users/Mislav/Documents/GitHub/PEAD/experiments"
++# output_zip_file <- "experiments.zip"
++# getDirSize <- function(dir) {
++#   # List all files in the directory and its subdirectories
++#   files <- list.files(dir, recursive = TRUE, full.names = TRUE)
++#
++#   # Get the size of each file
++#   file_sizes <- sapply(files, function(x) file.info(x)$size)
++#
++#   # Calculate the total size
++#   total_size <- sum(file_sizes, na.rm = TRUE)
++#
++#   return(total_size)
++# }
++# dir_size <- getDirSize(folder_to_zip)
++# dir_size_gb <- dir_size / 2^30
++# print(paste("Directory size:", formatC(dir_size_gb, format = "f", digits = 2), "GB"))
++# utils::zip(zipfile = output_zip_file, files = folder_to_zip, flags = "-r")
++#
++# blob_key = readLines('./blob_key.txt')
++# endpoint = "https://snpmarketdata.blob.core.windows.net/"
++# BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
++# cont = storage_container(BLOBENDPOINT, "jphd")
++# storage_upload(cont, output_zip_file, "pead_experiment.zip")
+
+598876b70b034bb3e939ff48cc210a5103ba9f06 unknown Wed Nov 8 11:35:32 2023 +0100 preprocesing locally
+diff --git a/.gitignore b/.gitignore
+index 1c4a708..2b71018 100644
+--- a/.gitignore
++++ b/.gitignore
+@@ -8,3 +8,7 @@ pead-predictors.csv
+ pead-predictors-update.csv
+ pead-predictors-sample.csv
+ H4*.
++experiment
++predictions
++experiments.zip
++pead_experiment.zip
+diff --git a/download.R b/download.R
+index 2a7266d..9dc402f 100644
+--- a/download.R
++++ b/download.R
+@@ -5,4 +5,12 @@ blob_key = readLines('./blob_key.txt')
+ endpoint = "https://snpmarketdata.blob.core.windows.net/"
+ BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+ cont = storage_container(BLOBENDPOINT, "jphd")
+-storage_download(cont, "pead-predictors-20231031.csv", overwrite=TRUE)
++storage_download(cont, "pead_experiment.zip", overwrite=TRUE)
++unzip(zipfile = "pead_experiment.zip", exdir = "experiment")
++dir.create("experiment2")
++dir_exp = list.files('./experiment/Users/Mislav/Documents/GitHub/PEAD/experiments',
++                     full.names = TRUE)
++file.copy(dir_exp, './experiment2', recursive = TRUE)
++unlink('./experiment', recursive = TRUE)
++file.rename("experiment2", "experiment")
++
+diff --git a/run_job.R b/run_job.R
+index 9f87e2e..037d4ba 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -14,7 +14,6 @@ library(batchtools)
+ library(mlr3batchmark)
+ library(checkmate)
+ library(stringi)
+-library(fs)
+ library(R6)
+ library(brew)
+ 
+@@ -106,7 +105,7 @@ UpdateBuffer = R6Class(
+ 
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
+-reg = loadRegistry("experiments")
++reg = loadRegistry("experiment")
+ # reg = loadRegistry("F:/H4-v9")
+ 
+ # extract not  done ids
+diff --git a/run_ml.sh b/run_ml.sh
+index e69de29..9663488 100644
+--- a/run_ml.sh
++++ b/run_ml.sh
+@@ -0,0 +1,13 @@
++
++#!/bin/bash
++
++#PBS -N PEAD
++#PBS -l ncpus=4
++#PBS -l mem=10GB
++#PBS -J 1-14256
++#PBS -o experiments/logs
++#PBS -j oe
++
++cd ${PBS_O_WORKDIR}
++apptainer run image.sif run_job.R
++
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+index 05e226d..d6b6b48 100644
+--- a/run_padobran_week.R
++++ b/run_padobran_week.R
+@@ -11,6 +11,7 @@ library(future)
+ library(future.apply)
+ library(batchtools)
+ library(mlr3batchmark)
++library(AzureStor)
+ 
+ 
+ # SETUP -------------------------------------------------------------------
+@@ -916,7 +917,7 @@ packages = c("data.table", "gausscov", "paradox", "mlr3", "mlr3pipelines",
+              "mlr3tuning", "mlr3misc", "future", "future.apply",
+              "mlr3extralearners", "stats")
+ reg = makeExperimentRegistry(
+-  file.dir = "./experiments2",
++  file.dir = "./experiments",
+   seed = 1,
+   packages = packages
+ )
+@@ -946,3 +947,29 @@ apptainer run image.sif run_job.R
+ sh_file_name = "run_ml.sh"
+ file.create(sh_file_name)
+ writeLines(sh_file, sh_file_name)
++
++# send zipped file to Azure
++folder_to_zip <- "C:/Users/Mislav/Documents/GitHub/PEAD/experiments"
++output_zip_file <- "experiments.zip"
++getDirSize <- function(dir) {
++  # List all files in the directory and its subdirectories
++  files <- list.files(dir, recursive = TRUE, full.names = TRUE)
++
++  # Get the size of each file
++  file_sizes <- sapply(files, function(x) file.info(x)$size)
++
++  # Calculate the total size
++  total_size <- sum(file_sizes, na.rm = TRUE)
++
++  return(total_size)
++}
++dir_size <- getDirSize(folder_to_zip)
++dir_size_gb <- dir_size / 2^30
++print(paste("Directory size:", formatC(dir_size_gb, format = "f", digits = 2), "GB"))
++utils::zip(zipfile = output_zip_file, files = folder_to_zip, flags = "-r")
++
++blob_key = readLines('./blob_key.txt')
++endpoint = "https://snpmarketdata.blob.core.windows.net/"
++BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
++cont = storage_container(BLOBENDPOINT, "jphd")
++storage_upload(cont, output_zip_file, "pead_experiment.zip")
+
+1a69431ab41eb21a3bcea63cd8dd23f59403d3b2 unknown Tue Nov 7 09:26:12 2023 +0100 bug fix
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+index 0295549..316c4c5 100644
+--- a/run_padobran_week.sh
++++ b/run_padobran_week.sh
+@@ -1,7 +1,7 @@
+ #!/bin/bash
+ 
+ #PBS -N PEADPREPARE
+-#PBS -l mem=35GBs
++#PBS -l mem=35GB
+ 
+ cd ${sPBS_O_WORKDIR}
+ apptainer run image.sif run_padobran_week.R
+
+df03284e10c8a3742ee101b0d138ea983d5047f7 unknown Tue Nov 7 09:21:11 2023 +0100 add sh file for run_padobran, craete sh file automaticly
+diff --git a/run_job.R b/run_job.R
+index c7a858b..9f87e2e 100644
+--- a/run_job.R
++++ b/run_job.R
+@@ -106,7 +106,7 @@ UpdateBuffer = R6Class(
+ 
+ # RUN JOB -----------------------------------------------------------------
+ # load registry
+-reg = loadRegistry("experiments2")
++reg = loadRegistry("experiments")
+ # reg = loadRegistry("F:/H4-v9")
+ 
+ # extract not  done ids
+diff --git a/run_ml.sh b/run_ml.sh
+new file mode 100644
+index 0000000..e69de29
+diff --git a/run_padobran_week.R b/run_padobran_week.R
+new file mode 100644
+index 0000000..05e226d
+--- /dev/null
++++ b/run_padobran_week.R
+@@ -0,0 +1,948 @@
++library(data.table)
++library(gausscov)
++library(paradox)
++library(mlr3)
++library(mlr3pipelines)
++library(mlr3viz)
++library(mlr3tuning)
++library(mlr3misc)
++library(mlr3extralearners)
++library(future)
++library(future.apply)
++library(batchtools)
++library(mlr3batchmark)
++
++
++# SETUP -------------------------------------------------------------------
++# utils https://stackoverflow.com/questions/1995933/number-of-months-between-two-dates
++monnb <- function(d) {
++  lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
++  lt$year*12 + lt$mon }
++mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
++diff_in_weeks = function(d1, d2) difftime(d2, d1, units = "weeks") # weeks
++
++# weeknb <- function(d) {
++#   as.numeric(difftime(as.Date(d), as.Date("1900-01-01"), units = "weeks"))
++# }
++# weekdf <- function(d1, d2) {
++#   weeknb(d2) - weeknb(d1)
++# }
++
++# snake to camel
++snakeToCamel <- function(snake_str) {
++  # Replace underscores with spaces
++  spaced_str <- gsub("_", " ", snake_str)
++
++  # Convert to title case using tools::toTitleCase
++  title_case_str <- tools::toTitleCase(spaced_str)
++
++  # Remove spaces and make the first character lowercase
++  camel_case_str <- gsub(" ", "", title_case_str)
++  camel_case_str <- sub("^.", tolower(substr(camel_case_str, 1, 1)), camel_case_str)
++
++  # I haeve added this to remove dot
++  camel_case_str <- gsub("\\.", "", camel_case_str)
++
++  return(camel_case_str)
++}
++
++
++# PREPARE DATA ------------------------------------------------------------
++print("Prepare data")
++
++# read predictors
++data_tbl <- fread("pead-predictors-20231031.csv")
++# data_tbl <- fread("D:/features/pead-predictors-20231031.csv")
++
++# convert tibble to data.table
++DT = as.data.table(data_tbl)
++
++# create group variable
++DT[, date_rolling := as.IDate(date_rolling)]
++DT[, yearmonthid := round(date_rolling, digits = "month")]
++DT[, weekid := round(date_rolling, digits = "week")]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
++DT[, yearmonthid := as.integer(yearmonthid)]
++DT[, weekid := as.integer(weekid)]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
++
++# remove industry and sector vars
++DT[, `:=`(industry = NULL, sector = NULL)]
++
++# define predictors
++cols_non_features <- c("symbol", "date", "time", "right_time",
++                       "bmo_return", "amc_return",
++                       "open", "high", "low", "close", "volume", "returns",
++                       "yearmonthid", "weekid", "date_rolling"
++)
++targets <- c(colnames(DT)[grep("ret_excess", colnames(DT))])
++cols_features <- setdiff(colnames(DT), c(cols_non_features, targets))
++
++# change feature and targets columns names due to lighgbm
++cols_features_new = vapply(cols_features, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
++setnames(DT, cols_features, cols_features_new)
++cols_features = cols_features_new
++targets_new = vapply(targets, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
++setnames(DT, targets, targets_new)
++targets = targets_new
++
++# convert columns to numeric. This is important only if we import existing features
++chr_to_num_cols <- setdiff(colnames(DT[, .SD, .SDcols = is.character]), c("symbol", "time", "right_time"))
++print(chr_to_num_cols)
++DT <- DT[, (chr_to_num_cols) := lapply(.SD, as.numeric), .SDcols = chr_to_num_cols]
++
++# remove constant columns in set
++features_ <- DT[, ..cols_features]
++remove_cols <- colnames(features_)[apply(features_, 2, var, na.rm=TRUE) == 0]
++print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
++cols_features <- setdiff(cols_features, remove_cols)
++
++# convert variables with low number of unique values to factors
++int_numbers = na.omit(DT[, ..cols_features])[, lapply(.SD, function(x) all(floor(x) == x))]
++int_cols = colnames(DT[, ..cols_features])[as.matrix(int_numbers)[1,]]
++factor_cols = DT[, ..int_cols][, lapply(.SD, function(x) length(unique(x)))]
++factor_cols = as.matrix(factor_cols)[1, ]
++factor_cols = factor_cols[factor_cols <= 100]
++DT = DT[, (names(factor_cols)) := lapply(.SD, as.factor), .SD = names(factor_cols)]
++
++# remove observations with missing target
++# if we want to keep as much data as possible an use only one predicitn horizont
++# we can skeep this step
++DT = na.omit(DT, cols = "retExcessStand5")
++
++# change IDate to date, because of error
++# Assertion on 'feature types' failed: Must be a subset of
++# {'logical','integer','numeric','character','factor','ordered','POSIXct'},
++# but has additional elements {'IDate'}.
++DT[, date := as.POSIXct(date, tz = "UTC")]
++# DT[, .(symbol,date, date_rolling, yearmonthid)]
++
++# sort
++# this returns error on HPC. Some problem with memory
++# setorder(DT, date)
++print("This was the problem")
++# DT = DT[order(date)] # DOESNT WORK TOO
++DT = DT[order(yearmonthid, weekid)]
++DT[, .(symbol, date, weekid, yearmonthid)]
++print("This was the problem. Solved.")
++
++
++# TASKS -------------------------------------------------------------------
++print("Tasks")
++
++# id coluns we always keep
++id_cols = c("symbol", "date", "yearmonthid", "weekid")
++
++# convert date to PosixCt because it is requireed by mlr3
++DT[, date := as.POSIXct(date, tz = "UTC")]
++
++# task with future week returns as target
++target_ = colnames(DT)[grep("^ret.*xcess.*tand.*5", colnames(DT))]
++cols_ = c(id_cols, target_, cols_features)
++task_ret_week <- as_task_regr(DT[, ..cols_],
++                              id = "taskRetWeek",
++                              target = target_)
++
++# set roles for symbol, date and yearmonth_id
++task_ret_week$col_roles$feature = setdiff(task_ret_week$col_roles$feature,
++                                          id_cols)
++
++
++# CROSS VALIDATIONS -------------------------------------------------------
++print("Cross validations")
++
++# cv split function
++nested_cv_split_week = function(task,
++                                train_length = 50,
++                                tune_length = 10,
++                                test_length = 1,
++                                gap_tune = 1,
++                                gap_test = 1,
++                                id = task$id) {
++
++  # # debug
++  # train_length = 144
++  # tune_length = 12
++  # test_length = 1
++  # gap_tune = 1
++  # gap_test = 1
++
++  # get year month id data
++  # task = task_ret_week$clone()
++  task_ = task$clone()
++  weekid_ = task_$backend$data(cols = c("weekid", "..row_id"),
++                               rows = 1:task_$nrow)
++  stopifnot(all(task_$row_ids == weekid_$`..row_id`))
++  groups_v = weekid_[, unlist(unique(weekid))]
++
++  # create cusom CV's for inner and outer sampling
++  custom_inner = rsmp("custom", id = task$id)
++  custom_outer = rsmp("custom", id = task$id)
++
++  # util vars
++  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test-gap_tune)
++  get_row_ids = function(mid) unlist(weekid_[weekid %in% mid, 2], use.names = FALSE)
++
++  # create train data
++  train_groups <- lapply(start_folds,
++                         function(x) groups_v[x:(x+train_length-1)])
++  train_sets <- lapply(train_groups, get_row_ids)
++
++  # create tune set
++  tune_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+train_length+gap_tune):(x+train_length+gap_tune+tune_length-1)])
++  tune_sets <- lapply(tune_groups, get_row_ids)
++
++  # test train and tune
++  test_1 = vapply(seq_along(train_groups), function(i) {
++    diff_in_weeks(
++      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
++      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
++    )
++  }, FUN.VALUE = numeric(1L))
++  stopifnot(all(as.integer(test_1) %in% c(1-gap_tune+1, 1+gap_tune, 1+gap_tune+1)))
++  # test_2 = vapply(seq_along(train_groups), function(i) {
++  #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
++  # }, FUN.VALUE = numeric(1L))
++  # stopifnot(all(test_2 > ))
++
++  # create test sets
++  insample_length = train_length + gap_tune + tune_length + gap_test
++  test_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
++  test_sets <- lapply(test_groups, get_row_ids)
++
++  # test tune and test
++  test_3 = vapply(seq_along(train_groups), function(i) {
++    mondf(
++      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
++      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
++    )
++  }, FUN.VALUE = numeric(1L))
++  stopifnot(all(as.integer(test_1) %in% c(1 + gap_test - 1, 1 + gap_test, 1 + gap_test + 1)))
++  # test_4 = vapply(seq_along(train_groups), function(i) {
++  #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
++  # }, FUN.VALUE = numeric(1L))
++  # stopifnot(all(test_2 == 1))
++
++  # test
++  # as.Date(train_groups[[2]])
++  # as.Date(tune_groups[[2]])
++  # as.Date(test_groups[[2]])
++
++  # create inner and outer resamplings
++  custom_inner$instantiate(task, train_sets, tune_sets)
++  inner_sets = lapply(seq_along(train_groups), function(i) {
++    c(train_sets[[i]], tune_sets[[i]])
++  })
++  custom_outer$instantiate(task, inner_sets, test_sets)
++  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
++}
++
++# generate cv's
++custom_cvs = list()
++custom_cvs[[1]] = nested_cv_split_week(task_ret_week, 48, 12, 1, 1, 1)
++custom_cvs[[2]] = nested_cv_split_week(task_ret_week, 96, 12, 1, 1, 1)
++custom_cvs[[3]] = nested_cv_split_week(task_ret_week, 144, 12, 1, 1, 1)
++
++
++# ADD PIPELINES -----------------------------------------------------------
++print("Add pipelines")
++
++# source pipes, filters and other
++source("mlr3_winsorization.R")
++source("mlr3_uniformization.R")
++source("mlr3_gausscov_f1st.R")
++source("mlr3_gausscov_f3st.R")
++source("mlr3_dropna.R")
++source("mlr3_dropnacol.R")
++source("mlr3_filter_drop_corr.R")
++source("mlr3_winsorizationsimple.R")
++source("mlr3_winsorizationsimplegroup.R")
++source("PipeOpPCAExplained.R")
++# measures
++source("Linex.R")
++source("AdjLoss2.R")
++source("PortfolioRet.R")
++
++# add my pipes to mlr dictionary
++mlr_pipeops$add("uniformization", PipeOpUniform)
++mlr_pipeops$add("winsorize", PipeOpWinsorize)
++mlr_pipeops$add("winsorizesimple", PipeOpWinsorizeSimple)
++mlr_pipeops$add("winsorizesimplegroup", PipeOpWinsorizeSimpleGroup)
++mlr_pipeops$add("dropna", PipeOpDropNA)
++mlr_pipeops$add("dropnacol", PipeOpDropNACol)
++mlr_pipeops$add("dropcorr", PipeOpDropCorr)
++mlr_pipeops$add("pca_explained", PipeOpPCAExplained)
++mlr_filters$add("gausscov_f1st", FilterGausscovF1st)
++mlr_filters$add("gausscov_f3st", FilterGausscovF3st)
++mlr_measures$add("linex", Linex)
++mlr_measures$add("adjloss2", AdjLoss2)
++mlr_measures$add("portfolio_ret", PortfolioRet)
++
++
++# GRAPH V2 ----------------------------------------------------------------
++# graph template
++gr = gunion(list(
++  po("nop", id = "nop_union_pca"),
++  po("pca", center = FALSE, rank. = 100)
++)) %>>% po("featureunion")
++graph_template =
++  po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
++  po("dropna", id = "dropna") %>>%
++  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
++  po("fixfactors", id = "fixfactors") %>>%
++  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
++  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
++  # po("uniformization") %>>%
++  # scale branch
++  po("branch", options = c("uniformization", "scale"), id = "scale_branch") %>>%
++  gunion(list(po("uniformization"),
++              po("scale")
++  )) %>>%
++  po("unbranch", id = "scale_unbranch") %>>%
++  po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
++  # filters
++  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
++  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
++              po("filter", filter = flt("relief"), filter.frac = 0.05),
++              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
++  )) %>>%
++  po("unbranch", id = "filter_unbranch") %>>%
++  # modelmatrix
++  po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
++  gunion(list(
++    po("nop", id = "nop_interaction"),
++    po("modelmatrix", formula = ~ . ^ 2))) %>>%
++  po("unbranch", id = "interaction_unbranch") %>>%
++  po("removeconstants", id = "removeconstants_3", ratio = 0)
++
++# hyperparameters template
++graph_template$param_set
++search_space_template = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix"))
++)
++
++# random forest graph
++graph_rf = graph_template %>>%
++  po("learner", learner = lrn("regr.ranger"))
++plot(graph_rf)
++graph_rf = as_learner(graph_rf)
++as.data.table(graph_rf$param_set)[, .(id, class, lower, upper, levels)]
++search_space_rf = search_space_template$clone()
++search_space_rf$add(
++  ps(regr.ranger.max.depth  = p_int(1, 20),
++     regr.ranger.replace    = p_lgl(),
++     regr.ranger.mtry.ratio = p_dbl(0.1, 1),
++     regr.ranger.splitrule  = p_fct(levels = c("variance", "extratrees")))
++)
++# regr.ranger.min.node.size   = p_int(1, 20), # Adjust the range as needed
++# regr.ranger.sample.fraction = p_dbl(0.1, 1),
++
++# xgboost graph
++graph_xgboost = graph_template %>>%
++  po("learner", learner = lrn("regr.xgboost"))
++plot(graph_xgboost)
++graph_xgboost = as_learner(graph_xgboost)
++as.data.table(graph_xgboost$param_set)[grep("depth", id), .(id, class, lower, upper, levels)]
++search_space_xgboost = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.xgboost.alpha     = p_dbl(0.001, 100, logscale = TRUE),
++  regr.xgboost.max_depth = p_int(1, 20),
++  regr.xgboost.eta       = p_dbl(0.0001, 1, logscale = TRUE),
++  regr.xgboost.nrounds   = p_int(1, 5000),
++  regr.xgboost.subsample = p_dbl(0.1, 1)
++)
++
++# gbm graph
++graph_gbm = graph_template %>>%
++  po("learner", learner = lrn("regr.gbm"))
++plot(graph_gbm)
++graph_gbm = as_learner(graph_gbm)
++as.data.table(graph_gbm$param_set)[, .(id, class, lower, upper, levels)]
++search_space_gbm = search_space_template$clone()
++search_space_gbm$add(
++  ps(regr.gbm.distribution      = p_fct(levels = c("gaussian", "tdist")),
++     regr.gbm.shrinkage         = p_dbl(lower = 0.001, upper = 0.1),
++     regr.gbm.n.trees           = p_int(lower = 50, upper = 200),
++     regr.gbm.interaction.depth = p_int(lower = 1, upper = 4))
++  # ....
++)
++
++# catboost graph
++graph_catboost = graph_template %>>%
++  po("learner", learner = lrn("regr.catboost"))
++graph_catboost = as_learner(graph_catboost)
++as.data.table(graph_catboost$param_set)[, .(id, class, lower, upper, levels)]
++search_space_catboost = search_space_template$clone()
++# https://catboost.ai/en/docs/concepts/parameter-tuning#description10
++search_space_catboost$add(
++  ps(regr.catboost.learning_rate   = p_dbl(lower = 0.01, upper = 0.3),
++     regr.catboost.depth           = p_int(lower = 4, upper = 10),
++     regr.catboost.l2_leaf_reg     = p_int(lower = 1, upper = 5),
++     regr.catboost.random_strength = p_int(lower = 0, upper = 3))
++)
++
++# # gamboost graph
++# # Error in eval(predvars, data, env) : object 'adxDx14' not found
++# # This happened PipeOp regr.gamboost's $train()
++# # In addition: There were 50 or more warnings (use warnings() to see the first 50)
++# graph_gamboost = graph_template %>>%
++#   po("learner", learner = lrn("regr.gamboost"))
++# graph_gamboost = as_learner(graph_gamboost)
++# as.data.table(graph_gamboost$param_set)[, .(id, class, lower, upper, levels)]
++# search_space_gamboost = search_space_template$clone()
++# search_space_gamboost$add(
++#   ps(regr.gamboost.mstop       = p_int(lower = 10, upper = 100),
++#      regr.gamboost.nu          = p_dbl(lower = 0.01, upper = 0.5),
++#      regr.gamboost.baselearner = p_fct(levels = c("bbs", "bols", "btree")))
++# )
++
++# kknn graph
++graph_kknn = graph_template %>>%
++  po("learner", learner = lrn("regr.kknn"))
++graph_kknn = as_learner(graph_kknn)
++as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
++search_space_kknn = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(
++        x,
++        "0.80" = 0.80,
++        "0.90" = 0.90,
++        "0.95" = 0.95,
++        "0.99" = 0.99
++      )
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.kknn.k        = p_int(
++    lower = 1,
++    upper = 50,
++    logscale = TRUE
++  ),
++  regr.kknn.distance = p_dbl(lower = 1, upper = 5),
++  regr.kknn.kernel   = p_fct(
++    levels = c(
++      "rectangular",
++      "optimal",
++      "epanechnikov",
++      "biweight",
++      "triweight",
++      "cos",
++      "inv",
++      "gaussian",
++      "rank"
++    )
++  )
++)
++
++# nnet graph
++graph_nnet = graph_template %>>%
++  po("learner", learner = lrn("regr.nnet", MaxNWts = 40000))
++graph_nnet = as_learner(graph_nnet)
++as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
++search_space_nnet = search_space_template$clone()
++search_space_nnet$add(
++  ps(regr.nnet.size  = p_int(lower = 5, upper = 30),
++     regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
++     regr.nnet.maxit = p_int(lower = 50, upper = 500))
++)
++
++# glmnet graph
++graph_glmnet = graph_template %>>%
++  po("learner", learner = lrn("regr.glmnet"))
++graph_glmnet = as_learner(graph_glmnet)
++as.data.table(graph_glmnet$param_set)[, .(id, class, lower, upper, levels)]
++search_space_glmnet = ps(
++  # subsample for hyperband
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.glmnet.s     = p_int(lower = 5, upper = 30),
++  regr.glmnet.alpha = p_dbl(lower = 1e-4, upper = 1e4, logscale = TRUE)
++)
++
++
++### THIS LEARNER UNSTABLE ####
++# ksvm graph
++# graph_ksvm = graph_template %>>%
++#   po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
++# graph_ksvm = as_learner(graph_ksvm)
++# as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
++# search_space_ksvm = ps(
++#   # subsample for hyperband
++#   subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++#   # preprocessing
++#   dropcorr.cutoff = p_fct(
++#     levels = c("0.80", "0.90", "0.95", "0.99"),
++#     trafo = function(x, param_set) {
++#       switch(x,
++#              "0.80" = 0.80,
++#              "0.90" = 0.90,
++#              "0.95" = 0.95,
++#              "0.99" = 0.99)
++#     }
++#   ),
++#   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++#   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++#   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++#   # filters
++#   filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++#   # interaction
++#   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++#   # learner
++#   regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
++#                                        "laplacedot", "besseldot", "anovadot")),
++#   regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
++#   regr.ksvm.degree  = p_int(lower = 1, upper = 5,
++#                             depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
++#   regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
++# )
++### THIS LEARNER UNSTABLE ####
++
++# LAST
++# lightgbm graph
++# [LightGBM] [Fatal] Do not support special JSON characters in feature name.
++graph_template =
++  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
++  po("dropna", id = "dropna") %>>%
++  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
++  po("fixfactors", id = "fixfactors") %>>%
++  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
++  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
++  # scale branch
++  po("branch", options = c("uniformization", "scale"), id = "scale_branch") %>>%
++  gunion(list(po("uniformization"),
++              po("scale")
++  )) %>>%
++  po("unbranch", id = "scale_unbranch") %>>%
++  po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
++  # filters
++  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
++  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
++              po("filter", filter = flt("relief"), filter.frac = 0.05),
++              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
++  )) %>>%
++  po("unbranch", id = "filter_unbranch")
++search_space_template = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov"))
++)
++graph_lightgbm = graph_template %>>%
++  po("learner", learner = lrn("regr.lightgbm"))
++graph_lightgbm = as_learner(graph_lightgbm)
++as.data.table(graph_lightgbm$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
++search_space_lightgbm = search_space_template$clone()
++search_space_lightgbm$add(
++  ps(regr.lightgbm.max_depth     = p_int(lower = 2, upper = 10),
++     regr.lightgbm.learning_rate = p_dbl(lower = 0.001, upper = 0.3),
++     regr.lightgbm.num_leaves    = p_int(lower = 10, upper = 100))
++)
++
++# earth graph
++graph_earth = graph_template %>>%
++  po("learner", learner = lrn("regr.earth"))
++graph_earth = as_learner(graph_earth)
++as.data.table(graph_earth$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
++search_space_earth = search_space_template$clone()
++search_space_earth$add(
++  ps(regr.earth.degree  = p_int(lower = 1, upper = 4),
++     # regr.earth.penalty = p_int(lower = 1, upper = 5),
++     regr.earth.nk      = p_int(lower = 50, upper = 250))
++)
++
++# rsm graph
++graph_rsm = graph_template %>>%
++  po("learner", learner = lrn("regr.rsm"))
++plot(graph_rsm)
++graph_rsm = as_learner(graph_rsm)
++as.data.table(graph_rsm$param_set)[, .(id, class, lower, upper, levels)]
++search_space_rsm = search_space_template$clone()
++search_space_rsm$add(
++  ps(regr.rsm.modelfun = p_fct(levels = c("FO", "TWI", "SO")))
++)
++# Error in fo[, i] * fo[, j] : non-numeric argument to binary operator
++# This happened PipeOp regr.rsm's $train()
++# Calls: lapply ... resolve.list -> signalConditionsASAP -> signalConditions
++# In addition: Warning message:
++# In bbandsDn5:volumeDownUpRatio :
++#   numerical expression has 4076 elements: only the first used
++# This happened PipeOp regr.rsm's $train()
++
++# BART graph
++graph_bart = graph_template %>>%
++  po("learner", learner = lrn("regr.bart"))
++graph_bart = as_learner(graph_bart)
++as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
++search_space_bart = search_space_template$clone()
++search_space_bart$add(
++  ps(regr.bart.k      = p_int(lower = 1, upper = 10),
++     regr.bart.numcut = p_int(lower = 10, upper = 200),
++     regr.bart.ntree  = p_int(lower = 50, upper = 500))
++)
++# chatgpt returns this
++# n_chains = p_int(lower = 1, upper = 5),
++# m_try = p_int(lower = 1, upper = 13),
++# nu = p_dbl(lower = 0.1, upper = 10),
++# alpha = p_dbl(lower = 0.01, upper = 1),
++# beta = p_dbl(lower = 0.01, upper = 1),
++# burn = p_int(lower = 10, upper = 100),
++# iter = p_int(lower = 100, upper = 1000)
++
++# threads
++threads = 4
++set_threads(graph_rf, n = threads)
++set_threads(graph_xgboost, n = threads)
++# set_threads(graph_bart, n = threads)
++# set_threads(graph_ksvm, n = threads) # unstable
++set_threads(graph_nnet, n = threads)
++set_threads(graph_kknn, n = threads)
++set_threads(graph_lightgbm, n = threads)
++set_threads(graph_earth, n = threads)
++set_threads(graph_gbm, n = threads)
++set_threads(graph_rsm, n = threads)
++set_threads(graph_catboost, n = threads)
++set_threads(graph_glmnet, n = threads)
++
++
++# DESIGNS -----------------------------------------------------------------
++designs_l = lapply(custom_cvs, function(cv_) {
++  # debug
++  # cv_ = custom_cvs[[1]]
++
++  # get cv inner object
++  cv_inner = cv_$custom_inner
++  cv_outer = cv_$custom_outer
++  cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
++
++  designs_cv_l = lapply(1:cv_inner$iters, function(i) {
++    # debug
++    # i = 1
++
++    # inner resampling
++    custom_ = rsmp("custom")
++    custom_$id = paste0("custom_", cv_inner$iters, "_", i)
++    custom_$instantiate(task_ret_week,
++                        list(cv_inner$train_set(i)),
++                        list(cv_inner$test_set(i)))
++
++    # objects for all autotuners
++    measure_ = msr("portfolio_ret")
++    tuner_   = tnr("hyperband", eta = 4)
++    # tuner_   = tnr("mbo")
++    # term_evals = 20
++
++    # auto tuner rf
++    at_rf = auto_tuner(
++      tuner = tuner_,
++      learner = graph_rf,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_rf,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner xgboost
++    at_xgboost = auto_tuner(
++      tuner = tuner_,
++      learner = graph_xgboost,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_xgboost,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner BART
++    at_bart = auto_tuner(
++      tuner = tuner_,
++      learner = graph_bart,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_bart,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner nnet
++    at_nnet = auto_tuner(
++      tuner = tuner_,
++      learner = graph_nnet,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_nnet,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner lightgbm
++    at_lightgbm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_lightgbm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_lightgbm,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner earth
++    at_earth = auto_tuner(
++      tuner = tuner_,
++      learner = graph_earth,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_earth,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner kknn
++    at_kknn = auto_tuner(
++      tuner = tuner_,
++      learner = graph_kknn,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_kknn,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner gbm
++    at_gbm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_gbm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_gbm,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner rsm
++    at_rsm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_rsm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_rsm,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner rsm
++    at_bart = auto_tuner(
++      tuner = tuner_,
++      learner = graph_bart,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_bart,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner catboost
++    at_catboost = auto_tuner(
++      tuner = tuner_,
++      learner = graph_catboost,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_catboost,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # auto tuner glmnet
++    at_glmnet = auto_tuner(
++      tuner = tuner_,
++      learner = graph_glmnet,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_glmnet,
++      terminator = trm("none")
++      # term_evals = term_evals
++    )
++
++    # outer resampling
++    customo_ = rsmp("custom")
++    customo_$id = paste0("custom_", cv_inner$iters, "_", i)
++    customo_$instantiate(task_ret_week, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
++
++    # nested CV for one round
++    design = benchmark_grid(
++      tasks = task_ret_week,
++      learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth,
++                      at_kknn, at_gbm, at_rsm, at_bart, at_catboost, at_glmnet),
++      resamplings = customo_
++    )
++  })
++  designs_cv = do.call(rbind, designs_cv_l)
++})
++designs = do.call(rbind, designs_l)
++
++# create registry
++print("Create registry")
++packages = c("data.table", "gausscov", "paradox", "mlr3", "mlr3pipelines",
++             "mlr3tuning", "mlr3misc", "future", "future.apply",
++             "mlr3extralearners", "stats")
++reg = makeExperimentRegistry(
++  file.dir = "./experiments2",
++  seed = 1,
++  packages = packages
++)
++
++# populate registry with problems and algorithms to form the jobs
++print("Batchmark")
++batchmark(designs, reg = reg)
++
++# save registry
++print("Save registry")
++saveRegistry(reg = reg)
++
++# create sh file
++sh_file = sprintf("
++#!/bin/bash
++
++#PBS -N PEAD
++#PBS -l ncpus=4
++#PBS -l mem=10GB
++#PBS -J 1-%d
++#PBS -o experiments/logs
++#PBS -j oe
++
++cd ${PBS_O_WORKDIR}
++apptainer run image.sif run_job.R
++", nrow(designs))
++sh_file_name = "run_ml.sh"
++file.create(sh_file_name)
++writeLines(sh_file, sh_file_name)
+diff --git a/run_padobran_week.sh b/run_padobran_week.sh
+new file mode 100644
+index 0000000..0295549
+--- /dev/null
++++ b/run_padobran_week.sh
+@@ -0,0 +1,7 @@
++#!/bin/bash
++
++#PBS -N PEADPREPARE
++#PBS -l mem=35GBs
++
++cd ${sPBS_O_WORKDIR}
++apptainer run image.sif run_padobran_week.R
+
+c2ae845ed9099a24dd2f70b9f5436f97a959f419 unknown Mon Nov 6 11:01:29 2023 +0100 add glmnet learner, add also to imadedef, add new, updated dataset
+diff --git a/run_padobran.R b/run_padobran.R
+index 1a69c3b..91df16f 100644
+--- a/run_padobran.R
++++ b/run_padobran.R
+@@ -51,7 +51,7 @@ snakeToCamel <- function(snake_str) {
+ print("Prepare data")
+ 
+ # read predictors
+-data_tbl <- fread("D:/features/pead-predictors-20231031.csv")
++data_tbl <- fread("pead-predictors-20231031.csv")
+ 
+ # convert tibble to data.table
+ DT = as.data.table(data_tbl)
+
+e5219cebfd07e292ae5ce80429381332f21e4b57 unknown Mon Nov 6 11:00:57 2023 +0100 add glmnet learner, add also to imadedef, add new, updated dataset
+diff --git a/download.R b/download.R
+index baa6bb6..2a7266d 100644
+--- a/download.R
++++ b/download.R
+@@ -5,6 +5,4 @@ blob_key = readLines('./blob_key.txt')
+ endpoint = "https://snpmarketdata.blob.core.windows.net/"
+ BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+ cont = storage_container(BLOBENDPOINT, "jphd")
+-storage_download(cont, "pead-predictors.csv", overwrite=TRUE) # uncomment this for full dataset
+-storage_download(cont, "pead-predictors-update.csv", overwrite=TRUE) # uncomment this for full dataset
+-storage_download(cont, "pead-predictors-sample.csv", overwrite=TRUE)
++storage_download(cont, "pead-predictors-20231031.csv", overwrite=TRUE)
+diff --git a/generate_predictors.R b/generate_predictors.R
+index 7f3abe4..11465a8 100644
+--- a/generate_predictors.R
++++ b/generate_predictors.R
+@@ -2,7 +2,7 @@ options(progress_enabled = FALSE)
+ 
+ library(data.table)
+ library(checkmate)
+-library(tiledb)
++library(arrow)
+ library(httr)
+ library(fredr)
+ library(alfred)
+@@ -16,6 +16,8 @@ library(DescTools)
+ library(reticulate)
+ library(findata)
+ library(AzureStor)
++library(fs)
++library(duckdb)
+ # Python environment and python modules
+ # Instructions: some functions use python modules. Steps to use python include:
+ # 1. create new conda environment:
+@@ -38,37 +40,35 @@ warnigns = reticulate::import("warnings", convert = FALSE)
+ warnigns$filterwarnings('ignore')
+ 
+ 
+-
+ # SET UP ------------------------------------------------------------------
+ # check if we have all necessary env variables
+-assert_choice("AWS-ACCESS-KEY", names(Sys.getenv()))
+-assert_choice("AWS-SECRET-KEY", names(Sys.getenv()))
+-assert_choice("AWS-REGION", names(Sys.getenv()))
++# assert_choice("AWS-ACCESS-KEY", names(Sys.getenv()))
++# assert_choice("AWS-SECRET-KEY", names(Sys.getenv()))
++# assert_choice("AWS-REGION", names(Sys.getenv()))
+ assert_choice("BLOB-ENDPOINT", names(Sys.getenv()))
+ assert_choice("BLOB-KEY", names(Sys.getenv()))
+ assert_choice("APIKEY-FMPCLOUD", names(Sys.getenv()))
+ assert_choice("FRED-KEY", names(Sys.getenv()))
+ 
+-# set credentials
+-config <- tiledb_config()
+-config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
+-config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
+-config["vfs.s3.region"] <- Sys.getenv("AWS-REGION")
+-context_with_config <- tiledb_ctx(config)
+-fredr_set_key(Sys.getenv("FRED-KEY"))
++# # set credentials
++# config <- tiledb_config()
++# config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
++# config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
++# config["vfs.s3.region"] <- Sys.getenv("AWS-REGION")
++# context_with_config <- tiledb_ctx(config)
++# fredr_set_key(Sys.getenv("FRED-KEY"))
++
++# global vars
++PATH = "F:/equity/usa"
+ 
+ # parameters
+ strategy = "PEAD"  # PEAD (for predicting post announcement drift) or PRE (for predicting pre announcement)
+ events_data <- "intersection" # data source, one of "fmp", "investingcom", "intersection"
+ 
+ 
+-
+ # EARING ANNOUNCEMENT DATA ------------------------------------------------
+-# get events data from FMP
+-arr <- tiledb_array("s3://equity-usa-earningsevents", as.data.frame = TRUE)
+-events <- arr[]
+-events <- as.data.table(events)
+-setorder(events, date)
++# get events data
++events = read_parquet(fs::path(PATH, "fundamentals", "earning_announcements", ext = "parquet"))
+ 
+ # coarse filtering
+ events <- events[date < Sys.Date()]                 # remove announcements for today
+@@ -99,10 +99,10 @@ print(paste0("Remove ", nrow(events[!(symbol %in% us_symbols)]),
+ events <- events[symbol %in% us_symbols] # keep only US stocks
+ 
+ # get investing.com data
+-arr <- tiledb_array("s3://equity-usa-earningsevents-investingcom",
+-                    as.data.frame = TRUE)
+-investingcom_ea <- arr[]
+-investingcom_ea <- as.data.table(investingcom_ea)
++investingcom_ea = read_parquet(fs::path(PATH,
++                                        "fundamentals",
++                                        "earning_announcements_investingcom",
++                                        ext = "parquet"))
+ if (strategy == "PEAD") {
+   investingcom_ea <- na.omit(investingcom_ea, cols = c("eps", "eps_forecast"))
+ }
+@@ -152,30 +152,43 @@ if (events_data == "intersection") {
+ # remove duplicated events
+ events <- unique(events, by = c("symbol", "date"))
+ 
++# check last date
++events[, max(date)]
++events[date == max(date), symbol]
++
+ 
+ # MARKET DATA AND FUNDAMENTALS ---------------------------------------------
+-# import market data and fundamentals
+-factors = Factors$new()
+-factors_l = factors$get_factors()
+-price_factors <- factors_l$prices_factos
+-fundamental_factors <- factors_l$fundamental_factors
+-macro <- factors_l$macro
+-
+-# free resources
+-rm(factors_l)
+-gc()
++# import factors
++prices_dt = read_parquet(fs::path(PATH,
++                                  "predictors-daily",
++                                  "factors",
++                                  "prices_factors",
++                                  ext = "parquet"))
+ 
+ # filter dates and symbols
+-prices_dt <- unique(price_factors, by = c("symbol", "date"))
+-setorder(prices_dt, symbol, date)
++prices_dt <- prices_dt[date > as.Date("2009-01-01")]
+ prices_dt <- prices_dt[symbol %in% c(unique(events$symbol), "SPY")]
+-prices_dt <- prices_dt[date > as.Date("2010-01-01")]
++prices_dt <- unique(prices_dt, by = c("symbol", "date"))
++setorder(prices_dt, symbol, date)
+ prices_n <- prices_dt[, .N, by = symbol]
+ prices_n <- prices_n[which(prices_n$N > 700)]  # remove prices with only 700 or less observations
+ prices_dt <- prices_dt[symbol %in% prices_n$symbol]
+ 
+-# save SPY for later and keep only events symbols
+-spy <- prices_dt[symbol == "SPY", .(symbol, date, open, high, low, close, volume, returns)]
++# SPY data
++con <- dbConnect(duckdb::duckdb())
++symbol = "SPY"
++query <- sprintf("
++  SELECT *
++  FROM 'F:/equity/daily_fmp_all.csv'
++  WHERE Symbol = '%s'
++", symbol)
++data_ <- dbGetQuery(con, query)
++dbDisconnect(con)
++data_ = as.data.table(data_)
++data_ = data_[, .(date = date, close = adjClose)]
++data_[, returns := close / shift(close) - 1]
++spy = na.omit(data_)
++
+ 
+ 
+ # REGRESSION LABELING ----------------------------------------------------------
+@@ -195,10 +208,10 @@ prices_dt[, sd_44 := roll::roll_sd(close / shift(close, 1L) - 1, 44), by = "symb
+ prices_dt[, sd_66 := roll::roll_sd(close / shift(close, 1L) - 1, 66), by = "symbol"]
+ 
+ # calculate spy returns
+-spy[, ret_5_spy := shift(close, -5L, "shift") / shift(close, -1L, "shift") - 1, by = "symbol"]
+-spy[, ret_22_spy := shift(close, -21L, "shift") / shift(close, -1L, "shift") - 1, by = "symbol"]
+-spy[, ret_44_spy := shift(close, -43L, "shift") / shift(close, -1L, "shift") - 1, by = "symbol"]
+-spy[, ret_66_spy := shift(close, -65L, "shift") / shift(close, -1L, "shift") - 1, by = "symbol"]
++spy[, ret_5_spy := shift(close, -5L, "shift") / shift(close, -1L, "shift") - 1]
++spy[, ret_22_spy := shift(close, -21L, "shift") / shift(close, -1L, "shift") - 1]
++spy[, ret_44_spy := shift(close, -43L, "shift") / shift(close, -1L, "shift") - 1]
++spy[, ret_66_spy := shift(close, -65L, "shift") / shift(close, -1L, "shift") - 1]
+ 
+ # calculate excess returns
+ prices_dt <- merge(prices_dt,
+@@ -229,14 +242,15 @@ prices_dt[, `:=`(ret_5 = NULL, ret_22 = NULL, ret_44 = NULL, ret_66 = NULL,
+ #                                          "ret_excess_stand_66"))
+ 
+ 
+-
+ # MERGE MARKET DATA, EVENTS AND CLASSIF LABELS ---------------------------------
+ # merge clf_data and labels
+-dataset <- merge(events,
+-                 prices_dt[, .(symbol, date,
+-                               ret_excess_stand_5, ret_excess_stand_22,
+-                               ret_excess_stand_44, ret_excess_stand_66,
+-                               amc_return, bmo_return)],
++cols_ = colnames(prices_dt)
++cols_keep_prices = c(
++  "symbol", "date", "ret_excess_stand_5", "ret_excess_stand_22",
++  "ret_excess_stand_44", "ret_excess_stand_66", "amc_return", "bmo_return",
++  cols_[which(cols_ == "e"):(which(cols_ == "amc_return")-1)]
++)
++dataset <- merge(events, prices_dt[, ..cols_keep_prices],
+                  by = c("symbol", "date"), all.x = TRUE, all.y = FALSE)
+ 
+ # extreme labeling
+@@ -286,7 +300,6 @@ dataset[, (bin_decile_col_names) := lapply(.SD, function(x) {
+ setorderv(dataset, c("symbol", "date"))
+ 
+ 
+-
+ # FEATURES ----------------------------------------------------------------
+ # Ohlcv feaures
+ OhlcvInstance = Ohlcv$new(prices_dt[, .(symbol, date, open, high, low, close, volume)],
+@@ -299,11 +312,6 @@ if (strategy == "PEAD") {
+   # ako je red u events bmo. label je open_t / close_t-1; lag je -2L
+ }
+ 
+-# free memory
+-rm(prices_events)
+-gc()
+-
+-
+ # util function that returns most recently saved predictor object
+ get_latest = function(predictors = "RollingExuberFeatures") {
+   f = file.info(list.files("D:/features", full.names = TRUE, pattern = predictors))
+@@ -480,7 +488,7 @@ if (length(at_) > 0) {
+   RollingTheftCatch22Features[, c("feasts____22_5", "feasts____25_22") := NULL]
+   # cols = colnames(RollingTheftCatch22Features)
+   # RollingTheftCatch22FeaturesNew = RollingTheftCatch22FeaturesNew[, ..cols]
+-  RollingTheftCatch22FeaturesNewMerged = rbind(RollingTheftCatch22Features, RollingTheftCatch22FeaturesNew)
++  RollingTheftCatch22FeaturesNewMerged = rbind(RollingTheftCatch22Features, RollingTheftCatch22FeaturesNew, fill = TRUE)
+   time_ <- format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+   fwrite(RollingTheftCatch22FeaturesNewMerged, paste0("D:/features/PEAD-RollingTheftCatch22Features-", time_, ".csv"))
+ }
+@@ -563,22 +571,21 @@ if (length(at_) > 0) {
+ # Fracdiff
+ print("Fradiff predictors")
+ at_ = get_at_(RollingFracdiffFeatures)
+-at_ = ifelse(at_)
+ if (length(at_) > 0) {
+-  RollingWaveletArimaInstance = RollingWaveletArima$new(windows = 252, workers = 6L,
+-                                                        lag = lag_, at = at_, filter = "haar")
+-  RollingWaveletArimaFeaturesNew = RollingWaveletArimaInstance$get_rolling_features(OhlcvInstance)
++  RollingFracdiffInstance = RollingFracdiff$new(windows = 252, workers = 6L,
++                                                lag = lag_, at = at_,
++                                                nar = c(1, 2), nma = c(1, 2),
++                                                bandw_exp = c(0.1, 0.5, 0.9))
++  RollingFracdiffFeaturesNew = RollingFracdiffInstance$get_rolling_features(OhlcvInstance)
+   gc()
+ 
+   # save
+-  RollingWaveletArimaFeaturesNew[, date := as.IDate(date)]
+-  RollingWaveletArimaFeaturesNewMerged = rbind(RollingWaveletArimaFeatures, RollingWaveletArimaFeaturesNew)
++  RollingFracdiffFeaturesNew[, date := as.IDate(date)]
++  RollingFracdiffFeaturesNewMerged = rbind(RollingFracdiffFeatures, RollingFracdiffFeaturesNew)
+   time_ <- format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+-  fwrite(RollingWaveletArimaFeaturesNewMerged, paste0("D:/features/PEAD-RollingWaveletArimaFeatures-", time_, ".csv"))
++  fwrite(RollingFracdiffFeaturesNewMerged, paste0("D:/features/PEAD-RollingWaveletArimaFeatures-", time_, ".csv"))
+ }
+ 
+-
+-
+ # prepare arguments for features
+ prices_events <- merge(prices_dt, dataset[, .(symbol, date, eps)],
+                        by = c("symbol", "date"), all.x = TRUE, all.y = FALSE)
+@@ -593,6 +600,9 @@ OhlcvFeaturesSet = OhlcvFeaturesInit$get_ohlcv_features(OhlcvInstance)
+ OhlcvFeaturesSetSample <- OhlcvFeaturesSet[at_ - lag_]
+ setorderv(OhlcvFeaturesSetSample, c("symbol", "date"))
+ # DEBUG
++events[date == max(date)]
++events[date == max(date), symbol]
++OhlcvFeaturesSet[symbol %in% events[date == max(date), symbol]]
+ head(dataset[, .(symbol, date)])
+ head(OhlcvFeaturesSetSample[symbol == "A", .(symbol, date)])
+ tail(dataset[, .(symbol, date)], 10)
+@@ -602,11 +612,10 @@ OhlcvFeaturesSetSample[symbol == "ZYXI", .(symbol, date)]
+ rm(OhlcvFeaturesSet)
+ gc()
+ 
+-# save Ohlcv data
+-time_ <- format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+-fwrite(OhlcvFeaturesSetSample, paste0("D:/features/PEAD-OhlcvFeaturesSetSample-", time_, ".csv"))
+-
+-
++# THINK THIS IS NOT NECESSARY
++# # save Ohlcv data
++# time_ <- format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
++# fwrite(OhlcvFeaturesSetSample, paste0("D:/features/PEAD-OhlcvFeaturesSetSample-", time_, ".csv"))
+ 
+ # util function that returns most recently saved predictor object
+ get_latest = function(predictors = "RollingExuberFeatures") {
+@@ -616,7 +625,7 @@ get_latest = function(predictors = "RollingExuberFeatures") {
+ }
+ 
+ # import all saved predictors
+-OhlcvFeaturesSetSample = fread(get_latest("OhlcvFeaturesSetSample"))
++# OhlcvFeaturesSetSample = fread(get_latest("OhlcvFeaturesSetSample"))
+ RollingBidAskFeatures = fread(get_latest("RollingBidAskFeatures"))
+ RollingBackCusumFeatures = fread(get_latest("RollingBackCusumFeatures"))
+ RollingExuberFeatures = fread(get_latest("RollingExuberFeatures"))
+@@ -626,7 +635,8 @@ RollingTheftCatch22Features = fread(get_latest("RollingTheftCatch22Features"))
+ RollingTheftTsfelFeatures = fread(get_latest("RollingTheftTsfelFeatures"))
+ RollingTsfeaturesFeatures = fread(get_latest("RollingTsfeaturesFeatures"))
+ # RollingQuarksFeatures = fread(get_latest("RollingQuarksFeatures"))
+-RollingWaveletArimaFeatures = fread(get_latest("RollingWaveletArimaFeatures"))
++# RollingWaveletArimaFeatures = fread(get_latest("RollingWaveletArimaFeatures")) # TODO: add i next itertion
++# RollingFracdiffFeatures = fread(get_latest("RollingFracdiffFeatures")) # TODO: add i next itertion
+ 
+ # merge all features test
+ rolling_predictors <- Reduce(
+@@ -640,9 +650,10 @@ rolling_predictors <- Reduce(
+     RollingGpdFeatures,
+     RollingTheftCatch22Features,
+     RollingTheftTsfelFeatures,
+-    RollingTsfeaturesFeatures,
++    RollingTsfeaturesFeatures
+     # RollingQuarksFeatures,
+-    RollingWaveletArimaFeatures
++    # RollingWaveletArimaFeatures # TODO Add this in next iteration
++    # RollingFracdiffFeatures     # TODO Add this in next iteration
+   )
+ )
+ 
+@@ -663,6 +674,9 @@ rolling_predictors[, date_rolling := date]
+ OhlcvFeaturesSetSample[, date_ohlcv := date]
+ features <- rolling_predictors[OhlcvFeaturesSetSample, on = c("symbol", "date"), roll = Inf]
+ 
++# check last date
++features[, max(date)]
++
+ # check for duplicates
+ features[duplicated(features[, .(symbol, date)]), .(symbol, date)]
+ features[duplicated(features[, .(symbol, date_ohlcv)]), .(symbol, date_ohlcv)]
+@@ -695,8 +709,17 @@ features[, `:=`(
+   eps_diff = (eps - epsEstimated + 0.00001) / (epsEstimated + 0.00001)
+ )]
+ 
++# import fundamnetal fators
++fundamentals = read_parquet(fs::path(PATH,
++                                     "predictors-daily",
++                                     "factors",
++                                     "fundamental_factors",
++                                     ext = "parquet"))
++
+ # clean fundamentals
+-fundamentals <- fundamental_factors[date > as.Date("2009-01-01")]
++fundamentals = fundamentals[date > as.Date("2009-01-01")]
++fundamentals[, acceptedDateTime := as.POSIXct(acceptedDate, tz = "America/New_York")]
++fundamentals[, acceptedDate := as.Date(acceptedDateTime)]
+ fundamentals[, acceptedDateFundamentals := acceptedDate]
+ data.table::setnames(fundamentals, "date", "fundamental_date")
+ fundamentals <- unique(fundamentals, by = c("symbol", "acceptedDate"))
+@@ -707,49 +730,61 @@ features = fundamentals[features, on = c("symbol", "acceptedDate" = "date_ohlcv"
+ features[, .(symbol, acceptedDate, acceptedDateTime, date_day_after_event, date)]
+ 
+ # remove unnecesary columns
+-features[, `:=`(period = NULL, link = NULL, finalLink = NULL, reportedCurrency = NULL)]
++features[, `:=`(period = NULL, link = NULL, finalLink = NULL,
++                reportedCurrency = NULL, cik = NULL, calendarYear = NULL)]
+ features[symbol == "AAPL", .(symbol, fundamental_date, acceptedDate,
+                              acceptedDateFundamentals, date_day_after_event, date)]
+ 
+ # convert char features to numeric features
+ char_cols <- features[, colnames(.SD), .SDcols = is.character]
+-char_cols <- setdiff(char_cols, c("symbol", "time", "right_time"))
++char_cols <- setdiff(char_cols, c("symbol", "time", "right_time", "industry", "sector"))
+ features[, (char_cols) := lapply(.SD, as.numeric), .SDcols = char_cols]
+ 
++
++
++############# ADD TRANSCRIPTS #################
+ # import transcripts sentiments datadata
+-config <- tiledb_config()
+-config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
+-config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
+-config["vfs.s3.region"] <- "us-east-1"
+-context_with_config <- tiledb_ctx(config)
+-arr <- tiledb_array("s3://equity-transcripts-sentiments",
+-                    as.data.frame = TRUE,
+-                    query_layout = "UNORDERED",
+-)
+-system.time(transcript_sentiments <- arr[])
+-tiledb_array_close(arr)
+-sentiments_dt <- as.data.table(transcript_sentiments)
+-setnames(sentiments_dt, "date", "time_transcript")
+-attr(sentiments_dt$time, "tz") <- "UTC"
+-sentiments_dt[, date := as.Date(time)]
+-sentiments_dt[, time := NULL]
+-cols_sentiment = colnames(sentiments_dt)[grep("FLS", colnames(sentiments_dt))]
++# config <- tiledb_config()
++# config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
++# config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
++# config["vfs.s3.region"] <- "us-east-1"
++# context_with_config <- tiledb_ctx(config)
++# arr <- tiledb_array("s3://equity-transcripts-sentiments",
++#                     as.data.frame = TRUE,
++#                     query_layout = "UNORDERED",
++# )
++# system.time(transcript_sentiments <- arr[])
++# tiledb_array_close(arr)
++# sentiments_dt <- as.data.table(transcript_sentiments)
++# setnames(sentiments_dt, "date", "time_transcript")
++# attr(sentiments_dt$time, "tz") <- "UTC"
++# sentiments_dt[, date := as.Date(time)]
++# sentiments_dt[, time := NULL]
++# cols_sentiment = colnames(sentiments_dt)[grep("FLS", colnames(sentiments_dt))]
+ 
+ # merge with features
+-features[, date_day_after_event_ := date_day_after_event]
+-features <- sentiments_dt[features, on = c("symbol", "date" = "date_day_after_event_"), roll = Inf]
+-features[, .(symbol, date, date_day_after_event, time_transcript, Not_FLS_positive)]
+-features[1:50, .(symbol, date, date_day_after_event, time_transcript, Not_FLS_positive)]
++# features[, date_day_after_event_ := date_day_after_event]
++# features <- sentiments_dt[features, on = c("symbol", "date" = "date_day_after_event_"), roll = Inf]
++# features[, .(symbol, date, date_day_after_event, time_transcript, Not_FLS_positive)]
++# features[1:50, .(symbol, date, date_day_after_event, time_transcript, Not_FLS_positive)]
+ 
+ # remove observations where transcripts are more than 2 days away
+-features <- features[date - as.IDate(as.Date(time_transcript)) >= 3,
+-                     (cols_sentiment) := NA]
+-features[, ..cols_sentiment]
++# features <- features[date - as.IDate(as.Date(time_transcript)) >= 3,
++#                      (cols_sentiment) := NA]
++# features[, ..cols_sentiment]
++############# ADD TRANSCRIPTS ###############
++
++# import macro factors
++macros = read_parquet(fs::path(PATH,
++                               "predictors-daily",
++                               "factors",
++                               "macro_factors",
++                               ext = "parquet"))
+ 
+ # macro data
+ features[, date_day_after_event_ := date_day_after_event]
+-macro[, date_macro := date]
+-features <- macro[features, on = c("date" = "date_day_after_event_"), roll = Inf]
++macros[, date_macro := date]
++features <- macros[features, on = c("date" = "date_day_after_event_"), roll = Inf]
+ features[, .(symbol, date, date_day_after_event, date_macro, vix)]
+ 
+ # final checks for predictors
+@@ -758,7 +793,6 @@ features[duplicated(features[, .(symbol, date_day_after_event)]), .(symbol, date
+ features[duplicated(features[, .(symbol, date)]), .(symbol, date)]
+ 
+ 
+-
+ # FEATURES SPACE ----------------------------------------------------------
+ # features space from features raw
+ cols_remove <- c("trading_date_after_event", "time", "datetime_investingcom",
+@@ -772,7 +806,9 @@ cols_remove <- c("trading_date_after_event", "time", "datetime_investingcom",
+                  "same_announce_time", "eps", "epsEstimated", "revenue", "revenueEstimated",
+                  "same_announce_time", "time_transcript", "i.time",
+                  # remove dates we don't need
+-                 setdiff(colnames(features)[grep("date", colnames(features), ignore.case = TRUE)], c("date", "date_rolling"))
++                 setdiff(colnames(features)[grep("date", colnames(features), ignore.case = TRUE)], c("date", "date_rolling")),
++                 # remove columns with i - possible duplicates
++                 colnames(features)[grep("i\\.|\\.y", colnames(features))]
+                  )
+ cols_non_features <- c("symbol", "date", "date_rolling", "time", "right_time",
+                        "ret_excess_stand_5", "ret_excess_stand_22", "ret_excess_stand_44", "ret_excess_stand_66",
+@@ -780,8 +816,8 @@ cols_non_features <- c("symbol", "date", "date_rolling", "time", "right_time",
+                        colnames(features)[grep("extreme", colnames(features))],
+                        colnames(features)[grep("bin_simple", colnames(features))],
+                        colnames(features)[grep("bin_decile", colnames(features))],
+-                       "bmo_return", "amc_return",
+-                       "open", "high", "low", "close", "volume", "returns")
++                       "bmo_return", "amc_return")
++                       # "open", "high", "low", "close", "volume", "returns")
+ cols_features <- setdiff(colnames(features), c(cols_remove, cols_non_features))
+ head(cols_features, 10)
+ tail(cols_features, 500)
+@@ -789,6 +825,7 @@ cols <- c(cols_non_features, cols_features)
+ features <- features[, .SD, .SDcols = cols]
+ 
+ # checks
++features[, max(date)]
+ features[, .(symbol, date, date_rolling)]
+ 
+ 
+@@ -796,7 +833,8 @@ features[, .(symbol, date, date_rolling)]
+ # CLEAN DATA --------------------------------------------------------------
+ # convert columns to numeric. This is important only if we import existing features
+ clf_data <- copy(features)
+-chr_to_num_cols <- setdiff(colnames(clf_data[, .SD, .SDcols = is.character]), c("symbol", "time", "right_time"))
++chr_to_num_cols <- setdiff(colnames(clf_data[, .SD, .SDcols = is.character]),
++                           c("symbol", "time", "right_time", "industry", "sector"))
+ clf_data <- clf_data[, (chr_to_num_cols) := lapply(.SD, as.numeric), .SDcols = chr_to_num_cols]
+ # int_to_num_cols <- colnames(clf_data[, .SD, .SDcols = is.integer])
+ # clf_data <- clf_data[, (int_to_num_cols) := lapply(.SD, as.numeric), .SDcols = int_to_num_cols]
+@@ -824,17 +862,24 @@ clf_data <- clf_data[is.finite(rowSums(clf_data[, .SD, .SDcols = is.numeric], na
+ n_1 <- nrow(clf_data)
+ print(paste0("Removing ", n_0 - n_1, " rows because of Inf values"))
+ 
++# final checks
++clf_data[, .(symbol, date, date_rolling)]
++features[, .(symbol, date, date_rolling)]
++features[, max(date)]
++clf_data[, max(date)]
++
+ # save features
+-time_ <- strftime(Sys.time(), "%Y%m%d%H%M%S")
+-file_mame <- paste0("D:/features/pead-predictors-", time_, ".csv")
+-fwrite(clf_data, file_mame)
++last_pead_date = strftime(clf_data[, max(date)], "%Y%m%d")
++file_name = paste0("pead-predictors-", last_pead_date, ".csv")
++file_name_local = fs::path("D:/features", file_name)
++fwrite(clf_data, file_name_local)
+ 
+ # save to Azure blob
+ endpoint = "https://snpmarketdata.blob.core.windows.net/"
+ blob_key = readLines('./blob_key.txt')
+ BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+ cont = storage_container(BLOBENDPOINT, "jphd")
+-storage_write_csv(clf_data, cont, "pead-predictors-update.csv")
++storage_write_csv(clf_data, cont, file_name)
+ 
+ 
+ 
+@@ -846,17 +891,6 @@ storage_write_csv(clf_data, cont, "pead-predictors-update.csv")
+ # For non-time series data, you can compute the difference between two or more related features.
+ # Squares, Cubes, and Higher-order Polynomials:
+ #
+-# 
+-# 2
+-# ,
+-# 
+-# 3
+-# ,
+-# 
+-# x
+-# 2
+-#  ,x
+-# 3
+ #  , can capture non-linear relationships.
+ # Interaction Features:
+ #
+diff --git a/image.def b/image.def
+index 6713aaa..5d18d6c 100644
+--- a/image.def
++++ b/image.def
+@@ -42,6 +42,7 @@ From: r-base:4.3.0
+   R --slave -e 'install.packages("kernlab")'
+   R --slave -e 'install.packages("gbm")'
+   R --slave -e 'install.packages("rsm")'
++  R --slave -e 'install.packages("glmnet")'
+   R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.1/catboost-R-Linux-1.2.2.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
+   R --slave -e 'install.packages("torch")'
+   R --slave -e 'torch::install_torch()'
+diff --git a/paper_trading.R b/paper_trading.R
+new file mode 100644
+index 0000000..d229d2a
+--- /dev/null
++++ b/paper_trading.R
+@@ -0,0 +1,909 @@
++library(data.table)
++library(gausscov)
++library(paradox)
++library(mlr3)
++library(mlr3pipelines)
++library(mlr3viz)
++library(mlr3tuning)
++library(mlr3misc)
++library(mlr3extralearners)
++library(future)
++library(future.apply)
++library(batchtools)
++library(mlr3batchmark)
++
++
++# SETUP -------------------------------------------------------------------
++# utils https://stackoverflow.com/questions/1995933/number-of-months-between-two-dates
++monnb <- function(d) {
++  lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
++  lt$year*12 + lt$mon }
++mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
++
++# snake to camel
++snakeToCamel <- function(snake_str) {
++  # Replace underscores with spaces
++  spaced_str <- gsub("_", " ", snake_str)
++
++  # Convert to title case using tools::toTitleCase
++  title_case_str <- tools::toTitleCase(spaced_str)
++
++  # Remove spaces and make the first character lowercase
++  camel_case_str <- gsub(" ", "", title_case_str)
++  camel_case_str <- sub("^.", tolower(substr(camel_case_str, 1, 1)), camel_case_str)
++
++  # I haeve added this to remove dot
++  camel_case_str <- gsub("\\.", "", camel_case_str)
++
++  return(camel_case_str)
++}
++
++
++# PREPARE DATA ------------------------------------------------------------
++print("Prepare data")
++
++# read predictors
++data_tbl = fread("D:/features/pead-predictors-20231031.csv")
++
++# convert tibble to data.table
++DT = as.data.table(data_tbl)
++
++# create group variable
++DT[, date_rolling := as.IDate(date_rolling)]
++DT[, yearmonthid := round(date_rolling, digits = "month")]
++DT[, .(date, date_rolling, yearmonthid)]
++DT[, yearmonthid := as.integer(yearmonthid)]
++DT[, .(date, date_rolling, yearmonthid)]
++
++# remove industry and sector vars
++DT[, `:=`(industry = NULL, sector = NULL)]
++
++# define predictors
++cols_non_features <- c("symbol", "date", "time", "right_time",
++                       "bmo_return", "amc_return",
++                       "open", "high", "low", "close", "volume", "returns",
++                       "yearmonthid", "date_rolling"
++)
++targets <- c(colnames(DT)[grep("ret_excess", colnames(DT))])
++cols_features <- setdiff(colnames(DT), c(cols_non_features, targets))
++
++# change feature and targets columns names due to lighgbm
++cols_features_new = vapply(cols_features, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
++setnames(DT, cols_features, cols_features_new)
++cols_features = cols_features_new
++targets_new = vapply(targets, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
++setnames(DT, targets, targets_new)
++targets = targets_new
++
++# convert columns to numeric. This is important only if we import existing features
++chr_to_num_cols <- setdiff(colnames(DT[, .SD, .SDcols = is.character]), c("symbol", "time", "right_time"))
++print(chr_to_num_cols)
++DT <- DT[, (chr_to_num_cols) := lapply(.SD, as.numeric), .SDcols = chr_to_num_cols]
++
++# remove constant columns in set
++features_ <- DT[, ..cols_features]
++remove_cols <- colnames(features_)[apply(features_, 2, var, na.rm=TRUE) == 0]
++print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
++cols_features <- setdiff(cols_features, remove_cols)
++
++# convert variables with low number of unique values to factors
++int_numbers = na.omit(DT[, ..cols_features])[, lapply(.SD, function(x) all(floor(x) == x))]
++int_cols = colnames(DT[, ..cols_features])[as.matrix(int_numbers)[1,]]
++factor_cols = DT[, ..int_cols][, lapply(.SD, function(x) length(unique(x)))]
++factor_cols = as.matrix(factor_cols)[1, ]
++factor_cols = factor_cols[factor_cols <= 100]
++DT = DT[, (names(factor_cols)) := lapply(.SD, as.factor), .SD = names(factor_cols)]
++
++# change IDate to date, because of error
++DT[, date := as.POSIXct(date, tz = "UTC")]
++
++# sort
++DT = DT[order(yearmonthid)]
++
++
++
++
++# TASKS -------------------------------------------------------------------
++print("Tasks")
++
++# id coluns we always keep
++id_cols = c("symbol", "date", "yearmonthid")
++
++# convert date to PosixCt because it is requireed by mlr3
++DT[, date := as.POSIXct(date, tz = "UTC")]
++
++# task with future week returns as target
++target_ = colnames(DT)[grep("^ret.*xcess.*tand.*5", colnames(DT))]
++cols_ = c(id_cols, target_, cols_features)
++task_ret_week <- as_task_regr(DT[, ..cols_],
++                              id = "taskRetWeek",
++                              target = target_)
++
++# set roles for symbol, date and yearmonth_id
++task_ret_week$col_roles$feature = setdiff(task_ret_week$col_roles$feature,
++                                          id_cols)
++
++
++# CROSS VALIDATIONS -------------------------------------------------------
++print("Cross validations")
++
++# create train, tune and test set
++nested_cv_split = function(task,
++                           train_length = 12,
++                           tune_length = 1,
++                           gap_tune = 0,
++                           id = task$id) {
++
++  # get year month id data
++  # task = task_ret_week$clone()
++  task_ = task$clone()
++  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
++                                    rows = 1:task_$nrow)
++  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
++  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
++
++  # create cusom CV's for inner and outer sampling
++  custom_inner = rsmp("custom", id = task$id)
++  custom_outer = rsmp("custom", id = task$id)
++
++  # util vars
++  start_folds = 1:(length(groups_v)-train_length-tune_length-gap_test)
++  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
++
++  # create train data
++  train_groups <- lapply(start_folds,
++                         function(x) groups_v[x:(x+train_length-1)])
++  train_sets <- lapply(train_groups, get_row_ids)
++
++  # create tune set
++  tune_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+train_length+gap_tune):(x+train_length+gap_tune+tune_length-1)])
++  tune_sets <- lapply(tune_groups, get_row_ids)
++
++  # test train and tune
++  test_1 = vapply(seq_along(train_groups), function(i) {
++    mondf(
++      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
++      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
++    )
++  }, FUN.VALUE = numeric(1L))
++  stopifnot(all(test_1 == (1+gap_tune)))
++  # test_2 = vapply(seq_along(train_groups), function(i) {
++  #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
++  # }, FUN.VALUE = numeric(1L))
++  # stopifnot(all(test_2 > ))
++
++  # create test sets
++  insample_length = train_length + gap_tune +  tune_length + gap_test
++  test_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
++  test_sets <- lapply(test_groups, get_row_ids)
++
++  # test tune and test
++  test_3 = vapply(seq_along(train_groups), function(i) {
++    mondf(
++      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
++      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
++    )
++  }, FUN.VALUE = numeric(1L))
++  stopifnot(all(test_1 == 1 + gap_test))
++  # test_4 = vapply(seq_along(train_groups), function(i) {
++  #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
++  # }, FUN.VALUE = numeric(1L))
++  # stopifnot(all(test_2 == 1))
++
++  # test
++  # as.Date(train_groups[[2]])
++  # as.Date(tune_groups[[2]])
++  # as.Date(test_groups[[2]])
++
++  # create inner and outer resamplings
++  custom_inner$instantiate(task, train_sets, tune_sets)
++  inner_sets = lapply(seq_along(train_groups), function(i) {
++    c(train_sets[[i]], tune_sets[[i]])
++  })
++  custom_outer$instantiate(task, inner_sets, test_sets)
++  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
++}
++
++# generate cv's
++train_sets = seq(12, 12 * 3, 12)
++gap_sets = c(0)
++mat = cbind(train = train_sets)
++expanded_list  = lapply(gap_sets, function(v) {
++  cbind.data.frame(mat, gap = v)
++})
++cv_param_grid = rbindlist(expanded_list)
++cv_param_grid[ ,tune := 3]
++custom_cvs = list()
++for (i in 1:nrow(cv_param_grid)) {
++  print(i)
++  param_ = cv_param_grid[i]
++  if (param_$gap == 0) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_week,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++  } else if (param_$gap == 1) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_month,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  } else if (param_$gap == 2) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  } else if (param_$gap == 3) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  }
++}
++
++# test
++length(custom_cvs) == nrow(cv_param_grid)
++
++
++# ADD PIPELINES -----------------------------------------------------------
++print("Add pipelines")
++
++# source pipes, filters and other
++source("mlr3_winsorization.R")
++source("mlr3_uniformization.R")
++source("mlr3_gausscov_f1st.R")
++source("mlr3_gausscov_f3st.R")
++source("mlr3_dropna.R")
++source("mlr3_dropnacol.R")
++source("mlr3_filter_drop_corr.R")
++source("mlr3_winsorizationsimple.R")
++source("mlr3_winsorizationsimplegroup.R")
++source("PipeOpPCAExplained.R")
++# measures
++source("Linex.R")
++source("AdjLoss2.R")
++source("PortfolioRet.R")
++
++# add my pipes to mlr dictionary
++mlr_pipeops$add("uniformization", PipeOpUniform)
++mlr_pipeops$add("winsorize", PipeOpWinsorize)
++mlr_pipeops$add("winsorizesimple", PipeOpWinsorizeSimple)
++mlr_pipeops$add("winsorizesimplegroup", PipeOpWinsorizeSimpleGroup)
++mlr_pipeops$add("dropna", PipeOpDropNA)
++mlr_pipeops$add("dropnacol", PipeOpDropNACol)
++mlr_pipeops$add("dropcorr", PipeOpDropCorr)
++mlr_pipeops$add("pca_explained", PipeOpPCAExplained)
++mlr_filters$add("gausscov_f1st", FilterGausscovF1st)
++mlr_filters$add("gausscov_f3st", FilterGausscovF3st)
++mlr_measures$add("linex", Linex)
++mlr_measures$add("adjloss2", AdjLoss2)
++mlr_measures$add("portfolio_ret", PortfolioRet)
++
++
++# GRAPH V2 ----------------------------------------------------------------
++# graph template
++graph_template =
++  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
++  po("dropna", id = "dropna") %>>%
++  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
++  po("fixfactors", id = "fixfactors") %>>%
++  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
++  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
++  # po("uniformization") %>>%
++  # scale branch
++  po("branch", options = c("uniformization", "scale"), id = "scale_branch") %>>%
++  gunion(list(po("uniformization"),
++              po("scale")
++  )) %>>%
++  po("unbranch", id = "scale_unbranch") %>>%
++  po("dropna", id = "dropna_v2") %>>%
++  # filters
++  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
++  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
++              po("filter", filter = flt("relief"), filter.frac = 0.05),
++              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
++  )) %>>%
++  # po("nop", id = "nop_filter"))) %>>%
++  po("unbranch", id = "filter_unbranch") %>>%
++  # modelmatrix
++  po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
++  gunion(list(
++    po("nop", id = "nop_interaction"),
++    po("modelmatrix", formula = ~ . ^ 2))) %>>%
++  po("unbranch", id = "interaction_unbranch") %>>%
++  po("removeconstants", id = "removeconstants_3", ratio = 0)
++
++# hyperparameters template
++graph_template$param_set
++search_space_template = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix"))
++)
++
++# random forest graph
++graph_rf = graph_template %>>%
++  po("learner", learner = lrn("regr.ranger"))
++plot(graph_rf)
++graph_rf = as_learner(graph_rf)
++as.data.table(graph_rf$param_set)[, .(id, class, lower, upper, levels)]
++search_space_rf = search_space_template$clone()
++search_space_rf$add(
++  ps(regr.ranger.max.depth  = p_int(1, 20),
++     regr.ranger.replace    = p_lgl(),
++     regr.ranger.mtry.ratio = p_dbl(0.1, 1),
++     regr.ranger.splitrule  = p_fct(levels = c("variance", "extratrees")))
++)
++# regr.ranger.min.node.size   = p_int(1, 20), # Adjust the range as needed
++# regr.ranger.sample.fraction = p_dbl(0.1, 1),
++
++# xgboost graph
++graph_xgboost = graph_template %>>%
++  po("learner", learner = lrn("regr.xgboost"))
++plot(graph_xgboost)
++graph_xgboost = as_learner(graph_xgboost)
++as.data.table(graph_xgboost$param_set)[grep("depth", id), .(id, class, lower, upper, levels)]
++search_space_xgboost = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.xgboost.alpha     = p_dbl(0.001, 100, logscale = TRUE),
++  regr.xgboost.max_depth = p_int(1, 20),
++  regr.xgboost.eta       = p_dbl(0.0001, 1, logscale = TRUE),
++  regr.xgboost.nrounds   = p_int(1, 5000),
++  regr.xgboost.subsample = p_dbl(0.1, 1)
++)
++
++# gbm graph
++graph_gbm = graph_template %>>%
++  po("learner", learner = lrn("regr.gbm"))
++plot(graph_gbm)
++graph_gbm = as_learner(graph_gbm)
++as.data.table(graph_gbm$param_set)[, .(id, class, lower, upper, levels)]
++search_space_gbm = search_space_template$clone()
++search_space_gbm$add(
++  ps(regr.gbm.distribution      = p_fct(levels = c("gaussian", "tdist")),
++     regr.gbm.shrinkage         = p_dbl(lower = 0.001, upper = 0.1),
++     regr.gbm.n.trees           = p_int(lower = 50, upper = 200),
++     regr.gbm.interaction.depth = p_int(lower = 1, upper = 4))
++  # ....
++)
++
++# catboost graph
++graph_catboost = graph_template %>>%
++  po("learner", learner = lrn("regr.catboost"))
++graph_catboost = as_learner(graph_catboost)
++as.data.table(graph_catboost$param_set)[, .(id, class, lower, upper, levels)]
++search_space_catboost = search_space_template$clone()
++# https://catboost.ai/en/docs/concepts/parameter-tuning#description10
++search_space_catboost$add(
++  ps(regr.catboost.learning_rate   = p_dbl(lower = 0.01, upper = 0.3),
++     regr.catboost.depth           = p_int(lower = 4, upper = 10),
++     regr.catboost.l2_leaf_reg     = p_int(lower = 1, upper = 5),
++     regr.catboost.random_strength = p_int(lower = 0, upper = 3))
++)
++
++# # gamboost graph
++# # Error in eval(predvars, data, env) : object 'adxDx14' not found
++# # This happened PipeOp regr.gamboost's $train()
++# # In addition: There were 50 or more warnings (use warnings() to see the first 50)
++# graph_gamboost = graph_template %>>%
++#   po("learner", learner = lrn("regr.gamboost"))
++# graph_gamboost = as_learner(graph_gamboost)
++# as.data.table(graph_gamboost$param_set)[, .(id, class, lower, upper, levels)]
++# search_space_gamboost = search_space_template$clone()
++# search_space_gamboost$add(
++#   ps(regr.gamboost.mstop       = p_int(lower = 10, upper = 100),
++#      regr.gamboost.nu          = p_dbl(lower = 0.01, upper = 0.5),
++#      regr.gamboost.baselearner = p_fct(levels = c("bbs", "bols", "btree")))
++# )
++
++# kknn graph
++graph_kknn = graph_template %>>%
++  po("learner", learner = lrn("regr.kknn"))
++graph_kknn = as_learner(graph_kknn)
++as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
++search_space_kknn = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(
++        x,
++        "0.80" = 0.80,
++        "0.90" = 0.90,
++        "0.95" = 0.95,
++        "0.99" = 0.99
++      )
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.kknn.k        = p_int(
++    lower = 1,
++    upper = 50,
++    logscale = TRUE
++  ),
++  regr.kknn.distance = p_dbl(lower = 1, upper = 5),
++  regr.kknn.kernel   = p_fct(
++    levels = c(
++      "rectangular",
++      "optimal",
++      "epanechnikov",
++      "biweight",
++      "triweight",
++      "cos",
++      "inv",
++      "gaussian",
++      "rank"
++    )
++  )
++)
++
++# nnet graph
++graph_nnet = graph_template %>>%
++  po("learner", learner = lrn("regr.nnet", MaxNWts = 40000))
++graph_nnet = as_learner(graph_nnet)
++as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
++search_space_nnet = search_space_template$clone()
++search_space_nnet$add(
++  ps(regr.nnet.size  = p_int(lower = 5, upper = 30),
++     regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
++     regr.nnet.maxit = p_int(lower = 50, upper = 500))
++)
++
++
++### THIS LEARNER UNSTABLE ####
++# ksvm graph
++# graph_ksvm = graph_template %>>%
++#   po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
++# graph_ksvm = as_learner(graph_ksvm)
++# as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
++# search_space_ksvm = ps(
++#   # subsample for hyperband
++#   subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++#   # preprocessing
++#   dropcorr.cutoff = p_fct(
++#     levels = c("0.80", "0.90", "0.95", "0.99"),
++#     trafo = function(x, param_set) {
++#       switch(x,
++#              "0.80" = 0.80,
++#              "0.90" = 0.90,
++#              "0.95" = 0.95,
++#              "0.99" = 0.99)
++#     }
++#   ),
++#   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++#   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++#   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++#   # filters
++#   filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++#   # interaction
++#   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++#   # learner
++#   regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
++#                                        "laplacedot", "besseldot", "anovadot")),
++#   regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
++#   regr.ksvm.degree  = p_int(lower = 1, upper = 5,
++#                             depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
++#   regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
++# )
++### THIS LEARNER UNSTABLE ####
++
++# LAST
++# lightgbm graph
++# [LightGBM] [Fatal] Do not support special JSON characters in feature name.
++graph_template =
++  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
++  po("dropna", id = "dropna") %>>%
++  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
++  po("fixfactors", id = "fixfactors") %>>%
++  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
++  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
++  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
++  # scale branch
++  po("branch", options = c("uniformization", "scale"), id = "scale_branch") %>>%
++  gunion(list(po("uniformization"),
++              po("scale")
++  )) %>>%
++  po("unbranch", id = "scale_unbranch") %>>%
++  po("dropna", id = "dropna_v2") %>>%
++  # filters
++  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
++  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
++              po("filter", filter = flt("relief"), filter.frac = 0.05),
++              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
++  )) %>>%
++  po("unbranch", id = "filter_unbranch")
++search_space_template = ps(
++  # subsample for hyperband
++  # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  # preprocessing
++  # dropnacol.affect_columns = p_fct(
++  #   levels = c("0.01", "0.05", "0.10"),
++  #   trafo = function(x, param_set) {
++  #     switch(x,
++  #            "0.01" = 0.01,
++  #            "0.05" = 0.05,
++  #            "0.10" = 0.1)
++  #   }
++  # ),
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov"))
++)
++graph_lightgbm = graph_template %>>%
++  po("learner", learner = lrn("regr.lightgbm"))
++graph_lightgbm = as_learner(graph_lightgbm)
++as.data.table(graph_lightgbm$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
++search_space_lightgbm = search_space_template$clone()
++search_space_lightgbm$add(
++  ps(regr.lightgbm.max_depth     = p_int(lower = 2, upper = 10),
++     regr.lightgbm.learning_rate = p_dbl(lower = 0.001, upper = 0.3),
++     regr.lightgbm.num_leaves    = p_int(lower = 10, upper = 100))
++)
++
++# earth graph
++graph_earth = graph_template %>>%
++  po("learner", learner = lrn("regr.earth"))
++graph_earth = as_learner(graph_earth)
++as.data.table(graph_earth$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
++search_space_earth = search_space_template$clone()
++search_space_earth$add(
++  ps(regr.earth.degree  = p_int(lower = 1, upper = 4),
++     # regr.earth.penalty = p_int(lower = 1, upper = 5),
++     regr.earth.nk      = p_int(lower = 50, upper = 250))
++)
++
++# rsm graph
++graph_rsm = graph_template %>>%
++  po("learner", learner = lrn("regr.rsm"))
++plot(graph_rsm)
++graph_rsm = as_learner(graph_rsm)
++as.data.table(graph_rsm$param_set)[, .(id, class, lower, upper, levels)]
++search_space_rsm = search_space_template$clone()
++search_space_rsm$add(
++  ps(regr.rsm.modelfun = p_fct(levels = c("FO", "TWI", "SO")))
++)
++# Error in fo[, i] * fo[, j] : non-numeric argument to binary operator
++# This happened PipeOp regr.rsm's $train()
++# Calls: lapply ... resolve.list -> signalConditionsASAP -> signalConditions
++# In addition: Warning message:
++# In bbandsDn5:volumeDownUpRatio :
++#   numerical expression has 4076 elements: only the first used
++# This happened PipeOp regr.rsm's $train()
++
++# BART graph
++graph_bart = graph_template %>>%
++  po("learner", learner = lrn("regr.bart"))
++graph_bart = as_learner(graph_bart)
++as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
++search_space_bart = search_space_template$clone()
++search_space_bart$add(
++  ps(regr.bart.k      = p_int(lower = 1, upper = 10),
++     regr.bart.numcut = p_int(lower = 10, upper = 200),
++     regr.bart.ntree  = p_int(lower = 50, upper = 500))
++)
++# chatgpt returns this
++# n_chains = p_int(lower = 1, upper = 5),
++# m_try = p_int(lower = 1, upper = 13),
++# nu = p_dbl(lower = 0.1, upper = 10),
++# alpha = p_dbl(lower = 0.01, upper = 1),
++# beta = p_dbl(lower = 0.01, upper = 1),
++# burn = p_int(lower = 10, upper = 100),
++# iter = p_int(lower = 100, upper = 1000)
++
++# threads
++threads = 4
++set_threads(graph_rf, n = threads)
++set_threads(graph_xgboost, n = threads)
++# set_threads(graph_bart, n = threads)
++# set_threads(graph_ksvm, n = threads) # unstable
++set_threads(graph_nnet, n = threads)
++set_threads(graph_kknn, n = threads)
++set_threads(graph_lightgbm, n = threads)
++set_threads(graph_earth, n = threads)
++set_threads(graph_gbm, n = threads)
++set_threads(graph_rsm, n = threads)
++set_threads(graph_catboost, n = threads)
++
++
++# DESIGNS -----------------------------------------------------------------
++designs_l = lapply(custom_cvs, function(cv_) {
++  # debug
++  # cv_ = custom_cvs[[1]]
++
++  # get cv inner object
++  cv_inner = cv_$custom_inner
++  cv_outer = cv_$custom_outer
++  cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
++
++  # choose task_
++  print(cv_inner$id)
++  if (cv_inner$id == "taskRetWeek") {
++    task_ = task_ret_week$clone()
++  } else if (cv_inner$id == "taskRetMonth") {
++    task_ = task_ret_month$clone()
++  } else if (cv_inner$id == "taskRetMonth2") {
++    task_ = task_ret_month2$clone()
++  } else if (cv_inner$id == "taskRetQuarter") {
++    task_ = task_ret_quarter$clone()
++  }
++
++  designs_cv_l = lapply(1:cv_inner$iters, function(i) { # 1:cv_inner$iters
++    # debug
++    # i = 1
++
++    # choose task_
++    print(cv_inner$id)
++    if (cv_inner$id == "taskRetWeek") {
++      task_ = task_ret_week$clone()
++    } else if (cv_inner$id == "taskRetMonth") {
++      task_ = task_ret_month$clone()
++    } else if (cv_inner$id == "taskRetMonth2") {
++      task_ = task_ret_month2$clone()
++    } else if (cv_inner$id == "taskRetQuarter") {
++      task_ = task_ret_quarter$clone()
++    }
++
++    # inner resampling
++    custom_ = rsmp("custom")
++    custom_$id = paste0("custom_", cv_inner$iters, "_", i)
++    custom_$instantiate(task_ret_week,
++                        list(cv_inner$train_set(i)),
++                        list(cv_inner$test_set(i)))
++
++    # objects for all autotuners
++    measure_ = msr("portfolio_ret")
++    # tuner_   = tnr("hyperband", eta = 4)
++    tuner_   = tnr("mbo")
++    term_evals = 20
++
++    # auto tuner rf
++    at_rf = auto_tuner(
++      tuner = tuner_,
++      learner = graph_rf,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_rf,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner xgboost
++    at_xgboost = auto_tuner(
++      tuner = tuner_,
++      learner = graph_xgboost,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_xgboost,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner BART
++    at_bart = auto_tuner(
++      tuner = tuner_,
++      learner = graph_bart,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_bart,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner nnet
++    at_nnet = auto_tuner(
++      tuner = tuner_,
++      learner = graph_nnet,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_nnet,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner lightgbm
++    at_lightgbm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_lightgbm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_lightgbm,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner earth
++    at_earth = auto_tuner(
++      tuner = tuner_,
++      learner = graph_earth,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_earth,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner kknn
++    at_kknn = auto_tuner(
++      tuner = tuner_,
++      learner = graph_kknn,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_kknn,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner gbm
++    at_gbm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_gbm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_gbm,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner rsm
++    at_rsm = auto_tuner(
++      tuner = tuner_,
++      learner = graph_rsm,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_rsm,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner rsm
++    at_bart = auto_tuner(
++      tuner = tuner_,
++      learner = graph_bart,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_bart,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # auto tuner catboost
++    at_catboost = auto_tuner(
++      tuner = tuner_,
++      learner = graph_catboost,
++      resampling = custom_,
++      measure = measure_,
++      search_space = search_space_catboost,
++      # terminator = trm("none")
++      term_evals = term_evals
++    )
++
++    # outer resampling
++    customo_ = rsmp("custom")
++    customo_$id = paste0("custom_", cv_inner$iters, "_", i)
++    customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
++
++    # nested CV for one round
++    design = benchmark_grid(
++      tasks = task_,
++      learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth,
++                      at_kknn, at_gbm, at_rsm, at_bart, at_catboost),
++      resamplings = customo_
++    )
++  })
++  designs_cv = do.call(rbind, designs_cv_l)
++})
++designs = do.call(rbind, designs_l)
++
++
++
++
+diff --git a/plot_cv.png b/plot_cv.png
+new file mode 100644
+index 0000000..aaade87
+Binary files /dev/null and b/plot_cv.png differ
+diff --git a/results_light.R b/results_light.R
+index f715610..55d4010 100644
+--- a/results_light.R
++++ b/results_light.R
+@@ -14,7 +14,7 @@ endpoint = "https://snpmarketdata.blob.core.windows.net/"
+ BLOBENDPOINT = storage_endpoint(endpoint, key=blob_key)
+ 
+ # load registry
+-reg = loadRegistry("F:/H4", work.dir="F:/H4")
++reg = loadRegistry("F:/H4v2", work.dir="F:/H4v2")
+ 
+ # used memory
+ reg$status[!is.na(mem.used)]
+@@ -28,13 +28,15 @@ ids_notdone = findNotDone(reg=reg)
+ rbind(ids_notdone, ids_done[job.id %in% results_files])
+ 
+ # import already saved predictions
+-fs::dir_ls("predictions")
++# fs::dir_ls("predictions")
+ # predictions = readRDS("predictions/predictions-20231025215620.rds")
+ 
+ # get results
+ plan("multisession", workers = 4L)
+ start_time = Sys.time()
+ results = future_lapply(ids_done[, job.id], function(id_) {
++  # id_ = 6819
++  print(id_)
+   # bmr object
+   bmrs = reduceResultsBatchmark(id_, store_backends = FALSE, reg = reg)
+   bmrs_dt = as.data.table(bmrs)
+@@ -70,7 +72,7 @@ predictions = rbindlist(results, fill = TRUE)
+ time_ = strftime(Sys.time(), format = "%Y%m%d%H%M%S")
+ file_name = paste0("predictions/predictions-", time_, ".rds")
+ if (!fs::dir_exists("predictions")) fs::dir_create("predictions")
+-# saveRDS(predictions, file_name)
++saveRDS(predictions, file_name)
+ 
+ # import tasks
+ tasks_files = dir_ls("F:/H4/problems")
+diff --git a/run_padobran.R b/run_padobran.R
+index 2b3460e..1a69c3b 100644
+--- a/run_padobran.R
++++ b/run_padobran.R
+@@ -19,6 +19,14 @@ monnb <- function(d) {
+   lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
+   lt$year*12 + lt$mon }
+ mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
++diff_in_weeks = function(d1, d2) difftime(d2, d1, units = "weeks") # weeks
++
++# weeknb <- function(d) {
++#   as.numeric(difftime(as.Date(d), as.Date("1900-01-01"), units = "weeks"))
++# }
++# weekdf <- function(d1, d2) {
++#   weeknb(d2) - weeknb(d1)
++# }
+ 
+ # snake to camel
+ snakeToCamel <- function(snake_str) {
+@@ -43,8 +51,7 @@ snakeToCamel <- function(snake_str) {
+ print("Prepare data")
+ 
+ # read predictors
+-data_tbl = fread("./pead-predictors-update.csv")
+-# data_tbl <- fread("D:/features/pead-predictors-20230523202603.csv")
++data_tbl <- fread("D:/features/pead-predictors-20231031.csv")
+ 
+ # convert tibble to data.table
+ DT = as.data.table(data_tbl)
+@@ -52,15 +59,20 @@ DT = as.data.table(data_tbl)
+ # create group variable
+ DT[, date_rolling := as.IDate(date_rolling)]
+ DT[, yearmonthid := round(date_rolling, digits = "month")]
+-DT[, .(date, date_rolling, yearmonthid)]
++DT[, weekid := round(date_rolling, digits = "week")]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
+ DT[, yearmonthid := as.integer(yearmonthid)]
+-DT[, .(date, date_rolling, yearmonthid)]
++DT[, weekid := as.integer(weekid)]
++DT[, .(date, date_rolling, yearmonthid, weekid)]
++
++# remove industry and sector vars
++DT[, `:=`(industry = NULL, sector = NULL)]
+ 
+ # define predictors
+ cols_non_features <- c("symbol", "date", "time", "right_time",
+                        "bmo_return", "amc_return",
+                        "open", "high", "low", "close", "volume", "returns",
+-                       "yearmonthid", "date_rolling"
++                       "yearmonthid", "weekid", "date_rolling"
+ )
+ targets <- c(colnames(DT)[grep("ret_excess", colnames(DT))])
+ cols_features <- setdiff(colnames(DT), c(cols_non_features, targets))
+@@ -109,16 +121,16 @@ DT[, date := as.POSIXct(date, tz = "UTC")]
+ # setorder(DT, date)
+ print("This was the problem")
+ # DT = DT[order(date)] # DOESNT WORK TOO
+-DT = DT[order(yearmonthid)]
++DT = DT[order(yearmonthid, weekid)]
++DT[, .(symbol, date, weekid, yearmonthid)]
+ print("This was the problem. Solved.")
+ 
+ 
+-
+ # TASKS -------------------------------------------------------------------
+ print("Tasks")
+ 
+ # id coluns we always keep
+-id_cols = c("symbol", "date", "yearmonthid")
++id_cols = c("symbol", "date", "yearmonthid", "weekid")
+ 
+ # convert date to PosixCt because it is requireed by mlr3
+ DT[, date := as.POSIXct(date, tz = "UTC")]
+@@ -175,7 +187,7 @@ nested_cv_split = function(task,
+                            id = task$id) {
+ 
+   # get year month id data
+-  # task = task_ret_quarter$clone()
++  # task = task_ret_week$clone()
+   task_ = task$clone()
+   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                     rows = 1:task_$nrow)
+@@ -187,7 +199,7 @@ nested_cv_split = function(task,
+   custom_outer = rsmp("custom", id = task$id)
+ 
+   # util vars
+-  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test)
++  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test-gap_tune)
+   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+ 
+   # create train data
+@@ -214,7 +226,7 @@ nested_cv_split = function(task,
+   # stopifnot(all(test_2 > ))
+ 
+   # create test sets
+-  insample_length = train_length + gap_tune +  tune_length + gap_test
++  insample_length = train_length + gap_tune + tune_length + gap_test
+   test_groups <- lapply(start_folds,
+                         function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
+   test_sets <- lapply(test_groups, get_row_ids)
+@@ -246,130 +258,84 @@ nested_cv_split = function(task,
+   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+ }
+ 
+-# generate cv's
+-train_sets = seq(12, 12 * 3, 12)
+-gap_sets = c(0:3)
+-mat = cbind(train = train_sets)
+-expanded_list  = lapply(gap_sets, function(v) {
+-  cbind.data.frame(mat, gap = v)
+-})
+-cv_param_grid = rbindlist(expanded_list)
+-cv_param_grid[ ,tune := 3]
+-custom_cvs = list()
+-for (i in 1:nrow(cv_param_grid)) {
+-  print(i)
+-  param_ = cv_param_grid[i]
+-  if (param_$gap == 0) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_week,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-  } else if (param_$gap == 1) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_month,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  } else if (param_$gap == 2) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  } else if (param_$gap == 3) {
+-    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
+-                                      param_$train,
+-                                      param_$tune,
+-                                      1,
+-                                      param_$gap,
+-                                      param_$gap)
+-
+-  }
+-}
+-
+-# test
+-length(custom_cvs) == nrow(cv_param_grid)
+-
+-# create expanding window function
+-nested_cv_split_expanding = function(task,
+-                                     train_length_start = 6,
+-                                     tune_length = 3,
+-                                     test_length = 1,
+-                                     gap_tune = 1,
+-                                     gap_test = 1,
+-                                     id = task$id) {
++nested_cv_split_week = function(task,
++                                train_length = 50,
++                                tune_length = 10,
++                                test_length = 1,
++                                gap_tune = 1,
++                                gap_test = 1,
++                                id = task$id) {
++
++  # # debug
++  # train_length = 144
++  # tune_length = 12
++  # test_length = 1
++  # gap_tune = 1
++  # gap_test = 1
+ 
+   # get year month id data
+   # task = task_ret_week$clone()
+   task_ = task$clone()
+-  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+-                                    rows = 1:task_$nrow)
+-  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+-  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
++  weekid_ = task_$backend$data(cols = c("weekid", "..row_id"),
++                               rows = 1:task_$nrow)
++  stopifnot(all(task_$row_ids == weekid_$`..row_id`))
++  groups_v = weekid_[, unlist(unique(weekid))]
+ 
+   # create cusom CV's for inner and outer sampling
+   custom_inner = rsmp("custom", id = task$id)
+   custom_outer = rsmp("custom", id = task$id)
+ 
+   # util vars
+-  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
++  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test-gap_tune)
++  get_row_ids = function(mid) unlist(weekid_[weekid %in% mid, 2], use.names = FALSE)
+ 
+   # create train data
+-  train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
++  train_groups <- lapply(start_folds,
++                         function(x) groups_v[x:(x+train_length-1)])
++  train_sets <- lapply(train_groups, get_row_ids)
+ 
+   # create tune set
+-  tune_groups <- lapply((train_length_start+gap_tune+1):length(groups_v), function(i) groups_v[i:(i+tune_length+gap_tune-1)])
+-  index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+-  tune_groups = tune_groups[index_keep]
+-
+-  # equalize train and tune sets
+-  train_groups = train_groups[1:length(tune_groups)]
+-
+-  # create test sets
+-  insample_length = vapply(train_groups, function(x) as.integer(length(x) + gap_tune + tune_length + gap_test), FUN.VALUE = integer(1))
+-  test_groups <- lapply(insample_length+gap_test+1, function(i) groups_v[i:(i+test_length-1)])
+-  index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+-  test_groups = test_groups[index_keep]
+-
+-  # equalize train, tune and test sets
+-  train_groups = train_groups[1:length(test_groups)]
+-  tune_groups = tune_groups[1:length(test_groups)]
+-
+-  # make sets
+-  train_sets <- lapply(train_groups, get_row_ids)
++  tune_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+train_length+gap_tune):(x+train_length+gap_tune+tune_length-1)])
+   tune_sets <- lapply(tune_groups, get_row_ids)
+-  test_sets <- lapply(test_groups, get_row_ids)
+ 
+-  # test tune and test
++  # test train and tune
+   test_1 = vapply(seq_along(train_groups), function(i) {
+-    mondf(
++    diff_in_weeks(
+       tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+     )
+   }, FUN.VALUE = numeric(1L))
+-  stopifnot(all(test_1 == 1 + gap_tune))
++  stopifnot(all(as.integer(test_1) %in% c(1-gap_tune+1, 1+gap_tune, 1+gap_tune+1)))
+   # test_2 = vapply(seq_along(train_groups), function(i) {
+   #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+   # }, FUN.VALUE = numeric(1L))
+-  # stopifnot(all(test_2 == 1))
++  # stopifnot(all(test_2 > ))
++
++  # create test sets
++  insample_length = train_length + gap_tune + tune_length + gap_test
++  test_groups <- lapply(start_folds,
++                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
++  test_sets <- lapply(test_groups, get_row_ids)
++
++  # test tune and test
+   test_3 = vapply(seq_along(train_groups), function(i) {
+     mondf(
+       tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+     )
+   }, FUN.VALUE = numeric(1L))
+-  stopifnot(all(test_3 == 1 + gap_test))
++  stopifnot(all(as.integer(test_1) %in% c(1 + gap_test - 1, 1 + gap_test, 1 + gap_test + 1)))
+   # test_4 = vapply(seq_along(train_groups), function(i) {
+   #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+   # }, FUN.VALUE = numeric(1L))
+   # stopifnot(all(test_2 == 1))
+ 
++  # test
++  # as.Date(train_groups[[2]])
++  # as.Date(tune_groups[[2]])
++  # as.Date(test_groups[[2]])
++
+   # create inner and outer resamplings
+   custom_inner$instantiate(task, train_sets, tune_sets)
+   inner_sets = lapply(seq_along(train_groups), function(i) {
+@@ -379,6 +345,183 @@ nested_cv_split_expanding = function(task,
+   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+ }
+ 
++# generate cv's
++train_sets = seq(12, 12 * 3, 12)
++gap_sets = c(0:3)
++mat = cbind(train = train_sets)
++expanded_list  = lapply(gap_sets, function(v) {
++  cbind.data.frame(mat, gap = v)
++})
++cv_param_grid = rbindlist(expanded_list)
++cv_param_grid[ ,tune := 3]
++cv_param_grid[1:3, `:=`(train = train * 4, tune = 3 * 4)]
++custom_cvs = list()
++for (i in 1:nrow(cv_param_grid)) {
++  print(i)
++  param_ = cv_param_grid[i]
++  if (param_$train > 47) {
++    custom_cvs[[i]] = nested_cv_split_week(
++      task = task_ret_week,
++      train_length = param_$train,
++      tune_length = param_$tune,
++      test_length = 1,
++      gap_tune = param_$gap + 1,
++      gap_test = param_$gap + 1
++    )
++  } else if (param_$gap == 1) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_month,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  } else if (param_$gap == 2) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  } else if (param_$gap == 3) {
++    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
++                                      param_$train,
++                                      param_$tune,
++                                      1,
++                                      param_$gap,
++                                      param_$gap)
++
++  }
++}
++
++# test
++length(custom_cvs) == nrow(cv_param_grid)
++
++# # create expanding window function
++# nested_cv_split_expanding = function(task,
++#                                      train_length_start = 6,
++#                                      tune_length = 3,
++#                                      test_length = 1,
++#                                      gap_tune = 1,
++#                                      gap_test = 1,
++#                                      id = task$id) {
++#
++#   # get year month id data
++#   # task = task_ret_week$clone()
++#   task_ = task$clone()
++#   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
++#                                     rows = 1:task_$nrow)
++#   stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
++#   groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
++#
++#   # create cusom CV's for inner and outer sampling
++#   custom_inner = rsmp("custom", id = task$id)
++#   custom_outer = rsmp("custom", id = task$id)
++#
++#   # util vars
++#   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
++#
++#   # create train data
++#   train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
++#
++#   # create tune set
++#   tune_groups <- lapply((train_length_start+gap_tune+1):length(groups_v), function(i) groups_v[i:(i+tune_length+gap_tune-1)])
++#   index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
++#   tune_groups = tune_groups[index_keep]
++#
++#   # equalize train and tune sets
++#   train_groups = train_groups[1:length(tune_groups)]
++#
++#   # create test sets
++#   insample_length = vapply(train_groups, function(x) as.integer(length(x) + gap_tune + tune_length + gap_test), FUN.VALUE = integer(1))
++#   test_groups <- lapply(insample_length+gap_test+1, function(i) groups_v[i:(i+test_length-1)])
++#   index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
++#   test_groups = test_groups[index_keep]
++#
++#   # equalize train, tune and test sets
++#   train_groups = train_groups[1:length(test_groups)]
++#   tune_groups = tune_groups[1:length(test_groups)]
++#
++#   # make sets
++#   train_sets <- lapply(train_groups, get_row_ids)
++#   tune_sets <- lapply(tune_groups, get_row_ids)
++#   test_sets <- lapply(test_groups, get_row_ids)
++#
++#   # test tune and test
++#   test_1 = vapply(seq_along(train_groups), function(i) {
++#     mondf(
++#       tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
++#       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
++#     )
++#   }, FUN.VALUE = numeric(1L))
++#   stopifnot(all(test_1 == 1 + gap_tune))
++#   # test_2 = vapply(seq_along(train_groups), function(i) {
++#   #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
++#   # }, FUN.VALUE = numeric(1L))
++#   # stopifnot(all(test_2 == 1))
++#   test_3 = vapply(seq_along(train_groups), function(i) {
++#     mondf(
++#       tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
++#       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
++#     )
++#   }, FUN.VALUE = numeric(1L))
++#   stopifnot(all(test_3 == 1 + gap_test))
++#   # test_4 = vapply(seq_along(train_groups), function(i) {
++#   #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
++#   # }, FUN.VALUE = numeric(1L))
++#   # stopifnot(all(test_2 == 1))
++#
++#   # create inner and outer resamplings
++#   custom_inner$instantiate(task, train_sets, tune_sets)
++#   inner_sets = lapply(seq_along(train_groups), function(i) {
++#     c(train_sets[[i]], tune_sets[[i]])
++#   })
++#   custom_outer$instantiate(task, inner_sets, test_sets)
++#   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
++# }
++
++# # visualize test
++# prepare_cv_plot = function(x, set = "train") {
++#   x = lapply(x, function(x) data.table(ID = x))
++#   x = rbindlist(x, idcol = "fold")
++#   x[, fold := as.factor(fold)]
++#   x[, set := as.factor(set)]
++#   x[, ID := as.numeric(ID)]
++# }
++# plot_cv = function(cv, n = 5) {
++#   cv_test_inner = cv$custom_inner
++#   cv_test_outer = cv$custom_outer
++#
++#   # define task
++#   if (cv_test_inner$id == "taskRetQuarter") {
++#     task_ = task_ret_quarter$clone()
++#   } else if (cv_test_inner$id == "taskRetMonth2") {
++#     task_ = task_ret_month2$clone()
++#   } else if (cv_test_inner$id == "taskRetMonth") {
++#     task_ = task_ret_month$clone()
++#   } else if (cv_test_inner$id == "taskRetWeek") {
++#     task_ = task_ret_week$clone()
++#   }
++#
++#   # prepare train, tune and test folds
++#   train_sets = cv_test_inner$instance$train[1:n]
++#   train_sets = prepare_cv_plot(train_sets)
++#   tune_sets = cv_test_inner$instance$test[1:n]
++#   tune_sets = prepare_cv_plot(tune_sets, set = "tune")
++#   test_sets = cv_test_outer$instance$test[1:n]
++#   test_sets = prepare_cv_plot(test_sets, set = "test")
++#   dt_vis = rbind(train_sets, tune_sets, test_sets)
++#   ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
++#     geom_point() +
++#     theme_minimal() +
++#     coord_flip() +
++#     labs(x = "", y = '', title = cv_test_inner$id)
++# }
++# plots = lapply(custom_cvs[c(1, 4, 7, 11)], plot_cv, n = 12)
++# wp = wrap_plots(plots)
++# ggsave("plot_cv.png", plot = wp, width = 10, height = 8, dpi = 300)
++
+ 
+ # ADD PIPELINES -----------------------------------------------------------
+ print("Add pipelines")
+@@ -417,8 +560,12 @@ mlr_measures$add("portfolio_ret", PortfolioRet)
+ 
+ # GRAPH V2 ----------------------------------------------------------------
+ # graph template
++gr = gunion(list(
++  po("nop", id = "nop_union_pca"),
++  po("pca", center = FALSE, rank. = 100)
++)) %>>% po("featureunion")
+ graph_template =
+-  # po("subsample") %>>% # uncomment this for hyperparameter tuning
++  po("subsample") %>>% # uncomment this for hyperparameter tuning
+   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+   po("dropna", id = "dropna") %>>%
+   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+@@ -435,13 +582,14 @@ graph_template =
+   )) %>>%
+   po("unbranch", id = "scale_unbranch") %>>%
+   po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
+   # filters
+   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+               po("filter", filter = flt("relief"), filter.frac = 0.05),
+               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0)
+   )) %>>%
+-  # po("nop", id = "nop_filter"))) %>>%
+   po("unbranch", id = "filter_unbranch") %>>%
+   # modelmatrix
+   po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
+@@ -601,16 +749,6 @@ as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
+ search_space_kknn = ps(
+   # subsample for hyperband
+   # subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+-  # preprocessing
+-  # dropnacol.affect_columns = p_fct(
+-  #   levels = c("0.01", "0.05", "0.10"),
+-  #   trafo = function(x, param_set) {
+-  #     switch(x,
+-  #            "0.01" = 0.01,
+-  #            "0.05" = 0.05,
+-  #            "0.10" = 0.1)
+-  #   }
+-  # ),
+   dropcorr.cutoff = p_fct(
+     levels = c("0.80", "0.90", "0.95", "0.99"),
+     trafo = function(x, param_set) {
+@@ -666,6 +804,38 @@ search_space_nnet$add(
+      regr.nnet.maxit = p_int(lower = 50, upper = 500))
+ )
+ 
++# glmnet graph
++graph_glmnet = graph_template %>>%
++  po("learner", learner = lrn("regr.glmnet"))
++graph_glmnet = as_learner(graph_glmnet)
++as.data.table(graph_glmnet$param_set)[, .(id, class, lower, upper, levels)]
++search_space_glmnet = ps(
++  # subsample for hyperband
++  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
++  dropcorr.cutoff = p_fct(
++    levels = c("0.80", "0.90", "0.95", "0.99"),
++    trafo = function(x, param_set) {
++      switch(x,
++             "0.80" = 0.80,
++             "0.90" = 0.90,
++             "0.95" = 0.95,
++             "0.99" = 0.99)
++    }
++  ),
++  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
++  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
++  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
++  # scaling
++  scale_branch.selection = p_fct(levels = c("uniformization", "scale")),
++  # filters
++  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
++  # interaction
++  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
++  # learner
++  regr.glmnet.s     = p_int(lower = 5, upper = 30),
++  regr.glmnet.alpha = p_dbl(lower = 1e-4, upper = 1e4, logscale = TRUE)
++)
++
+ 
+ ### THIS LEARNER UNSTABLE ####
+ # ksvm graph
+@@ -724,6 +894,8 @@ graph_template =
+   )) %>>%
+   po("unbranch", id = "scale_unbranch") %>>%
+   po("dropna", id = "dropna_v2") %>>%
++  # add pca columns
++  gr %>>%
+   # filters
+   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+@@ -1019,7 +1191,7 @@ designs = do.call(rbind, designs_l)
+ print("Create registry")
+ packages = c("data.table", "gausscov", "paradox", "mlr3", "mlr3pipelines",
+              "mlr3tuning", "mlr3misc", "future", "future.apply",
+-             "mlr3extralearners")
++             "mlr3extralearners", "stats")
+ reg = makeExperimentRegistry(
+   file.dir = "./experiments2",
+   seed = 1,
diff --git a/generate_predictors.R b/generate_predictors.R
index 11465a8..fad72aa 100644
--- a/generate_predictors.R
+++ b/generate_predictors.R
@@ -42,28 +42,17 @@ warnigns$filterwarnings('ignore')
 
 # SET UP ------------------------------------------------------------------
 # check if we have all necessary env variables
-# assert_choice("AWS-ACCESS-KEY", names(Sys.getenv()))
-# assert_choice("AWS-SECRET-KEY", names(Sys.getenv()))
-# assert_choice("AWS-REGION", names(Sys.getenv()))
 assert_choice("BLOB-ENDPOINT", names(Sys.getenv()))
 assert_choice("BLOB-KEY", names(Sys.getenv()))
 assert_choice("APIKEY-FMPCLOUD", names(Sys.getenv()))
 assert_choice("FRED-KEY", names(Sys.getenv()))
 
-# # set credentials
-# config <- tiledb_config()
-# config["vfs.s3.aws_access_key_id"] <- Sys.getenv("AWS-ACCESS-KEY")
-# config["vfs.s3.aws_secret_access_key"] <- Sys.getenv("AWS-SECRET-KEY")
-# config["vfs.s3.region"] <- Sys.getenv("AWS-REGION")
-# context_with_config <- tiledb_ctx(config)
-# fredr_set_key(Sys.getenv("FRED-KEY"))
-
 # global vars
-PATH = "F:/equity/usa"
+PATH = "F:/data/equity/us"
 
 # parameters
 strategy = "PEAD"  # PEAD (for predicting post announcement drift) or PRE (for predicting pre announcement)
-events_data <- "intersection" # data source, one of "fmp", "investingcom", "intersection"
+events_data = "intersection" # data source, one of "fmp", "investingcom", "intersection"
 
 
 # EARING ANNOUNCEMENT DATA ------------------------------------------------
@@ -160,7 +149,7 @@ events[date == max(date), symbol]
 # MARKET DATA AND FUNDAMENTALS ---------------------------------------------
 # import factors
 prices_dt = read_parquet(fs::path(PATH,
-                                  "predictors-daily",
+                                  "predictors_daily",
                                   "factors",
                                   "prices_factors",
                                   ext = "parquet"))
@@ -177,11 +166,12 @@ prices_dt <- prices_dt[symbol %in% prices_n$symbol]
 # SPY data
 con <- dbConnect(duckdb::duckdb())
 symbol = "SPY"
+path_ = "F:/data/equity/daily_fmp_all.csv"
 query <- sprintf("
   SELECT *
-  FROM 'F:/equity/daily_fmp_all.csv'
+  FROM '%s'
   WHERE Symbol = '%s'
-", symbol)
+", path_, symbol)
 data_ <- dbGetQuery(con, query)
 dbDisconnect(con)
 data_ = as.data.table(data_)
@@ -190,7 +180,6 @@ data_[, returns := close / shift(close) - 1]
 spy = na.omit(data_)
 
 
-
 # REGRESSION LABELING ----------------------------------------------------------
 # calculate returns
 setorder(prices_dt, symbol, date)
@@ -555,7 +544,16 @@ if (length(at_) > 0) {
 # Wavelet arima
 print("Wavelet predictors")
 at_ = get_at_(RollingWaveletArimaFeatures)
-if (length(at_) > 0) {
+if (is.null(RollingWaveletArimaFeatures)) {
+  RollingWaveletArimaInstance = RollingWaveletArima$new(windows = 252, workers = 6L,
+                                                        lag = lag_, at = at_, filter = "haar")
+  RollingWaveletArimaFeaturesNew = RollingWaveletArimaInstance$get_rolling_features(OhlcvInstance)
+  gc()
+  RollingWaveletArimaFeaturesNew[, date := as.IDate(date)]
+  time_ <- format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+  fwrite(RollingWaveletArimaFeaturesNew,
+         paste0("D:/features/PEAD-RollingWaveletArimaFeatures-", time_, ".csv"))
+} else if (length(at_) > 0) {
   RollingWaveletArimaInstance = RollingWaveletArima$new(windows = 252, workers = 6L,
                                                         lag = lag_, at = at_, filter = "haar")
   RollingWaveletArimaFeaturesNew = RollingWaveletArimaInstance$get_rolling_features(OhlcvInstance)
diff --git a/plot_cv.png b/plot_cv.png
deleted file mode 100644
index 93a4e54..0000000
Binary files a/plot_cv.png and /dev/null differ
diff --git a/run_month.sh b/run_month.sh
index b2872ad..0af907d 100644
--- a/run_month.sh
+++ b/run_month.sh
@@ -2,8 +2,8 @@
 
 #PBS -N PEAD
 #PBS -l ncpus=4
-#PBS -l mem=8GB
-#PBS -J 1-10000
+#PBS -l mem=10GB
+#PBS -J 10000-16456
 #PBS -o experimentsmonth/logs
 #PBS -j oe
 

4b196ec412223856ef1a5fa2e24454f8e2c018a3 unknown Tue Dec 5 14:23:23 2023 +0100 experiments month
diff --git a/run_month.sh b/run_month.sh
index 97e945e..b2872ad 100644
--- a/run_month.sh
+++ b/run_month.sh
@@ -2,8 +2,8 @@
 
 #PBS -N PEAD
 #PBS -l ncpus=4
-#PBS -l mem=10GB
-#PBS -J 1-15000
+#PBS -l mem=8GB
+#PBS -J 1-10000
 #PBS -o experimentsmonth/logs
 #PBS -j oe
 

3da7d31ebbbb871b85675522100fb19e83789e47 unknown Tue Dec 5 13:40:03 2023 +0100 experiments month
diff --git a/run_month.sh b/run_month.sh
new file mode 100644
index 0000000..97e945e
--- /dev/null
+++ b/run_month.sh
@@ -0,0 +1,13 @@
+#!/bin/bash
+
+#PBS -N PEAD
+#PBS -l ncpus=4
+#PBS -l mem=10GB
+#PBS -J 1-15000
+#PBS -o experimentsmonth/logs
+#PBS -j oe
+
+cd ${PBS_O_WORKDIR}
+apptainer run image.sif run_job.R
+
+# 1-16456

3148c8bafc9b6399373489783d9ac65c152ff25a unknown Tue Dec 5 13:16:42 2023 +0100 experiments month
diff --git a/run_job.R b/run_job.R
index bcbc73e..4080029 100644
--- a/run_job.R
+++ b/run_job.R
@@ -108,7 +108,7 @@ UpdateBuffer = R6Class(
 if (interactive()) {
   reg = loadRegistry("experiments_test")
 } else {
-  reg = loadRegistry("experiments")
+  reg = loadRegistry("experimentsmonth")
 }
 
 # extract integer

76e12bfbb0eb95175cc85b23724638b96f532b2d unknown Mon Dec 4 14:37:01 2023 +0100 new month
diff --git a/run_padobran.sh b/run_padobran.sh
index f3b5710..86438d9 100644
--- a/run_padobran.sh
+++ b/run_padobran.sh
@@ -1,8 +1,7 @@
 #!/bin/bash
 
 #PBS -N PEADPREPAREMONTH
-#PBS -l mem=55GB
+#PBS -l mem=65GB
 
 cd ${PBS_O_WORKDIR}
 apptainer run image.sif run_padobran.R
-

35bfc12caedb6d364a1b316b3b58559466890e36 unknown Sun Dec 3 23:38:25 2023 +0100 new month
diff --git a/run_padobran.sh b/run_padobran.sh
new file mode 100644
index 0000000..f3b5710
--- /dev/null
+++ b/run_padobran.sh
@@ -0,0 +1,8 @@
+#!/bin/bash
+
+#PBS -N PEADPREPAREMONTH
+#PBS -l mem=55GB
+
+cd ${PBS_O_WORKDIR}
+apptainer run image.sif run_padobran.R
+

b65914a60bfc56bf3540b066b01c7699d84d14d2 unknown Sun Dec 3 23:28:40 2023 +0100 new month
diff --git a/run_padobran.R b/run_padobran.R
index 055edec..49582ca 100644
--- a/run_padobran.R
+++ b/run_padobran.R
@@ -978,7 +978,7 @@ designs_l = lapply(custom_cvs, function(cv_) {
 
     # objects for all autotuners
     measure_ = msr("portfolio_ret")
-    tuner_   = tnr("hyperband", eta = 4)
+    tuner_   = tnr("hyperband", eta = 5)
     # tuner_   = tnr("mbo")
     # term_evals = 20
 

df19b478dfbcf63be3fa4e3ec64b895415db9746 unknown Sun Dec 3 23:28:17 2023 +0100 new month
diff --git a/mlr3_winsorizationsimple.R b/mlr3_winsorizationsimple.R
index d32311e..ebe8bbd 100644
--- a/mlr3_winsorizationsimple.R
+++ b/mlr3_winsorizationsimple.R
@@ -50,8 +50,8 @@ PipeOpWinsorizeSimple = R6::R6Class(
     },
 
     .transform_dt  = function(dt, levels) {
-      dt = dt[, Map(function(a, b) fifelse(a < b, b, a), .SD, self$state$minvals)]
-      dt = dt[, Map(function(a, b) fifelse(a > b, b, a), .SD, self$state$maxvals)]
+      dt = dt[, Map(function(a, b) data.table::fifelse(a < b, b, a), .SD, self$state$minvals)]
+      dt = dt[, Map(function(a, b) data.table::fifelse(a > b, b, a), .SD, self$state$maxvals)]
       dt
     }
   )
diff --git a/paper_trading.R b/paper_trading.R
index fa007a4..43a227e 100644
--- a/paper_trading.R
+++ b/paper_trading.R
@@ -66,14 +66,14 @@ data_tbl <- fread("D:/features/pead-predictors-20231031.csv")
 # convert tibble to data.table
 DT = as.data.table(data_tbl)
 
-# create group variable
-DT[, date_rolling := as.IDate(date_rolling)]
-DT[, yearmonthid := round(date_rolling, digits = "month")]
-DT[, weekid := round(date_rolling, digits = "week")]
-DT[, .(date, date_rolling, yearmonthid, weekid)]
-DT[, yearmonthid := as.integer(yearmonthid)]
-DT[, weekid := as.integer(weekid)]
-DT[, .(date, date_rolling, yearmonthid, weekid)]
+  # create group variable
+  DT[, date_rolling := as.IDate(date_rolling)]
+  DT[, yearmonthid := round(date_rolling, digits = "month")]
+  DT[, weekid := round(date_rolling, digits = "week")]
+  DT[, .(date, date_rolling, yearmonthid, weekid)]
+  DT[, yearmonthid := as.integer(yearmonthid)]
+  DT[, weekid := as.integer(weekid)]
+  DT[, .(date, date_rolling, yearmonthid, weekid)]
 
 # remove industry and sector vars
 DT[, `:=`(industry = NULL, sector = NULL)]
@@ -760,10 +760,13 @@ lapply(custom_cvs, function(cv_) {
   # get last
   i = cv_inner$iters
 
+  task_inner = task_ret_week$clone()
+  task_inner$filter(c(cv_inner$train_set(i), cv_inner$test_set(i)))
+
   # inner resampling
   custom_ = rsmp("custom")
   custom_$id = paste0("custom_", cv_inner$iters, "_", i)
-  custom_$instantiate(task_ret_week,
+  custom_$instantiate(task_inner,
                       list(cv_inner$train_set(i)),
                       list(cv_inner$test_set(i)))
 
@@ -919,7 +922,7 @@ lapply(custom_cvs, function(cv_) {
   )
 
   # benchmark
-  # plan("multisession", 2)
+  # plan("multisession", workers = 4)
   bmr = benchmark(design, store_models = FALSE)
 
   # save locally and to list
@@ -927,3 +930,8 @@ lapply(custom_cvs, function(cv_) {
   file_name = paste0("paper-pead", cv_$custom_inner$iters, "-", time_, ".rds")
   saveRDS(bmr, file.path(mlr3_save_path, file_name))
 })
+
+# inspect results
+
+
+
diff --git a/results_light.R b/results_light.R
index 4718ee3..6806aa6 100644
--- a/results_light.R
+++ b/results_light.R
@@ -38,7 +38,7 @@ rbind(ids_notdone, ids_done[job.id %in% results_files])
 plan("multisession", workers = 4L)
 start_time = Sys.time()
 results = future_lapply(ids_done[, job.id], function(id_) {
-  # id_ = 2110
+  # id_ = 1
   print(id_)
   # bmr object
   bmrs = reduceResultsBatchmark(id_, store_backends = FALSE, reg = reg)
@@ -92,16 +92,17 @@ get_backend = function(task_name = "taskRetWeek") {
 }
 id_cols = c("symbol", "date", "yearmonthid", "..row_id", "epsDiff", "nincr", "nincr2y", "nincr3y")
 taskRetWeek    = get_backend()
-taskRetMonth   = get_backend("taskRetMonth")
-taskRetMonth2  = get_backend("taskRetMonth2")
-taskRetQuarter = get_backend("taskRetQuarter")
-test = all(c(identical(taskRetWeek, taskRetMonth),
-             identical(taskRetWeek, taskRetMonth2),
-             identical(taskRetWeek, taskRetQuarter)))
+# taskRetMonth   = get_backend("taskRetMonth")
+# taskRetMonth2  = get_backend("taskRetMonth2")
+# taskRetQuarter = get_backend("taskRetQuarter")
+# test = all(c(identical(taskRetWeek, taskRetMonth),
+#              identical(taskRetWeek, taskRetMonth2),
+#              identical(taskRetWeek, taskRetQuarter)))
 print(test)
 if (test) {
   backend = copy(taskRetWeek)
   setnames(backend, "..row_id", "row_ids")
+
   rm(list = c("taskRetWeek", "taskRetMonth", "taskRetMonth2", "taskRetQuarter"))
   rm(list = c("task_ret_week", "task_ret_month", "task_ret_month2", "task_ret_quarter"))
 }
@@ -124,6 +125,9 @@ setnames(predictions,
 
 
 # PREDICTIONS RESULTS -----------------------------------------------------
+# remove dupliactes - keep firt
+predictions = unique(predictions, by = c("row_ids", "date", "task", "learner", "cv"))
+
 # predictions
 predictions[, `:=`(
   truth_sign = as.factor(sign(truth)),
@@ -182,13 +186,13 @@ predictions_ensemble[, (cols_sign_response_neg) := lapply(sign_response_seq, fun
 predictions_ensemble[median_response > 0 & sd_response < 0.15, .(tr = truth_sign, res = 1)][, sum(tr == res) / length(tr)]
 
 # check only sign ensamble performance
-res = lapply(cols_sign_response_pos[1:5], function(x) {
-  print(x)
-  predictions_ensemble[get(x) == TRUE & sd_response < 0.3][
-    , mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = c("task")]
-})
-names(res) = cols_sign_response_pos
-res
+# res = lapply(cols_sign_response_pos[1:5], function(x) {
+#   print(x)
+#   predictions_ensemble[get(x) == TRUE & sd_response < 0.3][
+#     , mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = c("task")]
+# })
+# names(res) = cols_sign_response_pos
+# res
 
 # check only sign ensamble performance all
 res = lapply(cols_sign_response_pos, function(x) {
@@ -357,3 +361,10 @@ gausscov = rbindlist(gausscov, idcol = "task")
 gausscov[, sum(value), by = variable][order(V1)][, tail(.SD, 10)]
 gausscov[, sum(value), by = .(task, variable)][order(V1)][, tail(.SD, 5), by = task]
 
+
+# ISSUES ------------------------------------------------------------------
+# slow importing
+res_test = loadResult(1, reg = reg)
+
+
+
diff --git a/run_padobran.R b/run_padobran.R
index 10f49c4..055edec 100644
--- a/run_padobran.R
+++ b/run_padobran.R
@@ -263,7 +263,7 @@ nested_cv_split = function(task,
 }
 
 # generate cv's
-train_sets = seq(24, 12 * 3, 12)
+train_sets = seq(12, 12 * 4, 12)
 gap_sets = c(0:3)
 mat = cbind(train = train_sets)
 expanded_list  = lapply(gap_sets, function(v) {
@@ -720,7 +720,7 @@ graph_nnet = as_learner(graph_nnet)
 as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
 search_space_nnet = search_space_template$clone()
 search_space_nnet$add(
-  ps(regr.nnet.size  = p_int(lower = 2, upper = 25),
+  ps(regr.nnet.size  = p_int(lower = 2, upper = 15),
      regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
      regr.nnet.maxit = p_int(lower = 50, upper = 500))
 )
@@ -966,18 +966,21 @@ designs_l = lapply(custom_cvs, function(cv_) {
       task_ = task_ret_quarter$clone()
     }
 
+    task_inner = task_ret_week$clone()
+    task_inner$filter(c(cv_inner$train_set(i), cv_inner$test_set(i)))
+
     # inner resampling
     custom_ = rsmp("custom")
     custom_$id = paste0("custom_", cv_inner$iters, "_", i)
-    custom_$instantiate(task_ret_week,
+    custom_$instantiate(task_inner,
                         list(cv_inner$train_set(i)),
                         list(cv_inner$test_set(i)))
 
     # objects for all autotuners
     measure_ = msr("portfolio_ret")
-    # tuner_   = tnr("hyperband", eta = 4)
-    tuner_   = tnr("mbo")
-    term_evals = 20
+    tuner_   = tnr("hyperband", eta = 4)
+    # tuner_   = tnr("mbo")
+    # term_evals = 20
 
     # auto tuner rf
     at_rf = auto_tuner(
@@ -986,8 +989,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_rf,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner xgboost
@@ -997,8 +1000,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_xgboost,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner BART
@@ -1008,8 +1011,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_bart,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner nnet
@@ -1019,8 +1022,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_nnet,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner lightgbm
@@ -1030,8 +1033,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_lightgbm,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner earth
@@ -1041,8 +1044,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_earth,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner kknn
@@ -1052,8 +1055,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_kknn,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner gbm
@@ -1063,8 +1066,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_gbm,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner rsm
@@ -1074,8 +1077,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_rsm,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner rsm
@@ -1085,8 +1088,8 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_bart,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # auto tuner catboost
@@ -1096,8 +1099,19 @@ designs_l = lapply(custom_cvs, function(cv_) {
       resampling = custom_,
       measure = measure_,
       search_space = search_space_catboost,
-      # terminator = trm("none")
-      term_evals = term_evals
+      terminator = trm("none")
+      # term_evals = term_evals
+    )
+
+    # auto tuner glmnet
+    at_glmnet = auto_tuner(
+      tuner = tuner_,
+      learner = graph_glmnet,
+      resampling = custom_,
+      measure = measure_,
+      search_space = search_space_glmnet,
+      terminator = trm("none")
+      # term_evals = term_evals
     )
 
     # outer resampling
@@ -1107,9 +1121,9 @@ designs_l = lapply(custom_cvs, function(cv_) {
 
     # nested CV for one round
     design = benchmark_grid(
-      tasks = task_,
+      tasks = task_ret_week,
       learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth,
-                      at_kknn, at_gbm, at_rsm, at_bart, at_catboost),
+                      at_kknn, at_gbm, at_rsm, at_bart, at_catboost, at_glmnet),
       resamplings = customo_
     )
   })
@@ -1122,7 +1136,7 @@ if (interactive()) {
   dirname_ = "experiments_test"
   if (dir.exists(dirname_)) system(paste0("rm -r ", dirname_))
 } else {
-  dirname_ = "experiments"
+  dirname_ = "experimentsmonth"
 }
 
 # create registry
@@ -1137,4 +1151,23 @@ print("Batchmark")
 batchmark(designs, reg = reg)
 
 # save registry
+print("Save registry")
 saveRegistry(reg = reg)
+
+# create sh file
+sh_file = sprintf("
+#!/bin/bash
+
+#PBS -N PEAD
+#PBS -l ncpus=4
+#PBS -l mem=10GB
+#PBS -J 1-%d
+#PBS -o experiments/logs
+#PBS -j oe
+
+cd ${PBS_O_WORKDIR}
+apptainer run image.sif run_job.R
+", nrow(designs))
+sh_file_name = "run_month.sh"
+file.create(sh_file_name)
+writeLines(sh_file, sh_file_name)
diff --git a/run_padobran_week.R b/run_padobran_week.R
index 962cc9b..5af7e2a 100644
--- a/run_padobran_week.R
+++ b/run_padobran_week.R
@@ -537,7 +537,7 @@ graph_nnet = as_learner(graph_nnet)
 as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
 search_space_nnet = search_space_template$clone()
 search_space_nnet$add(
-  ps(regr.nnet.size  = p_int(lower = 2, upper = 25),
+  ps(regr.nnet.size  = p_int(lower = 2, upper = 20),
      regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
      regr.nnet.maxit = p_int(lower = 50, upper = 500))
 )
