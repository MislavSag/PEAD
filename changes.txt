commit a77676bb94967a62cd9c9dc205100542cb7d03c6
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Oct 4 13:14:14 2023 +0200

    save impor vars

diff --git a/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds b/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds
deleted file mode 100644
index bca3829..0000000
Binary files a/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds and /dev/null differ
diff --git a/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds b/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds
deleted file mode 100644
index 698a836..0000000
Binary files a/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds and /dev/null differ
diff --git a/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds b/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds
deleted file mode 100644
index fe44b67..0000000
Binary files a/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds and /dev/null differ
diff --git a/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds b/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds
deleted file mode 100644
index 1ea6b33..0000000
Binary files a/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds and /dev/null differ

commit 159c5d0705fe623289f611be71717d5e5d54a448
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Oct 4 13:13:48 2023 +0200

    save impor vars

diff --git a/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds b/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds
new file mode 100644
index 0000000..bca3829
Binary files /dev/null and b/H4-jobarray-/40-taskRetQuarter-91-20231004121303.rds differ
diff --git a/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds b/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds
new file mode 100644
index 0000000..698a836
Binary files /dev/null and b/H4-jobarray-/41-taskRetQuarter-79-20231004124946.rds differ
diff --git a/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds b/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds
new file mode 100644
index 0000000..fe44b67
Binary files /dev/null and b/gausscov_f3/gausscov_f3-taskRetQuarter-316238114429523.rds differ
diff --git a/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds b/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds
new file mode 100644
index 0000000..1ea6b33
Binary files /dev/null and b/gausscov_f3/gausscov_f3-taskRetQuarter-678306160811349.rds differ
diff --git a/mlr3_gausscov_f3st.R b/mlr3_gausscov_f3st.R
index 6f9ea93..fbe2ff1 100644
--- a/mlr3_gausscov_f3st.R
+++ b/mlr3_gausscov_f3st.R
@@ -66,6 +66,18 @@ FilterGausscovF3st = R6::R6Class(
       res_index  <- res_index [res_index  != 0]
 
       scores[res_index] = 1
+
+      # save scores
+      dir_name = "./gausscov_f3"
+      if (!dir.exists(dir_name)) {
+        dir.create(dir_name)
+      }
+      random_id <- paste0(sample(0:9, 15, replace = TRUE), collapse = "")
+      file_name = paste0("gausscov_f3-", task$id, "-", random_id, ".rds")
+      file_name = file.path(dir_name, file_name)
+      saveRDS(scores, file_name)
+
+      # return scores
       sort(scores, decreasing = TRUE)
     }
   )
diff --git a/postscriptum_light.R b/postscriptum_light.R
index b10144e..b2bfa1c 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -389,7 +389,7 @@ plot(as.xts.data.table(sample_[, .N, by = date]))
 
 # calculate indicator
 indicator = sample_[, .(ind = median(median_response)), by = "date"]
-indicator[, ind_ema := TTR::EMA(ind, 5, na.rm = TRUE)]
+indicator[, ind_ema := TTR::EMA(ind, 10, na.rm = TRUE)]
 indicator = na.omit(indicator)
 plot(as.xts.data.table(indicator))
 plot(as.xts.data.table(indicator)[, 2])
@@ -461,10 +461,3 @@ tvar = TVAR(
   plot = FALSE
 )
 summary(tvar)
-
-# var predictions
-runner(
-
-)
-
-
diff --git a/run.R b/run.R
index 057a096..f2132e8 100644
--- a/run.R
+++ b/run.R
@@ -543,7 +543,8 @@ graph_template =
   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
               po("filter", filter = flt("relief"), filter.frac = 0.05),
-              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
+              po("filter", filter = flt("gausscov_f3st"), m = 2, filter.cutoff = 0)
+              )) %>>%
   # po("nop", id = "nop_filter"))) %>>%
   po("unbranch", id = "filter_unbranch") %>>%
   # modelmatrix
@@ -788,7 +789,8 @@ graph_template =
   po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
               po("filter", filter = flt("relief"), filter.frac = 0.05),
-              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
+              po("filter", filter = flt("gausscov_f3st"), m = 2, filter.cutoff = 0)
+              )) %>>%
   po("unbranch", id = "filter_unbranch")
 search_space_template = ps(
   # subsample for hyperband
@@ -1050,9 +1052,9 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    # learners = list(at_bart),
-    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-                    at_gbm, at_rsm, at_bart),
+    learners = list(at_rf),
+    # learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
+    #                 at_gbm, at_rsm, at_bart),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
@@ -1077,8 +1079,8 @@ start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # i = 50
-  # cv_ = custom_cvs[[12]]
+  # i = 41
+  # cv_ = custom_cvs[[11]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner

commit 60747e2287366ec9198609456ef4deecbfdf4435
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Mon Oct 2 15:34:05 2023 +0200

    remove plots

diff --git a/run.R b/run.R
index 87596fe..057a096 100644
--- a/run.R
+++ b/run.R
@@ -433,45 +433,45 @@ nested_cv_split_expanding = function(task,
 #   c(test1, test2)
 # })
 
-# visualize test
-prepare_cv_plot = function(x, set = "train") {
-  x = lapply(x, function(x) data.table(ID = x))
-  x = rbindlist(x, idcol = "fold")
-  x[, fold := as.factor(fold)]
-  x[, set := as.factor(set)]
-  x[, ID := as.numeric(ID)]
-}
-plot_cv = function(cv, n = 5) {
-  cv_test_inner = cv$custom_inner
-  cv_test_outer = cv$custom_outer
-
-  # define task
-  if (cv_test_inner$id == "taskRetQuarter") {
-    task_ = task_ret_quarter$clone()
-  } else if (cv_test_inner$id == "taskRetMonth2") {
-    task_ = task_ret_month2$clone()
-  } else if (cv_test_inner$id == "taskRetMonth") {
-    task_ = task_ret_month$clone()
-  } else if (cv_test_inner$id == "taskRetWeek") {
-    task_ = task_ret_week$clone()
-  }
-
-  # prepare train, tune and test folds
-  train_sets = cv_test_inner$instance$train[1:n]
-  train_sets = prepare_cv_plot(train_sets)
-  tune_sets = cv_test_inner$instance$test[1:n]
-  tune_sets = prepare_cv_plot(tune_sets, set = "tune")
-  test_sets = cv_test_outer$instance$test[1:n]
-  test_sets = prepare_cv_plot(test_sets, set = "test")
-  dt_vis = rbind(train_sets, tune_sets, test_sets)
-  ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
-    geom_point() +
-    theme_minimal() +
-    coord_flip() +
-    labs(x = "", y = '', title = cv_test_inner$id)
-}
-plots = lapply(custom_cvs[c(1, 4, 7, 11)], plot_cv, n = 12)
-wrap_plots(plots)
+# # visualize test
+# prepare_cv_plot = function(x, set = "train") {
+#   x = lapply(x, function(x) data.table(ID = x))
+#   x = rbindlist(x, idcol = "fold")
+#   x[, fold := as.factor(fold)]
+#   x[, set := as.factor(set)]
+#   x[, ID := as.numeric(ID)]
+# }
+# plot_cv = function(cv, n = 5) {
+#   cv_test_inner = cv$custom_inner
+#   cv_test_outer = cv$custom_outer
+#
+#   # define task
+#   if (cv_test_inner$id == "taskRetQuarter") {
+#     task_ = task_ret_quarter$clone()
+#   } else if (cv_test_inner$id == "taskRetMonth2") {
+#     task_ = task_ret_month2$clone()
+#   } else if (cv_test_inner$id == "taskRetMonth") {
+#     task_ = task_ret_month$clone()
+#   } else if (cv_test_inner$id == "taskRetWeek") {
+#     task_ = task_ret_week$clone()
+#   }
+#
+#   # prepare train, tune and test folds
+#   train_sets = cv_test_inner$instance$train[1:n]
+#   train_sets = prepare_cv_plot(train_sets)
+#   tune_sets = cv_test_inner$instance$test[1:n]
+#   tune_sets = prepare_cv_plot(tune_sets, set = "tune")
+#   test_sets = cv_test_outer$instance$test[1:n]
+#   test_sets = prepare_cv_plot(test_sets, set = "test")
+#   dt_vis = rbind(train_sets, tune_sets, test_sets)
+#   ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
+#     geom_point() +
+#     theme_minimal() +
+#     coord_flip() +
+#     labs(x = "", y = '', title = cv_test_inner$id)
+# }
+# plots = lapply(custom_cvs[c(1, 4, 7, 11)], plot_cv, n = 12)
+# wrap_plots(plots)
 
 
 # ADD PIPELINES -----------------------------------------------------------

commit 918b72e0c73744d8e579c15b72d6307c226af1e2
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Mon Oct 2 15:16:37 2023 +0200

    fix array

diff --git a/run.sh b/run.sh
index 3ce6806..49b16d0 100644
--- a/run.sh
+++ b/run.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 
 #PBS -l ncpus=8
-#PBS -J 95
+#PBS -J 1-95
 
 cd ${PBS_O_WORKDIR}
 apptainer run image.sif run.R

commit 41300c8ccbb04792de67a29e90afeab31cd85382
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Mon Oct 2 15:13:17 2023 +0200

    add bart learner again from add to the last. Increase PB array index.

diff --git a/postscriptum_light.R b/postscriptum_light.R
index f41d342..b10144e 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -142,10 +142,14 @@ id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 bmr_files = list.files(list.files("F:/", pattern = "^H4-v7", full.names = TRUE), full.names = TRUE)
 
 # arrange files
-cv_ = as.integer(gsub("\\d+-.*-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
-i_ = as.integer(gsub("-.*-\\d+", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
-bmr_files = cbind.data.frame(bmr_files, cv = cv_, i = i_)
-setorder(bmr_files, cv, i)
+cv_   = as.integer(gsub("\\d+-.*-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
+i_    = as.integer(gsub("-.*-\\d+", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
+task_ = gsub(".*/\\d+-|-\\d+-\\d+.rds", "", bmr_files)
+bmr_files = cbind.data.frame(bmr_files, task = task_, cv = cv_, i = i_)
+setorder(bmr_files, task, cv, i)
+bmr_files[bmr_files$task == "taskRetWeek",]
+bmr_files[bmr_files$task == "taskRetMonth",]
+bmr_files[bmr_files$task == "taskRetMonth2",]
 
 # extract needed information from banchmark objects
 predictions_l = list()
@@ -193,19 +197,28 @@ for (i in 1:nrow(bmr_files)) {
   aggs_l[[i]] = agg_
 }
 
+# checks
+na_test = vapply(aggs_l, function(x) any(is.na(x)), FUN.VALUE = logical(1))
+which(na_test)
+aggs_l_naomit = aggs_l[-which(na_test)]
+
 # aggregated results
-aggregate_results = rbindlist(aggs_l, fill = TRUE)
+aggregate_results = rbindlist(aggs_l_naomit, fill = TRUE)
 cols = colnames(aggregate_results)[4:ncol(aggregate_results)]
 aggregate_results[, lapply(.SD, function(x) mean(x)), by = .(task_id, learner_id), .SDcols = cols]
-aggregate_results[, lapply(.SD, function(x) mean(x)), by = .(task_id, learner_id), .SDcols = cols]
 
 # predictions
-predictions_dt = rbindlist(predictions_l)
+predictions_l_naomit = predictions_l[-which(na_test)]
+predictions_dt = rbindlist(predictions_l_naomit)
 predictions_dt[, `:=`(
   truth_sign = as.factor(sign(truth)),
   response_sign = as.factor(sign(response))
 )]
 
+# number of predictions by task and cv
+unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c("task")]
+unique(predictions_dt, by = c("task", "learner", "cv", "row_ids"))[, .N, by = c("task", "cv")]
+
 # accuracy by ids
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv")]
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("task")]
@@ -244,9 +257,9 @@ predictions_dt_ensemble[, `:=`(
   # response_sign_sign_pos = sign_response > 15,
   # response_sign_sign_neg = sign_response < -15
 )]
-predictions_dt_ensemble = unique(predictions_dt_ensemble)
+predictions_dt_ensemble = unique(predictions_dt_ensemble, by = c("task", "row_ids"))
 sign_response_max = predictions_dt_ensemble[, max(sign_response)]
-sign_response_seq = seq(as.integer(sign_response_max / 2)-3, sign_response_max - 1)
+sign_response_seq = seq(as.integer(sign_response_max / 2), sign_response_max - 1)
 cols_sign_response_pos = paste0("response_sign_sign_pos", sign_response_seq)
 predictions_dt_ensemble[, (cols_sign_response_pos) := lapply(sign_response_seq, function(x) sign_response > x)]
 cols_sign_response_neg = paste0("response_sign_sign_neg", sign_response_seq)
@@ -293,7 +306,7 @@ res
 cont = storage_container(BLOBENDPOINT, "qc-backtest")
 lapply(unique(predictions_dt_ensemble$task), function(x) {
   # debug
-  # x = "taskRetMonth"
+  # x = "taskRetWeek"
 
   # prepare data
   y = predictions_dt_ensemble[task == x]
@@ -303,7 +316,11 @@ lapply(unique(predictions_dt_ensemble$task), function(x) {
   y = unique(y)
 
   # remove where all false
-  y = y[response_sign_sign_pos9 == TRUE]
+  y = y[response_sign_sign_pos16 == TRUE]
+
+  # min and max date
+  y[, min(date)]
+  y[, max(date)]
 
   # by date
   # cols_ = setdiff(cols, "date")
@@ -366,22 +383,24 @@ new_dt = sample_[, ..pos_cols] - sample_[, ..neg_cols]
 setnames(new_dt, gsub("pos", "net", pos_cols))
 sample_ = cbind(sample_, new_dt)
 sample_[, max(date)]
-# sample_ = sample_[date < as.Date("2018-02-01")]
+sample_ = sample_[date < sample_[, max(date)]]
+sample_ = sample_[date > sample_[, min(date)]]
 plot(as.xts.data.table(sample_[, .N, by = date]))
 
-indicator = sample_[, .(mean_response_agg = mean(mean_response)),
-                    by = "date"]
-indicator[, `:=`(
-  mean_response_agg_ema = TTR::EMA(mean_response_agg, 10, na.rm = TRUE)
-)
-]
-indicator = indicator[date > as.Date("2017-01-01") & date < as.Date("2023-01-01")]
+# calculate indicator
+indicator = sample_[, .(ind = median(median_response)), by = "date"]
+indicator[, ind_ema := TTR::EMA(ind, 5, na.rm = TRUE)]
+indicator = na.omit(indicator)
 plot(as.xts.data.table(indicator))
+plot(as.xts.data.table(indicator)[, 2])
 
+# create backtest data
 backtest_data =  merge(spy, indicator, by = "date", all.x = TRUE, all.y = FALSE)
 backtest_data = backtest_data[date > indicator[, min(date)]]
+backtest_data = backtest_data[date < indicator[, max(date)]]
 backtest_data[, signal := 1]
-backtest_data[shift(mean_response_agg_ema) < 0, signal := 0]
+backtest_data[shift(ind_ema) < 0, signal := 0]          # 1
+# backtest_data[shift(diff(mean_response_agg_ema, 5)) < -0.01, signal := 0] # 2
 backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1))])
 PerformanceAnalytics::charts.PerformanceSummary(backtest_data_xts)
 # backtest performance
@@ -403,3 +422,49 @@ Performance <- function(x) {
 }
 Performance(backtest_data_xts[, 1])
 Performance(backtest_data_xts[, 2])
+
+# analyse indicator
+library(forecast)
+ndiffs(as.xts.data.table(indicator)[, 1])
+plot(diff(as.xts.data.table(indicator)[, 1]))
+
+
+
+# MOVE THIS TO SOME OTHER PACKAGE -----------------------------------------
+# rolling var predictions
+library(runner)
+library(vars)
+library(tsDyn)
+
+# prepare dependent and independent vars in matrix form
+input_var = as.xts.data.table(backtest_data[, .(date, returns, ind)])
+input_var[, "ind"] = na.locf(input_var[, "ind"])
+
+# stat test
+ndiffs(input_var[, "ind"])
+input_var[, "ind"] = diff(input_var[, "ind"])
+input_var = na.omit(input_var)
+
+# var test
+var <- VAR(input_var, p=5, type="const")
+summary(var)
+
+# TVAR test
+tvar = TVAR(
+  data = input_var,
+  lag = 1,       # Number of lags to include in each regime
+  model = "TAR", # Whether the transition variable is taken in levels (TAR) or difference (MTAR)
+  nthresh = 2,   # Number of thresholds
+  thDelay = 1,   # 'time delay' for the threshold variable
+  trim = 0.05,   # trimming parameter indicating the minimal percentage of observations in each regime
+  mTh = 2,       # combination of variables with same lag order for the transition variable. Either a single value (indicating which variable to take) or a combination
+  plot = FALSE
+)
+summary(tvar)
+
+# var predictions
+runner(
+
+)
+
+
diff --git a/run.R b/run.R
index d3c3d7b..87596fe 100644
--- a/run.R
+++ b/run.R
@@ -253,13 +253,13 @@ nested_cv_split = function(task,
 
 # generate cv's
 train_sets = seq(12, 12 * 3, 12)
-validation_sets = train_sets / 12
 gap_sets = c(0:3)
-mat = cbind(train = train_sets, tune = validation_sets)
+mat = cbind(train = train_sets)
 expanded_list  = lapply(gap_sets, function(v) {
   cbind.data.frame(mat, gap = v)
 })
 cv_param_grid = rbindlist(expanded_list)
+cv_param_grid[ ,tune := 3]
 custom_cvs = list()
 for (i in 1:nrow(cv_param_grid)) {
   print(i)
@@ -269,8 +269,8 @@ for (i in 1:nrow(cv_param_grid)) {
                                       param_$train,
                                       param_$tune,
                                       1,
-                                      param_$gap+1,
-                                      param_$gap+1)
+                                      param_$gap,
+                                      param_$gap)
   } else if (param_$gap == 1) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month,
                                       param_$train,
@@ -384,31 +384,34 @@ nested_cv_split_expanding = function(task,
   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
 }
 
-# generate cv's for expanding windows
-custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
-                                                               train_length_start = 6,
-                                                               tune_length = 3,
-                                                               test_length = 1,
-                                                               gap_tune = 0,
-                                                               gap_test = 0)
-custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month,
-                                                               train_length_start = 6,
-                                                               tune_length = 3,
-                                                               test_length = 1,
-                                                               gap_tune = 1,
-                                                               gap_test = 1)
-custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month2,
-                                                               train_length_start = 6,
-                                                               tune_length = 3,
-                                                               test_length = 1,
-                                                               gap_tune = 2,
-                                                               gap_test = 2)
-custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_quarter,
-                                                               train_length_start = 6,
-                                                               tune_length = 3,
-                                                               test_length = 1,
-                                                               gap_tune = 3,
-                                                               gap_test = 3)
+############ UNCOMMENT THIS FOR EXPANDING WINDOWS ##############
+# # generate cv's for expanding windows
+# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
+#                                                                train_length_start = 6,
+#                                                                tune_length = 3,
+#                                                                test_length = 1,
+#                                                                gap_tune = 0,
+#                                                                gap_test = 0)
+# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month,
+#                                                                train_length_start = 6,
+#                                                                tune_length = 3,
+#                                                                test_length = 1,
+#                                                                gap_tune = 1,
+#                                                                gap_test = 1)
+# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month2,
+#                                                                train_length_start = 6,
+#                                                                tune_length = 3,
+#                                                                test_length = 1,
+#                                                                gap_tune = 2,
+#                                                                gap_test = 2)
+# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_quarter,
+#                                                                train_length_start = 6,
+#                                                                tune_length = 3,
+#                                                                test_length = 1,
+#                                                                gap_tune = 3,
+#                                                                gap_test = 3)
+############ UNCOMMENT THIS FOR EXPANDING WINDOWS ##############
+
 
 # test if tain , validation and tst set follow logic
 # lapply(seq_along(custom_cvs), function(i) {
@@ -430,45 +433,45 @@ custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_quarter,
 #   c(test1, test2)
 # })
 
-# # visualize test
-# prepare_cv_plot = function(x, set = "train") {
-#   x = lapply(x, function(x) data.table(ID = x))
-#   x = rbindlist(x, idcol = "fold")
-#   x[, fold := as.factor(fold)]
-#   x[, set := as.factor(set)]
-#   x[, ID := as.numeric(ID)]
-# }
-# plot_cv = function(cv, n = 5) {
-#   cv_test_inner = cv$custom_inner
-#   cv_test_outer = cv$custom_outer
-#
-#   # define task
-#   if (cv_test_inner$id == "taskRetQuarter") {
-#     task_ = task_ret_quarter$clone()
-#   } else if (cv_test_inner$id == "taskRetMonth2") {
-#     task_ = task_ret_month2$clone()
-#   } else if (cv_test_inner$id == "taskRetMonth") {
-#     task_ = task_ret_month$clone()
-#   } else if (cv_test_inner$id == "taskRetWeek") {
-#     task_ = task_ret_week$clone()
-#   }
-#
-#   # prepare train, tune and test folds
-#   train_sets = cv_test_inner$instance$train[1:n]
-#   train_sets = prepare_cv_plot(train_sets)
-#   tune_sets = cv_test_inner$instance$test[1:n]
-#   tune_sets = prepare_cv_plot(tune_sets, set = "tune")
-#   test_sets = cv_test_outer$instance$test[1:n]
-#   test_sets = prepare_cv_plot(test_sets, set = "test")
-#   dt_vis = rbind(train_sets, tune_sets, test_sets)
-#   ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
-#     geom_point() +
-#     theme_minimal() +
-#     coord_flip() +
-#     labs(x = "", y = '')
-# }
-# plots = lapply(custom_cvs[c(1, 4, 7, 11, 14)], plot_cv, n = 12)
-# wrap_plots(plots)
+# visualize test
+prepare_cv_plot = function(x, set = "train") {
+  x = lapply(x, function(x) data.table(ID = x))
+  x = rbindlist(x, idcol = "fold")
+  x[, fold := as.factor(fold)]
+  x[, set := as.factor(set)]
+  x[, ID := as.numeric(ID)]
+}
+plot_cv = function(cv, n = 5) {
+  cv_test_inner = cv$custom_inner
+  cv_test_outer = cv$custom_outer
+
+  # define task
+  if (cv_test_inner$id == "taskRetQuarter") {
+    task_ = task_ret_quarter$clone()
+  } else if (cv_test_inner$id == "taskRetMonth2") {
+    task_ = task_ret_month2$clone()
+  } else if (cv_test_inner$id == "taskRetMonth") {
+    task_ = task_ret_month$clone()
+  } else if (cv_test_inner$id == "taskRetWeek") {
+    task_ = task_ret_week$clone()
+  }
+
+  # prepare train, tune and test folds
+  train_sets = cv_test_inner$instance$train[1:n]
+  train_sets = prepare_cv_plot(train_sets)
+  tune_sets = cv_test_inner$instance$test[1:n]
+  tune_sets = prepare_cv_plot(tune_sets, set = "tune")
+  test_sets = cv_test_outer$instance$test[1:n]
+  test_sets = prepare_cv_plot(test_sets, set = "test")
+  dt_vis = rbind(train_sets, tune_sets, test_sets)
+  ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
+    geom_point() +
+    theme_minimal() +
+    coord_flip() +
+    labs(x = "", y = '', title = cv_test_inner$id)
+}
+plots = lapply(custom_cvs[c(1, 4, 7, 11)], plot_cv, n = 12)
+wrap_plots(plots)
 
 
 # ADD PIPELINES -----------------------------------------------------------
@@ -637,32 +640,6 @@ search_space_gbm$add(
   # ....
 )
 
-# BART graph
-# Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
-#   when list, drop must have length equal to x
-# This happened PipeOp regr.bart's $predict()
-graph_bart = graph_template %>>%
-  po("learner", learner = lrn("regr.bart"))
-graph_bart = as_learner(graph_bart)
-as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
-search_space_bart = search_space_template$clone()
-search_space_bart$add(
-  ps(regr.bart.k = p_int(lower = 1, upper = 10))
-  # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
-  # regr.bart.n_trees = p_int(lower = 10, upper = 100))
-)
-# chatgpt returns this
-# n_trees = p_int(lower = 10, upper = 100),
-# n_chains = p_int(lower = 1, upper = 5),
-# k = p_int(lower = 1, upper = 10),
-# m_try = p_int(lower = 1, upper = 13),
-# nu = p_dbl(lower = 0.1, upper = 10),
-# alpha = p_dbl(lower = 0.01, upper = 1),
-# beta = p_dbl(lower = 0.01, upper = 1),
-# burn = p_int(lower = 10, upper = 100),
-# iter = p_int(lower = 100, upper = 1000)
-
-
 # # catboost graph
 # ### REMOVED FROM MLR3EXTRALEARNERS FROM VERSION 0.7.0.
 # graph_catboost = graph_template %>>%
@@ -754,6 +731,7 @@ search_space_nnet$add(
      regr.nnet.maxit = p_int(lower = 50, upper = 500))
 )
 
+
 ### THIS LEARNER UNSTABLE ####
 # ksvm graph
 # graph_ksvm = graph_template %>>%
@@ -873,6 +851,32 @@ search_space_rsm$add(
 #   numerical expression has 4076 elements: only the first used
 # This happened PipeOp regr.rsm's $train()
 
+# BART graph
+# Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
+#   when list, drop must have length equal to x
+# This happened PipeOp regr.bart's $predict()
+graph_bart = graph_template %>>%
+  po("learner", learner = lrn("regr.bart"))
+graph_bart = as_learner(graph_bart)
+as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
+search_space_bart = search_space_template$clone()
+search_space_bart$add(
+  ps(regr.bart.k      = p_int(lower = 1, upper = 10),
+     regr.bart.numcut = p_int(lower = 10, upper = 200),
+     regr.bart.ntree  = p_int(lower = 50, upper = 500))
+)
+# chatgpt returns this
+# n_trees = p_int(lower = 10, upper = 100),
+# n_chains = p_int(lower = 1, upper = 5),
+# k = p_int(lower = 1, upper = 10),
+# m_try = p_int(lower = 1, upper = 13),
+# nu = p_dbl(lower = 0.1, upper = 10),
+# alpha = p_dbl(lower = 0.01, upper = 1),
+# beta = p_dbl(lower = 0.01, upper = 1),
+# burn = p_int(lower = 10, upper = 100),
+# iter = p_int(lower = 100, upper = 1000)
+
+
 # threads
 threads = as.integer(Sys.getenv("NCPUS"))
 set_threads(graph_rf, n = threads)
@@ -1008,6 +1012,16 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     terminator = trm("none")
   )
 
+  # auto tuner rsm
+  at_bart = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_bart,
+    resampling = custom_,
+    measure = measure_,
+    search_space = search_space_bart,
+    terminator = trm("none")
+  )
+
   # # auto tuner mboost
   # at_gamboost = auto_tuner(
   #   tuner = tnr("hyperband", eta = 5),
@@ -1036,9 +1050,9 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    # learners = list(at_rsm),
+    # learners = list(at_bart),
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-                    at_gbm, at_rsm),
+                    at_gbm, at_rsm, at_bart),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
@@ -1052,14 +1066,19 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   return(NULL)
 }
 
+############# IMPORTANT - SET INDEX #############
+iters = vapply(custom_cvs, function(x) x$custom_inner$iters, FUN.VALUE = integer(1L))
+print(max(iters))
+############# IMPORTANT - SET INDEX #############
+
 # main loop
 i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
 start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # i = 60
-  # cv_ = custom_cvs[[3]]
+  # i = 50
+  # cv_ = custom_cvs[[12]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner
@@ -1073,9 +1092,5 @@ lapply(custom_cvs, function(cv_) {
   # tail(cv_outer$train_set(1))
   # head(cv_outer$test_set(1))
 
-  nested_cv_benchmark(i, cv_inner, cv_outer)
+  tryCatch(nested_cv_benchmark(i, cv_inner, cv_outer), error = function(e) print(e))
 })
-
-# # test
-# bmr = readRDS("H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds")
-# bmr$aggregate(msrs(c("regr.mse", "regr.mae", "portfolio_ret")))
diff --git a/run.sh b/run.sh
index ada5ddf..3ce6806 100644
--- a/run.sh
+++ b/run.sh
@@ -1,7 +1,7 @@
 #!/bin/bash
 
 #PBS -l ncpus=8
-#PBS -J 1-80
+#PBS -J 95
 
 cd ${PBS_O_WORKDIR}
-apptainer run image.sif run.R
\ No newline at end of file
+apptainer run image.sif run.R

commit e0d0fdef838ce5e0282a3ba3538ba24f5f5fd558
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Sep 27 12:06:59 2023 +0200

    change new learner place

diff --git a/run.R b/run.R
index 52db3dc..d3c3d7b 100644
--- a/run.R
+++ b/run.R
@@ -1036,9 +1036,9 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    learners = list(at_rsm),
-    # learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-    #                 at_gbm, at_rsm),
+    # learners = list(at_rsm),
+    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
+                    at_gbm, at_rsm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
@@ -1058,7 +1058,7 @@ start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # i = 1
+  # i = 60
   # cv_ = custom_cvs[[3]]
 
   # get cv inner object

commit bacc0573c3ac99bc2f7ace5e602da8264bc36e2e
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Sep 27 12:03:42 2023 +0200

    change new learner place

diff --git a/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds b/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds
deleted file mode 100644
index 67bccf4..0000000
Binary files a/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds and /dev/null differ
diff --git a/run.R b/run.R
index 0a7b61f..52db3dc 100644
--- a/run.R
+++ b/run.R
@@ -637,17 +637,6 @@ search_space_gbm$add(
   # ....
 )
 
-# rsm graph
-graph_rsm = graph_template %>>%
-  po("learner", learner = lrn("regr.rsm"))
-plot(graph_rsm)
-graph_rsm = as_learner(graph_rsm)
-as.data.table(graph_rsm$param_set)[, .(id, class, lower, upper, levels)]
-search_space_rsm = search_space_template$clone()
-search_space_rsm$add(
-  ps(regr.rsm.modelfun = p_fct(levels = c("FO", "TWI", "SO")))
-)
-
 # BART graph
 # Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
 #   when list, drop must have length equal to x
@@ -866,6 +855,24 @@ search_space_earth$add(
      regr.earth.nk      = p_int(lower = 50, upper = 250))
 )
 
+# rsm graph
+graph_rsm = graph_template %>>%
+  po("learner", learner = lrn("regr.rsm"))
+plot(graph_rsm)
+graph_rsm = as_learner(graph_rsm)
+as.data.table(graph_rsm$param_set)[, .(id, class, lower, upper, levels)]
+search_space_rsm = search_space_template$clone()
+search_space_rsm$add(
+  ps(regr.rsm.modelfun = p_fct(levels = c("FO", "TWI", "SO")))
+)
+# Error in fo[, i] * fo[, j] : non-numeric argument to binary operator
+# This happened PipeOp regr.rsm's $train()
+# Calls: lapply ... resolve.list -> signalConditionsASAP -> signalConditions
+# In addition: Warning message:
+# In bbandsDn5:volumeDownUpRatio :
+#   numerical expression has 4076 elements: only the first used
+# This happened PipeOp regr.rsm's $train()
+
 # threads
 threads = as.integer(Sys.getenv("NCPUS"))
 set_threads(graph_rf, n = threads)
@@ -1029,8 +1036,9 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-                    at_gbm, at_rsm),
+    learners = list(at_rsm),
+    # learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
+    #                 at_gbm, at_rsm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
@@ -1051,7 +1059,7 @@ lapply(custom_cvs, function(cv_) {
 
   # debug
   # i = 1
-  # cv_ = custom_cvs[[10]]
+  # cv_ = custom_cvs[[3]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner

commit 6cce97e9839f2eafe2b8a5f1e3b1a634e5486952
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Tue Sep 26 22:43:51 2023 +0200

    add new lerner and fix test to 1

diff --git a/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds b/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds
new file mode 100644
index 0000000..67bccf4
Binary files /dev/null and b/H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds differ
diff --git a/image.def b/image.def
index 466b48a..422c044 100644
--- a/image.def
+++ b/image.def
@@ -39,6 +39,7 @@ From: r-base:4.3.0
   R --slave -e 'install.packages("kknn")'
   R --slave -e 'install.packages("kernlab")'
   R --slave -e 'install.packages("gbm")'
+  R --slave -e 'install.packages("rsm")'
   R --slave -e 'install.packages("torch")'
   R --slave -e 'remotes::install_github("mlr-org/mlr3torch")'
   # R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.1/catboost-R-Linux-1.2.1.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
diff --git a/postscriptum_light.R b/postscriptum_light.R
index ca42088..f41d342 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -139,7 +139,7 @@ mlr_measures$add("portfolio_ret", PortfolioRet)
 id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 
 # set files with benchmarks
-bmr_files = list.files(list.files("F:/", pattern = "^H4-v6", full.names = TRUE), full.names = TRUE)
+bmr_files = list.files(list.files("F:/", pattern = "^H4-v7", full.names = TRUE), full.names = TRUE)
 
 # arrange files
 cv_ = as.integer(gsub("\\d+-.*-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
@@ -212,16 +212,16 @@ predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("task")]
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("learner")]
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task")]
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "learner")]
-predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
 
 # hit ratio
 # predictions_dt = rbindlist(lapply(bmrs, function(x) x$predictions), idcol = "fold")
 setorderv(predictions_dt, c("cv", "i"))
-predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
-predictions_dt[response > 0.1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
-predictions_dt[response > 0.2, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
-predictions_dt[response > 0.5, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
-predictions_dt[response > 1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
+predictions_dt[response > 0.1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
+predictions_dt[response > 0.2, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
+predictions_dt[response > 0.5, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
+predictions_dt[response > 1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")][order(V1)]
 
 # remove ksvm learner, it looks pretty unstable
 predictions_dt = predictions_dt[learner != "ksvm"]
@@ -256,7 +256,7 @@ predictions_dt_ensemble[, lapply(.SD, function(x) sum(x == TRUE)), .SDcols = col
 
 # check only sign ensamble performance
 res = lapply(cols_sign_response_pos, function(x) {
-  predictions_dt_ensemble[get(x) == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = "task"]
+  predictions_dt_ensemble[get(x) == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = c("task")]
 })
 names(res) = cols_sign_response_pos
 res
@@ -372,14 +372,14 @@ plot(as.xts.data.table(sample_[, .N, by = date]))
 indicator = sample_[, .(mean_response_agg = mean(mean_response)),
                     by = "date"]
 indicator[, `:=`(
-  mean_response_agg_ema = TTR::EMA(mean_response_agg, 5, na.rm = TRUE)
+  mean_response_agg_ema = TTR::EMA(mean_response_agg, 10, na.rm = TRUE)
 )
 ]
 indicator = indicator[date > as.Date("2017-01-01") & date < as.Date("2023-01-01")]
 plot(as.xts.data.table(indicator))
 
 backtest_data =  merge(spy, indicator, by = "date", all.x = TRUE, all.y = FALSE)
-backtest_data = na.omit(backtest_data)
+backtest_data = backtest_data[date > indicator[, min(date)]]
 backtest_data[, signal := 1]
 backtest_data[shift(mean_response_agg_ema) < 0, signal := 0]
 backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1))])
diff --git a/run.R b/run.R
index 17be388..0a7b61f 100644
--- a/run.R
+++ b/run.R
@@ -268,14 +268,14 @@ for (i in 1:nrow(cv_param_grid)) {
     custom_cvs[[i]] = nested_cv_split(task_ret_week,
                                       param_$train,
                                       param_$tune,
-                                      2,
+                                      1,
                                       param_$gap+1,
                                       param_$gap+1)
   } else if (param_$gap == 1) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month,
                                       param_$train,
                                       param_$tune,
-                                      2,
+                                      1,
                                       param_$gap,
                                       param_$gap)
 
@@ -283,7 +283,7 @@ for (i in 1:nrow(cv_param_grid)) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month2,
                                       param_$train,
                                       param_$tune,
-                                      2,
+                                      1,
                                       param_$gap,
                                       param_$gap)
 
@@ -291,7 +291,7 @@ for (i in 1:nrow(cv_param_grid)) {
     custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
                                       param_$train,
                                       param_$tune,
-                                      2,
+                                      1,
                                       param_$gap,
                                       param_$gap)
 
@@ -305,7 +305,7 @@ length(custom_cvs) == nrow(cv_param_grid)
 nested_cv_split_expanding = function(task,
                                      train_length_start = 6,
                                      tune_length = 3,
-                                     test_length = 3,
+                                     test_length = 1,
                                      gap_tune = 1,
                                      gap_test = 1,
                                      id = task$id) {
@@ -388,25 +388,25 @@ nested_cv_split_expanding = function(task,
 custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
                                                                train_length_start = 6,
                                                                tune_length = 3,
-                                                               test_length = 3,
+                                                               test_length = 1,
                                                                gap_tune = 0,
                                                                gap_test = 0)
 custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month,
                                                                train_length_start = 6,
                                                                tune_length = 3,
-                                                               test_length = 3,
+                                                               test_length = 1,
                                                                gap_tune = 1,
                                                                gap_test = 1)
 custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month2,
                                                                train_length_start = 6,
                                                                tune_length = 3,
-                                                               test_length = 3,
+                                                               test_length = 1,
                                                                gap_tune = 2,
                                                                gap_test = 2)
 custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_quarter,
                                                                train_length_start = 6,
                                                                tune_length = 3,
-                                                               test_length = 3,
+                                                               test_length = 1,
                                                                gap_tune = 3,
                                                                gap_test = 3)
 
@@ -637,6 +637,16 @@ search_space_gbm$add(
   # ....
 )
 
+# rsm graph
+graph_rsm = graph_template %>>%
+  po("learner", learner = lrn("regr.rsm"))
+plot(graph_rsm)
+graph_rsm = as_learner(graph_rsm)
+as.data.table(graph_rsm$param_set)[, .(id, class, lower, upper, levels)]
+search_space_rsm = search_space_template$clone()
+search_space_rsm$add(
+  ps(regr.rsm.modelfun = p_fct(levels = c("FO", "TWI", "SO")))
+)
 
 # BART graph
 # Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
@@ -867,6 +877,7 @@ set_threads(graph_kknn, n = threads)
 set_threads(graph_lightgbm, n = threads)
 set_threads(graph_earth, n = threads)
 set_threads(graph_gbm, n = threads)
+set_threads(graph_rsm, n = threads)
 
 
 # NESTED CV BENCHMARK -----------------------------------------------------
@@ -980,6 +991,16 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     terminator = trm("none")
   )
 
+  # auto tuner rsm
+  at_rsm = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_rsm,
+    resampling = custom_,
+    measure = measure_,
+    search_space = search_space_rsm,
+    terminator = trm("none")
+  )
+
   # # auto tuner mboost
   # at_gamboost = auto_tuner(
   #   tuner = tnr("hyperband", eta = 5),
@@ -1009,7 +1030,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-                    at_gbm),
+                    at_gbm, at_rsm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
@@ -1046,3 +1067,7 @@ lapply(custom_cvs, function(cv_) {
 
   nested_cv_benchmark(i, cv_inner, cv_outer)
 })
+
+# # test
+# bmr = readRDS("H4-jobarray-/12-taskRetQuarter-93-20230926154703.rds")
+# bmr$aggregate(msrs(c("regr.mse", "regr.mae", "portfolio_ret")))

commit a1eeedcb3c21c1d1cda3f94b2bef93349d96085f
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Thu Sep 21 19:32:48 2023 +0200

    solve error

diff --git a/postscriptum_light.R b/postscriptum_light.R
index 504fa57..ca42088 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -356,7 +356,7 @@ spy = na.omit(spy)
 plot(spy[, close])
 
 # systemic risk
-task_ = "taskRetMonth"
+task_ = "taskRetWeek"
 sample_ = predictions_dt_ensemble[task == task_]
 sample_ = unique(sample_)
 setorder(sample_, date)
@@ -372,7 +372,7 @@ plot(as.xts.data.table(sample_[, .N, by = date]))
 indicator = sample_[, .(mean_response_agg = mean(mean_response)),
                     by = "date"]
 indicator[, `:=`(
-  mean_response_agg_ema = TTR::EMA(mean_response_agg, 22, na.rm = TRUE)
+  mean_response_agg_ema = TTR::EMA(mean_response_agg, 5, na.rm = TRUE)
 )
 ]
 indicator = indicator[date > as.Date("2017-01-01") & date < as.Date("2023-01-01")]
diff --git a/run.R b/run.R
index e1ac687..17be388 100644
--- a/run.R
+++ b/run.R
@@ -632,7 +632,7 @@ search_space_gbm = search_space_template$clone()
 search_space_gbm$add(
   ps(regr.gbm.distribution      = p_fct(levels = c("gaussian", "tdist")),
      regr.gbm.shrinkage         = p_dbl(lower = 0.001, upper = 0.1),
-     regr.gbm.n.trees           = p_dbl(lower = 50, upper = 200),
+     regr.gbm.n.trees           = p_int(lower = 50, upper = 200),
      regr.gbm.interaction.depth = p_int(lower = 1, upper = 4))
   # ....
 )

commit 7c3c010a47ff0709d23cfe5356be7c074999595d
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Thu Sep 21 12:25:07 2023 +0200

    add gbm learner and set portfolio return as main measure

diff --git a/PortfolioRet.R b/PortfolioRet.R
index 658543c..f460709 100644
--- a/PortfolioRet.R
+++ b/PortfolioRet.R
@@ -20,7 +20,7 @@ PortfolioRet = R6::R6Class(
         range = c(0, Inf),
 
         # minimize during tuning?
-        minimize = TRUE
+        minimize = FALSE
       )
     }
   ),
@@ -45,7 +45,7 @@ PortfolioRet = R6::R6Class(
         # calculate portfolio return
         # print(truth_resp)
         # print(weigths_)
-        -sum(truth_resp * weigths_)
+        sum(truth_resp * weigths_)
       }
 
       portfolio_ret_div(prediction$truth, prediction$response)
diff --git a/image.def b/image.def
index a196e72..466b48a 100644
--- a/image.def
+++ b/image.def
@@ -38,6 +38,7 @@ From: r-base:4.3.0
   R --slave -e 'install.packages("earth")'
   R --slave -e 'install.packages("kknn")'
   R --slave -e 'install.packages("kernlab")'
+  R --slave -e 'install.packages("gbm")'
   R --slave -e 'install.packages("torch")'
   R --slave -e 'remotes::install_github("mlr-org/mlr3torch")'
   # R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.1/catboost-R-Linux-1.2.1.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
diff --git a/postscriptum_light.R b/postscriptum_light.R
index 21c9ef5..504fa57 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -128,8 +128,10 @@ rm(list = c("task_ret_week", "task_ret_month", "task_ret_month2", "task_ret_quar
 # measures
 source("Linex.R")
 source("AdjLoss2.R")
+source("PortfolioRet.R")
 mlr_measures$add("linex", Linex)
 mlr_measures$add("adjloss2", AdjLoss2)
+mlr_measures$add("portfolio_ret", PortfolioRet)
 
 
 # RESULTS -----------------------------------------------------------------
@@ -137,7 +139,7 @@ mlr_measures$add("adjloss2", AdjLoss2)
 id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 
 # set files with benchmarks
-bmr_files = list.files(list.files("F:/", pattern = "^H4-v5", full.names = TRUE), full.names = TRUE)
+bmr_files = list.files(list.files("F:/", pattern = "^H4-v6", full.names = TRUE), full.names = TRUE)
 
 # arrange files
 cv_ = as.integer(gsub("\\d+-.*-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
@@ -159,7 +161,7 @@ for (i in 1:nrow(bmr_files)) {
   bmr_dt = as.data.table(bmr)
 
   # aggregate performances
-  agg_ = bmr$aggregate(msrs(c("regr.mse", "regr.mae", "adjloss2", "linex")))
+  agg_ = bmr$aggregate(msrs(c("regr.mse", "regr.mae", "adjloss2", "linex", "portfolio_ret")))
   cols = c("task_id", "learner_id", "iters", colnames(agg_)[7:length(colnames(agg_))])
   agg_ = agg_[, learner_id := gsub(".*regr\\.|\\.tuned", "", learner_id)][, ..cols]
 
@@ -195,14 +197,25 @@ for (i in 1:nrow(bmr_files)) {
 aggregate_results = rbindlist(aggs_l, fill = TRUE)
 cols = colnames(aggregate_results)[4:ncol(aggregate_results)]
 aggregate_results[, lapply(.SD, function(x) mean(x)), by = .(task_id, learner_id), .SDcols = cols]
+aggregate_results[, lapply(.SD, function(x) mean(x)), by = .(task_id, learner_id), .SDcols = cols]
 
-# hit ratio
-# predictions_dt = rbindlist(lapply(bmrs, function(x) x$predictions), idcol = "fold")
+# predictions
 predictions_dt = rbindlist(predictions_l)
 predictions_dt[, `:=`(
   truth_sign = as.factor(sign(truth)),
   response_sign = as.factor(sign(response))
 )]
+
+# accuracy by ids
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("task")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("learner")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "learner")]
+predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
+
+# hit ratio
+# predictions_dt = rbindlist(lapply(bmrs, function(x) x$predictions), idcol = "fold")
 setorderv(predictions_dt, c("cv", "i"))
 predictions_dt[, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
 predictions_dt[response > 0.1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
@@ -210,6 +223,9 @@ predictions_dt[response > 0.2, mlr3measures::acc(truth_sign, response_sign), by
 predictions_dt[response > 0.5, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
 predictions_dt[response > 1, mlr3measures::acc(truth_sign, response_sign), by = c("cv", "task", "learner")]
 
+# remove ksvm learner, it looks pretty unstable
+predictions_dt = predictions_dt[learner != "ksvm"]
+
 # hit ratio for ensamble
 predictions_dt_ensemble = predictions_dt[, .(mean_response = mean(response),
                                              median_response = median(response),
@@ -340,7 +356,7 @@ spy = na.omit(spy)
 plot(spy[, close])
 
 # systemic risk
-task_ = "taskRetMonth2"
+task_ = "taskRetMonth"
 sample_ = predictions_dt_ensemble[task == task_]
 sample_ = unique(sample_)
 setorder(sample_, date)
@@ -353,25 +369,19 @@ sample_[, max(date)]
 # sample_ = sample_[date < as.Date("2018-02-01")]
 plot(as.xts.data.table(sample_[, .N, by = date]))
 
-indicator = sample_[, .(response_sign_sign_net_agg = sum(response_sign_sign_net9),
-                        response_sign_sign_neg_agg = sum(response_sign_sign_neg9),
-                        mean_response_agg = sum(mean_response)),
+indicator = sample_[, .(mean_response_agg = mean(mean_response)),
                     by = "date"]
 indicator[, `:=`(
-  response_sign_sign_net_agg = TTR::EMA(response_sign_sign_net_agg, 5, na.rm = TRUE),
-  response_sign_sign_neg_agg = TTR::EMA(response_sign_sign_neg_agg, 5, na.rm = TRUE),
-  mean_response_agg = TTR::EMA(mean_response_agg, 5, na.rm = TRUE)
+  mean_response_agg_ema = TTR::EMA(mean_response_agg, 22, na.rm = TRUE)
 )
 ]
 indicator = indicator[date > as.Date("2017-01-01") & date < as.Date("2023-01-01")]
-plot(as.xts.data.table(indicator)[, 1])
-plot(as.xts.data.table(indicator)[, 2])
-plot(as.xts.data.table(indicator)[, 3])
+plot(as.xts.data.table(indicator))
 
-backtest_data =  merge(spy, indicator, by = "date")
+backtest_data =  merge(spy, indicator, by = "date", all.x = TRUE, all.y = FALSE)
 backtest_data = na.omit(backtest_data)
 backtest_data[, signal := 1]
-backtest_data[shift(mean_response_agg) < -5, signal := 0]
+backtest_data[shift(mean_response_agg_ema) < 0, signal := 0]
 backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1))])
 PerformanceAnalytics::charts.PerformanceSummary(backtest_data_xts)
 # backtest performance
diff --git a/run.R b/run.R
index 47257c8..e1ac687 100644
--- a/run.R
+++ b/run.R
@@ -488,6 +488,7 @@ source("PipeOpPCAExplained.R")
 # measures
 source("Linex.R")
 source("AdjLoss2.R")
+source("PortfolioRet.R")
 
 # add my pipes to mlr dictionary
 mlr_pipeops$add("uniformization", PipeOpUniform)
@@ -502,6 +503,7 @@ mlr_filters$add("gausscov_f1st", FilterGausscovF1st)
 mlr_filters$add("gausscov_f3st", FilterGausscovF3st)
 mlr_measures$add("linex", Linex)
 mlr_measures$add("adjloss2", AdjLoss2)
+mlr_measures$add("portfolio_ret", PortfolioRet)
 
 
 # GRAPH V2 ----------------------------------------------------------------
@@ -620,6 +622,22 @@ search_space_xgboost = ps(
   regr.xgboost.nrounds   = p_int(1, 5000)
 )
 
+# gbm graph
+graph_gbm = graph_template %>>%
+  po("learner", learner = lrn("regr.gbm"))
+plot(graph_gbm)
+graph_gbm = as_learner(graph_gbm)
+as.data.table(graph_gbm$param_set)[, .(id, class, lower, upper, levels)]
+search_space_gbm = search_space_template$clone()
+search_space_gbm$add(
+  ps(regr.gbm.distribution      = p_fct(levels = c("gaussian", "tdist")),
+     regr.gbm.shrinkage         = p_dbl(lower = 0.001, upper = 0.1),
+     regr.gbm.n.trees           = p_dbl(lower = 50, upper = 200),
+     regr.gbm.interaction.depth = p_int(lower = 1, upper = 4))
+  # ....
+)
+
+
 # BART graph
 # Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
 #   when list, drop must have length equal to x
@@ -737,40 +755,42 @@ search_space_nnet$add(
      regr.nnet.maxit = p_int(lower = 50, upper = 500))
 )
 
+### THIS LEARNER UNSTABLE ####
 # ksvm graph
-graph_ksvm = graph_template %>>%
-  po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
-graph_ksvm = as_learner(graph_ksvm)
-as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
-search_space_ksvm = ps(
-  # subsample for hyperband
-  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
-  # preprocessing
-  dropcorr.cutoff = p_fct(
-    levels = c("0.80", "0.90", "0.95", "0.99"),
-    trafo = function(x, param_set) {
-      switch(x,
-             "0.80" = 0.80,
-             "0.90" = 0.90,
-             "0.95" = 0.95,
-             "0.99" = 0.99)
-    }
-  ),
-  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
-  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
-  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
-  # filters
-  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
-  # interaction
-  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
-  # learner
-  regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
-                                       "laplacedot", "besseldot", "anovadot")),
-  regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
-  regr.ksvm.degree  = p_int(lower = 1, upper = 5,
-                            depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
-  regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
-)
+# graph_ksvm = graph_template %>>%
+#   po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
+# graph_ksvm = as_learner(graph_ksvm)
+# as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
+# search_space_ksvm = ps(
+#   # subsample for hyperband
+#   subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+#   # preprocessing
+#   dropcorr.cutoff = p_fct(
+#     levels = c("0.80", "0.90", "0.95", "0.99"),
+#     trafo = function(x, param_set) {
+#       switch(x,
+#              "0.80" = 0.80,
+#              "0.90" = 0.90,
+#              "0.95" = 0.95,
+#              "0.99" = 0.99)
+#     }
+#   ),
+#   # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+#   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+#   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+#   # filters
+#   filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+#   # interaction
+#   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+#   # learner
+#   regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
+#                                        "laplacedot", "besseldot", "anovadot")),
+#   regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
+#   regr.ksvm.degree  = p_int(lower = 1, upper = 5,
+#                             depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
+#   regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
+# )
+### THIS LEARNER UNSTABLE ####
 
 # LAST
 # lightgbm graph
@@ -841,11 +861,12 @@ threads = as.integer(Sys.getenv("NCPUS"))
 set_threads(graph_rf, n = threads)
 set_threads(graph_xgboost, n = threads)
 # set_threads(graph_bart, n = threads)
-set_threads(graph_ksvm, n = threads)
+# set_threads(graph_ksvm, n = threads) # unstable
 set_threads(graph_nnet, n = threads)
 set_threads(graph_kknn, n = threads)
 set_threads(graph_lightgbm, n = threads)
 set_threads(graph_earth, n = threads)
+set_threads(graph_gbm, n = threads)
 
 
 # NESTED CV BENCHMARK -----------------------------------------------------
@@ -876,12 +897,15 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
                       list(cv_inner$train_set(i)),
                       list(cv_inner$test_set(i)))
 
+  # objects for all autotuners
+  measure_ = msr("portfolio_ret")
+
   # auto tuner rf
   at_rf = auto_tuner(
     tuner = tnr("hyperband", eta = 5),
     learner = graph_rf,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_rf,
     terminator = trm("none")
   )
@@ -891,7 +915,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     tuner = tnr("hyperband", eta = 5),
     learner = graph_xgboost,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_xgboost,
     terminator = trm("none")
   )
@@ -901,27 +925,17 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     tuner = tnr("hyperband", eta = 5),
     learner = graph_bart,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_bart,
     terminator = trm("none")
   )
 
-  # auto tuner ksvm
-  at_ksvm = auto_tuner(
-    tuner = tnr("hyperband", eta = 5),
-    learner = graph_ksvm,
-    resampling = custom_,
-    measure = msr("adjloss2"),
-    search_space = search_space_ksvm,
-    terminator = trm("none")
-  )
-
   # auto tuner nnet
   at_nnet = auto_tuner(
     tuner = tnr("hyperband", eta = 5),
     learner = graph_nnet,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_nnet,
     terminator = trm("none")
   )
@@ -931,7 +945,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     tuner = tnr("hyperband", eta = 5),
     learner = graph_lightgbm,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_lightgbm,
     terminator = trm("none")
   )
@@ -941,7 +955,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     tuner = tnr("hyperband", eta = 5),
     learner = graph_earth,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_earth,
     terminator = trm("none")
   )
@@ -951,11 +965,21 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     tuner = tnr("hyperband", eta = 5),
     learner = graph_kknn,
     resampling = custom_,
-    measure = msr("adjloss2"),
+    measure = measure_,
     search_space = search_space_kknn,
     terminator = trm("none")
   )
 
+  # auto tuner gbm
+  at_gbm = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_gbm,
+    resampling = custom_,
+    measure = measure_,
+    search_space = search_space_gbm,
+    terminator = trm("none")
+  )
+
   # # auto tuner mboost
   # at_gamboost = auto_tuner(
   #   tuner = tnr("hyperband", eta = 5),
@@ -985,7 +1009,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
-                    at_ksvm),
+                    at_gbm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)

commit 0dac983ee422364a78c2056c36d626a519d43e28
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Sep 20 13:30:19 2023 +0200

    add rolling windows and add gaps between sets

diff --git a/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds b/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds
deleted file mode 100644
index 56fc3e7..0000000
Binary files a/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds and /dev/null differ
diff --git a/postscriptum_light.R b/postscriptum_light.R
index 8050455..21c9ef5 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -228,8 +228,9 @@ predictions_dt_ensemble[, `:=`(
   # response_sign_sign_pos = sign_response > 15,
   # response_sign_sign_neg = sign_response < -15
 )]
+predictions_dt_ensemble = unique(predictions_dt_ensemble)
 sign_response_max = predictions_dt_ensemble[, max(sign_response)]
-sign_response_seq = seq(as.integer(sign_response_max / 2), sign_response_max - 1)
+sign_response_seq = seq(as.integer(sign_response_max / 2)-3, sign_response_max - 1)
 cols_sign_response_pos = paste0("response_sign_sign_pos", sign_response_seq)
 predictions_dt_ensemble[, (cols_sign_response_pos) := lapply(sign_response_seq, function(x) sign_response > x)]
 cols_sign_response_neg = paste0("response_sign_sign_neg", sign_response_seq)
@@ -238,10 +239,11 @@ cols_ = colnames(predictions_dt_ensemble)[24:ncol(predictions_dt_ensemble)]
 predictions_dt_ensemble[, lapply(.SD, function(x) sum(x == TRUE)), .SDcols = cols_]
 
 # check only sign ensamble performance
-lapply(cols_sign_response_pos, function(x) {
+res = lapply(cols_sign_response_pos, function(x) {
   predictions_dt_ensemble[get(x) == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = "task"]
 })
-
+names(res) = cols_sign_response_pos
+res
 
 # predictions_dt_ensemble[, response_sign_sd_q := quantile(sd_response, probs = 0.05), by = "task"]
 # predictions_dt_ensemble[, mfd := as.factor(ifelse(sd_response < response_sign_sd_q, 1, -1))] # machine forecast dissagreement
@@ -314,8 +316,10 @@ lapply(unique(predictions_dt_ensemble$task), function(x) {
 # save data for PEAD-SPY
 dt_sample = predictions_dt_ensemble[, .(task, date, mean_response)]
 dt_sample = unique(dt_sample)
+# dt_sample = dt_sample[task == "taskRetWeek"]
 dt_sample = dt_sample[, .(resp = sum(mean_response)), by = date]
 setorder(dt_sample, date)
+plot(as.xts.data.table(dt_sample))
 dt_sample[, date := as.character(date)]
 cont = storage_container(BLOBENDPOINT, "qc-backtest")
 storage_write_csv(dt_sample, cont, paste0("pead-spy.csv"), col_names = FALSE)
@@ -336,7 +340,7 @@ spy = na.omit(spy)
 plot(spy[, close])
 
 # systemic risk
-task_ = "taskRetQuarter"
+task_ = "taskRetMonth2"
 sample_ = predictions_dt_ensemble[task == task_]
 sample_ = unique(sample_)
 setorder(sample_, date)
@@ -367,8 +371,8 @@ plot(as.xts.data.table(indicator)[, 3])
 backtest_data =  merge(spy, indicator, by = "date")
 backtest_data = na.omit(backtest_data)
 backtest_data[, signal := 1]
-backtest_data[shift(mean_response_agg) < -30, signal := 0]
-backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1.5))])
+backtest_data[shift(mean_response_agg) < -5, signal := 0]
+backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1))])
 PerformanceAnalytics::charts.PerformanceSummary(backtest_data_xts)
 # backtest performance
 Performance <- function(x) {
@@ -389,4 +393,3 @@ Performance <- function(x) {
 }
 Performance(backtest_data_xts[, 1])
 Performance(backtest_data_xts[, 2])
-
diff --git a/run.R b/run.R
index c5e4cdb..47257c8 100644
--- a/run.R
+++ b/run.R
@@ -8,6 +8,8 @@ library(mlr3tuning)
 library(mlr3misc)
 library(future)
 library(future.apply)
+# library(ggplot2)
+# library(patchwork)
 
 
 # SETUP -------------------------------------------------------------------
@@ -173,7 +175,8 @@ nested_cv_split = function(task,
                            train_length = 12,
                            tune_length = 1,
                            test_length = 1,
-                           gap_length = 3,
+                           gap_tune = 3,
+                           gap_test = 3,
                            id = task$id) {
 
   # get year month id data
@@ -188,9 +191,8 @@ nested_cv_split = function(task,
   custom_inner = rsmp("custom", id = task$id)
   custom_outer = rsmp("custom", id = task$id)
 
-
   # util vars
-  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_length)
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_test)
   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
 
   # create train data
@@ -200,7 +202,7 @@ nested_cv_split = function(task,
 
   # create tune set
   tune_groups <- lapply(start_folds,
-                        function(x) groups_v[(x+train_length):(x+train_length+tune_length-1)])
+                        function(x) groups_v[(x+train_length+gap_tune):(x+train_length+gap_tune+tune_length-1)])
   tune_sets <- lapply(tune_groups, get_row_ids)
 
   # test train and tune
@@ -210,14 +212,14 @@ nested_cv_split = function(task,
       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
     )
   }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_1 == 1))
-  test_2 = vapply(seq_along(train_groups), function(i) {
-    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_2 == 1))
+  stopifnot(all(test_1 == (1+gap_tune)))
+  # test_2 = vapply(seq_along(train_groups), function(i) {
+  #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  # }, FUN.VALUE = numeric(1L))
+  # stopifnot(all(test_2 > ))
 
   # create test sets
-  insample_length = train_length + tune_length + gap_length
+  insample_length = train_length + gap_tune +  tune_length + gap_test
   test_groups <- lapply(start_folds,
                         function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
   test_sets <- lapply(test_groups, get_row_ids)
@@ -229,11 +231,11 @@ nested_cv_split = function(task,
       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
     )
   }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_1 == 1))
-  test_4 = vapply(seq_along(train_groups), function(i) {
-    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_2 == 1))
+  stopifnot(all(test_1 == 1 + gap_test))
+  # test_4 = vapply(seq_along(train_groups), function(i) {
+  #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  # }, FUN.VALUE = numeric(1L))
+  # stopifnot(all(test_2 == 1))
 
   # test
   # as.Date(train_groups[[2]])
@@ -252,7 +254,7 @@ nested_cv_split = function(task,
 # generate cv's
 train_sets = seq(12, 12 * 3, 12)
 validation_sets = train_sets / 12
-gap_sets = c(-1:2)
+gap_sets = c(0:3)
 mat = cbind(train = train_sets, tune = validation_sets)
 expanded_list  = lapply(gap_sets, function(v) {
   cbind.data.frame(mat, gap = v)
@@ -262,31 +264,35 @@ custom_cvs = list()
 for (i in 1:nrow(cv_param_grid)) {
   print(i)
   param_ = cv_param_grid[i]
-  if (param_$gap == -1) {
+  if (param_$gap == 0) {
     custom_cvs[[i]] = nested_cv_split(task_ret_week,
                                       param_$train,
                                       param_$tune,
-                                      1,
+                                      2,
+                                      param_$gap+1,
                                       param_$gap+1)
-  } else if (param_$gap == 0) {
+  } else if (param_$gap == 1) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month,
                                       param_$train,
                                       param_$tune,
-                                      1,
+                                      2,
+                                      param_$gap,
                                       param_$gap)
 
-  } else if (param_$gap == 1) {
+  } else if (param_$gap == 2) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month2,
                                       param_$train,
                                       param_$tune,
-                                      1,
+                                      2,
+                                      param_$gap,
                                       param_$gap)
 
-  } else if (param_$gap == 2) {
+  } else if (param_$gap == 3) {
     custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
                                       param_$train,
                                       param_$tune,
-                                      1,
+                                      2,
+                                      param_$gap,
                                       param_$gap)
 
   }
@@ -295,93 +301,116 @@ for (i in 1:nrow(cv_param_grid)) {
 # test
 length(custom_cvs) == nrow(cv_param_grid)
 
-# # create expanding window function
-# nested_cv_split_expanding = function(task,
-#                                      train_length_start = 6,
-#                                      tune_length = 1,
-#                                      test_length = 1) {
-#
-#   # create cusom CV's for inner and outer sampling
-#   custom_inner = rsmp("custom")
-#   custom_outer = rsmp("custom")
-#
-#   # get year month id data
-#   # task = task_ret_week$clone()
-#   task_ = task$clone()
-#   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
-#                                     rows = 1:task_$nrow)
-#   stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
-#   groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
-#
-#   # util vars
-#   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
-#
-#   # create train data
-#   train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
-#
-#   # create tune set
-#   tune_groups <- lapply((train_length_start+1):length(groups_v), function(i) groups_v[i:(i+tune_length-1)])
-#   index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
-#   tune_groups = tune_groups[index_keep]
-#
-#   # equalize train and tune sets
-#   train_groups = train_groups[1:length(tune_groups)]
-#
-#   # create test sets
-#   insample_length = vapply(train_groups, function(x) as.integer(length(x) + tune_length), FUN.VALUE = integer(1))
-#   test_groups <- lapply((insample_length+1):length(groups_v), function(i) groups_v[i:(i+test_length-1)])
-#   index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
-#   test_groups = test_groups[index_keep]
-#
-#   # equalize train, tune and test sets
-#   train_groups = train_groups[1:length(test_groups)]
-#   tune_groups = tune_groups[1:length(test_groups)]
-#
-#   # make sets
-#   train_sets <- lapply(train_groups, get_row_ids)
-#   tune_sets <- lapply(tune_groups, get_row_ids)
-#   test_sets <- lapply(test_groups, get_row_ids)
-#
-#   # test tune and test
-#   test_1 = vapply(seq_along(train_groups), function(i) {
-#     mondf(
-#       tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
-#       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
-#     )
-#   }, FUN.VALUE = numeric(1L))
-#   stopifnot(all(test_1 == 1))
-#   test_2 = vapply(seq_along(train_groups), function(i) {
-#     unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
-#   }, FUN.VALUE = numeric(1L))
-#   stopifnot(all(test_2 == 1))
-#   test_3 = vapply(seq_along(train_groups), function(i) {
-#     mondf(
-#       tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
-#       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
-#     )
-#   }, FUN.VALUE = numeric(1L))
-#   stopifnot(all(test_1 == 1))
-#   test_4 = vapply(seq_along(train_groups), function(i) {
-#     unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
-#   }, FUN.VALUE = numeric(1L))
-#   stopifnot(all(test_2 == 1))
-#
-#   # create inner and outer resamplings
-#   custom_inner$instantiate(task, train_sets, tune_sets)
-#   inner_sets = lapply(seq_along(train_groups), function(i) {
-#     c(train_sets[[i]], tune_sets[[i]])
-#   })
-#   custom_outer$instantiate(task, inner_sets, test_sets)
-#   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
-# }
-#
-# # generate cv's for expanding windows
-# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
-#                                                                train_length_start = 6,
-#                                                                tune_length = 1,
-#                                                                test_length = 1)
-#
-# # test if tain , validation and tst set follow logic
+# create expanding window function
+nested_cv_split_expanding = function(task,
+                                     train_length_start = 6,
+                                     tune_length = 3,
+                                     test_length = 3,
+                                     gap_tune = 1,
+                                     gap_test = 1,
+                                     id = task$id) {
+
+  # get year month id data
+  # task = task_ret_week$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom", id = task$id)
+  custom_outer = rsmp("custom", id = task$id)
+
+  # util vars
+  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
+
+  # create tune set
+  tune_groups <- lapply((train_length_start+gap_tune+1):length(groups_v), function(i) groups_v[i:(i+tune_length+gap_tune-1)])
+  index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  tune_groups = tune_groups[index_keep]
+
+  # equalize train and tune sets
+  train_groups = train_groups[1:length(tune_groups)]
+
+  # create test sets
+  insample_length = vapply(train_groups, function(x) as.integer(length(x) + gap_tune + tune_length + gap_test), FUN.VALUE = integer(1))
+  test_groups <- lapply(insample_length+gap_test+1, function(i) groups_v[i:(i+test_length-1)])
+  index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  test_groups = test_groups[index_keep]
+
+  # equalize train, tune and test sets
+  train_groups = train_groups[1:length(test_groups)]
+  tune_groups = tune_groups[1:length(test_groups)]
+
+  # make sets
+  train_sets <- lapply(train_groups, get_row_ids)
+  tune_sets <- lapply(tune_groups, get_row_ids)
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1 + gap_tune))
+  # test_2 = vapply(seq_along(train_groups), function(i) {
+  #   unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  # }, FUN.VALUE = numeric(1L))
+  # stopifnot(all(test_2 == 1))
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_3 == 1 + gap_test))
+  # test_4 = vapply(seq_along(train_groups), function(i) {
+  #   unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  # }, FUN.VALUE = numeric(1L))
+  # stopifnot(all(test_2 == 1))
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# generate cv's for expanding windows
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
+                                                               train_length_start = 6,
+                                                               tune_length = 3,
+                                                               test_length = 3,
+                                                               gap_tune = 0,
+                                                               gap_test = 0)
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month,
+                                                               train_length_start = 6,
+                                                               tune_length = 3,
+                                                               test_length = 3,
+                                                               gap_tune = 1,
+                                                               gap_test = 1)
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_month2,
+                                                               train_length_start = 6,
+                                                               tune_length = 3,
+                                                               test_length = 3,
+                                                               gap_tune = 2,
+                                                               gap_test = 2)
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_quarter,
+                                                               train_length_start = 6,
+                                                               tune_length = 3,
+                                                               test_length = 3,
+                                                               gap_tune = 3,
+                                                               gap_test = 3)
+
+# test if tain , validation and tst set follow logic
 # lapply(seq_along(custom_cvs), function(i) {
 #   # extract custyom cv
 #   custom_cvs_ = custom_cvs[[i]]
@@ -401,6 +430,46 @@ length(custom_cvs) == nrow(cv_param_grid)
 #   c(test1, test2)
 # })
 
+# # visualize test
+# prepare_cv_plot = function(x, set = "train") {
+#   x = lapply(x, function(x) data.table(ID = x))
+#   x = rbindlist(x, idcol = "fold")
+#   x[, fold := as.factor(fold)]
+#   x[, set := as.factor(set)]
+#   x[, ID := as.numeric(ID)]
+# }
+# plot_cv = function(cv, n = 5) {
+#   cv_test_inner = cv$custom_inner
+#   cv_test_outer = cv$custom_outer
+#
+#   # define task
+#   if (cv_test_inner$id == "taskRetQuarter") {
+#     task_ = task_ret_quarter$clone()
+#   } else if (cv_test_inner$id == "taskRetMonth2") {
+#     task_ = task_ret_month2$clone()
+#   } else if (cv_test_inner$id == "taskRetMonth") {
+#     task_ = task_ret_month$clone()
+#   } else if (cv_test_inner$id == "taskRetWeek") {
+#     task_ = task_ret_week$clone()
+#   }
+#
+#   # prepare train, tune and test folds
+#   train_sets = cv_test_inner$instance$train[1:n]
+#   train_sets = prepare_cv_plot(train_sets)
+#   tune_sets = cv_test_inner$instance$test[1:n]
+#   tune_sets = prepare_cv_plot(tune_sets, set = "tune")
+#   test_sets = cv_test_outer$instance$test[1:n]
+#   test_sets = prepare_cv_plot(test_sets, set = "test")
+#   dt_vis = rbind(train_sets, tune_sets, test_sets)
+#   ggplot(dt_vis, aes(x = fold, y = ID, color = set)) +
+#     geom_point() +
+#     theme_minimal() +
+#     coord_flip() +
+#     labs(x = "", y = '')
+# }
+# plots = lapply(custom_cvs[c(1, 4, 7, 11, 14)], plot_cv, n = 12)
+# wrap_plots(plots)
+
 
 # ADD PIPELINES -----------------------------------------------------------
 print("Add pipelines")
@@ -911,7 +980,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   customo_ = rsmp("custom")
   customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
 
-  # # nested CV for one round
+  # nested CV for one round
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
@@ -936,7 +1005,8 @@ start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # cv_ = custom_cvs[[1]]
+  # i = 1
+  # cv_ = custom_cvs[[10]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner

commit af12cfc5f1290154e01a5cb2d89e7074893fcf88
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Mon Sep 18 22:38:20 2023 +0200

    add again ksvm and decrease gap by 1

diff --git a/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds b/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds
new file mode 100644
index 0000000..56fc3e7
Binary files /dev/null and b/H4-jobarray-/12-taskRetWeek-96-20230915184440.rds differ
diff --git a/postscriptum_light.R b/postscriptum_light.R
index 2b8b74a..8050455 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -2,7 +2,8 @@ library(data.table)
 library(mlr3verse)
 library(AzureStor)
 library(readr)
-# library(duckdb)
+library(duckdb)
+library(PerformanceAnalytics)
 
 
 
@@ -136,11 +137,11 @@ mlr_measures$add("adjloss2", AdjLoss2)
 id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 
 # set files with benchmarks
-bmr_files = list.files(list.files("F:", pattern = "^H4-v5", full.names = TRUE), full.names = TRUE)
+bmr_files = list.files(list.files("F:/", pattern = "^H4-v5", full.names = TRUE), full.names = TRUE)
 
 # arrange files
-cv_ = as.integer(gsub("\\d+-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
-i_ = as.integer(gsub("-\\d+", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
+cv_ = as.integer(gsub("\\d+-.*-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
+i_ = as.integer(gsub("-.*-\\d+", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
 bmr_files = cbind.data.frame(bmr_files, cv = cv_, i = i_)
 setorder(bmr_files, cv, i)
 
@@ -233,6 +234,8 @@ cols_sign_response_pos = paste0("response_sign_sign_pos", sign_response_seq)
 predictions_dt_ensemble[, (cols_sign_response_pos) := lapply(sign_response_seq, function(x) sign_response > x)]
 cols_sign_response_neg = paste0("response_sign_sign_neg", sign_response_seq)
 predictions_dt_ensemble[, (cols_sign_response_neg) := lapply(sign_response_seq, function(x) sign_response < -x)]
+cols_ = colnames(predictions_dt_ensemble)[24:ncol(predictions_dt_ensemble)]
+predictions_dt_ensemble[, lapply(.SD, function(x) sum(x == TRUE)), .SDcols = cols_]
 
 # check only sign ensamble performance
 lapply(cols_sign_response_pos, function(x) {
@@ -272,7 +275,7 @@ lapply(cols_sign_response_pos, function(x) {
 cont = storage_container(BLOBENDPOINT, "qc-backtest")
 lapply(unique(predictions_dt_ensemble$task), function(x) {
   # debug
-  # x = "taskRetQuarter"
+  # x = "taskRetMonth"
 
   # prepare data
   y = predictions_dt_ensemble[task == x]
@@ -303,7 +306,7 @@ lapply(unique(predictions_dt_ensemble$task), function(x) {
   # save to azure blob
   print(colnames(y))
   file_name_ =  paste0("pead-", x, ".csv")
-  storage_write_csv(y, cont, file_name_, col_names = FALSE)
+  storage_write_csv(y, cont, file_name_)
   # universe = y[, .(date, symbol)]
   # storage_write_csv(universe, cont, "pead_task_ret_week_universe.csv", col_names = FALSE)
 })
@@ -330,10 +333,10 @@ spy = as.data.table(spy)
 spy = spy[, .(date = Date, close = `Adj Close`)]
 spy[, returns := close / shift(close) - 1]
 spy = na.omit(spy)
+plot(spy[, close])
 
 # systemic risk
-library(PerformanceAnalytics)
-task_ = "taskRetMonth"
+task_ = "taskRetQuarter"
 sample_ = predictions_dt_ensemble[task == task_]
 sample_ = unique(sample_)
 setorder(sample_, date)
@@ -343,6 +346,7 @@ new_dt = sample_[, ..pos_cols] - sample_[, ..neg_cols]
 setnames(new_dt, gsub("pos", "net", pos_cols))
 sample_ = cbind(sample_, new_dt)
 sample_[, max(date)]
+# sample_ = sample_[date < as.Date("2018-02-01")]
 plot(as.xts.data.table(sample_[, .N, by = date]))
 
 indicator = sample_[, .(response_sign_sign_net_agg = sum(response_sign_sign_net9),
@@ -363,8 +367,8 @@ plot(as.xts.data.table(indicator)[, 3])
 backtest_data =  merge(spy, indicator, by = "date")
 backtest_data = na.omit(backtest_data)
 backtest_data[, signal := 1]
-backtest_data[shift(mean_response_agg) < 0, signal := 0]
-backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 2))])
+backtest_data[shift(mean_response_agg) < -30, signal := 0]
+backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 1.5))])
 PerformanceAnalytics::charts.PerformanceSummary(backtest_data_xts)
 # backtest performance
 Performance <- function(x) {
diff --git a/run.R b/run.R
index 80b94eb..c5e4cdb 100644
--- a/run.R
+++ b/run.R
@@ -252,7 +252,7 @@ nested_cv_split = function(task,
 # generate cv's
 train_sets = seq(12, 12 * 3, 12)
 validation_sets = train_sets / 12
-gap_sets = c(0:3)
+gap_sets = c(-1:2)
 mat = cbind(train = train_sets, tune = validation_sets)
 expanded_list  = lapply(gap_sets, function(v) {
   cbind.data.frame(mat, gap = v)
@@ -262,27 +262,27 @@ custom_cvs = list()
 for (i in 1:nrow(cv_param_grid)) {
   print(i)
   param_ = cv_param_grid[i]
-  if (param_$gap == 0) {
+  if (param_$gap == -1) {
     custom_cvs[[i]] = nested_cv_split(task_ret_week,
                                       param_$train,
                                       param_$tune,
                                       1,
-                                      param_$gap)
-  } else if (param_$gap == 1) {
+                                      param_$gap+1)
+  } else if (param_$gap == 0) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month,
                                       param_$train,
                                       param_$tune,
                                       1,
                                       param_$gap)
 
-  } else if (param_$gap == 2) {
+  } else if (param_$gap == 1) {
     custom_cvs[[i]] = nested_cv_split(task_ret_month2,
                                       param_$train,
                                       param_$tune,
                                       1,
                                       param_$gap)
 
-  } else if (param_$gap == 3) {
+  } else if (param_$gap == 2) {
     custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
                                       param_$train,
                                       param_$tune,
@@ -915,7 +915,8 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
+    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn,
+                    at_ksvm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)

commit 91fcd29477feebc202fb5dbf892a0220287c4ec0
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Fri Sep 15 15:47:21 2023 +0200

    try tosolve error

diff --git a/run.R b/run.R
index 96201a9..80b94eb 100644
--- a/run.R
+++ b/run.R
@@ -295,111 +295,111 @@ for (i in 1:nrow(cv_param_grid)) {
 # test
 length(custom_cvs) == nrow(cv_param_grid)
 
-# create expanding window function
-nested_cv_split_expanding = function(task,
-                                     train_length_start = 6,
-                                     tune_length = 1,
-                                     test_length = 1) {
-
-  # create cusom CV's for inner and outer sampling
-  custom_inner = rsmp("custom")
-  custom_outer = rsmp("custom")
-
-  # get year month id data
-  # task = task_ret_week$clone()
-  task_ = task$clone()
-  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
-                                    rows = 1:task_$nrow)
-  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
-  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
-
-  # util vars
-  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
-
-  # create train data
-  train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
-
-  # create tune set
-  tune_groups <- lapply((train_length_start+1):length(groups_v), function(i) groups_v[i:(i+tune_length-1)])
-  index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
-  tune_groups = tune_groups[index_keep]
-
-  # equalize train and tune sets
-  train_groups = train_groups[1:length(tune_groups)]
-
-  # create test sets
-  insample_length = vapply(train_groups, function(x) as.integer(length(x) + tune_length), FUN.VALUE = integer(1))
-  test_groups <- lapply((insample_length+1):length(groups_v), function(i) groups_v[i:(i+test_length-1)])
-  index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
-  test_groups = test_groups[index_keep]
-
-  # equalize train, tune and test sets
-  train_groups = train_groups[1:length(test_groups)]
-  tune_groups = tune_groups[1:length(test_groups)]
-
-  # make sets
-  train_sets <- lapply(train_groups, get_row_ids)
-  tune_sets <- lapply(tune_groups, get_row_ids)
-  test_sets <- lapply(test_groups, get_row_ids)
-
-  # test tune and test
-  test_1 = vapply(seq_along(train_groups), function(i) {
-    mondf(
-      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
-      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
-    )
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_1 == 1))
-  test_2 = vapply(seq_along(train_groups), function(i) {
-    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_2 == 1))
-  test_3 = vapply(seq_along(train_groups), function(i) {
-    mondf(
-      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
-      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
-    )
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_1 == 1))
-  test_4 = vapply(seq_along(train_groups), function(i) {
-    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
-  }, FUN.VALUE = numeric(1L))
-  stopifnot(all(test_2 == 1))
-
-  # create inner and outer resamplings
-  custom_inner$instantiate(task, train_sets, tune_sets)
-  inner_sets = lapply(seq_along(train_groups), function(i) {
-    c(train_sets[[i]], tune_sets[[i]])
-  })
-  custom_outer$instantiate(task, inner_sets, test_sets)
-  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
-}
-
-# generate cv's for expanding windows
-custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
-                                                               train_length_start = 6,
-                                                               tune_length = 1,
-                                                               test_length = 1)
-
-# test if tain , validation and tst set follow logic
-lapply(seq_along(custom_cvs), function(i) {
-  # extract custyom cv
-  custom_cvs_ = custom_cvs[[i]]
-  custom_inner = custom_cvs_$custom_inner
-  custom_outer = custom_cvs_$custom_outer
-
-  # test set start after train set
-  test1 = all(vapply(1:custom_inner$iters, function(i) {
-    (tail(custom_inner$train_set(i), 1) + 1) == custom_inner$test_set(i)[1]
-  }, FUN.VALUE = logical(1L)))
-
-  # train set in outersample contains ids in innersample 1
-  test2  = all(vapply(1:custom_inner$iters, function(i) {
-    all(c(custom_inner$train_set(i),
-          custom_inner$test_set(i)) == custom_outer$train_set(i))
-  }, FUN.VALUE = logical(1L)))
-  c(test1, test2)
-})
+# # create expanding window function
+# nested_cv_split_expanding = function(task,
+#                                      train_length_start = 6,
+#                                      tune_length = 1,
+#                                      test_length = 1) {
+#
+#   # create cusom CV's for inner and outer sampling
+#   custom_inner = rsmp("custom")
+#   custom_outer = rsmp("custom")
+#
+#   # get year month id data
+#   # task = task_ret_week$clone()
+#   task_ = task$clone()
+#   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+#                                     rows = 1:task_$nrow)
+#   stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+#   groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+#
+#   # util vars
+#   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+#
+#   # create train data
+#   train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
+#
+#   # create tune set
+#   tune_groups <- lapply((train_length_start+1):length(groups_v), function(i) groups_v[i:(i+tune_length-1)])
+#   index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+#   tune_groups = tune_groups[index_keep]
+#
+#   # equalize train and tune sets
+#   train_groups = train_groups[1:length(tune_groups)]
+#
+#   # create test sets
+#   insample_length = vapply(train_groups, function(x) as.integer(length(x) + tune_length), FUN.VALUE = integer(1))
+#   test_groups <- lapply((insample_length+1):length(groups_v), function(i) groups_v[i:(i+test_length-1)])
+#   index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+#   test_groups = test_groups[index_keep]
+#
+#   # equalize train, tune and test sets
+#   train_groups = train_groups[1:length(test_groups)]
+#   tune_groups = tune_groups[1:length(test_groups)]
+#
+#   # make sets
+#   train_sets <- lapply(train_groups, get_row_ids)
+#   tune_sets <- lapply(tune_groups, get_row_ids)
+#   test_sets <- lapply(test_groups, get_row_ids)
+#
+#   # test tune and test
+#   test_1 = vapply(seq_along(train_groups), function(i) {
+#     mondf(
+#       tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+#       head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+#     )
+#   }, FUN.VALUE = numeric(1L))
+#   stopifnot(all(test_1 == 1))
+#   test_2 = vapply(seq_along(train_groups), function(i) {
+#     unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+#   }, FUN.VALUE = numeric(1L))
+#   stopifnot(all(test_2 == 1))
+#   test_3 = vapply(seq_along(train_groups), function(i) {
+#     mondf(
+#       tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+#       head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+#     )
+#   }, FUN.VALUE = numeric(1L))
+#   stopifnot(all(test_1 == 1))
+#   test_4 = vapply(seq_along(train_groups), function(i) {
+#     unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+#   }, FUN.VALUE = numeric(1L))
+#   stopifnot(all(test_2 == 1))
+#
+#   # create inner and outer resamplings
+#   custom_inner$instantiate(task, train_sets, tune_sets)
+#   inner_sets = lapply(seq_along(train_groups), function(i) {
+#     c(train_sets[[i]], tune_sets[[i]])
+#   })
+#   custom_outer$instantiate(task, inner_sets, test_sets)
+#   return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+# }
+#
+# # generate cv's for expanding windows
+# custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
+#                                                                train_length_start = 6,
+#                                                                tune_length = 1,
+#                                                                test_length = 1)
+#
+# # test if tain , validation and tst set follow logic
+# lapply(seq_along(custom_cvs), function(i) {
+#   # extract custyom cv
+#   custom_cvs_ = custom_cvs[[i]]
+#   custom_inner = custom_cvs_$custom_inner
+#   custom_outer = custom_cvs_$custom_outer
+#
+#   # test set start after train set
+#   test1 = all(vapply(1:custom_inner$iters, function(i) {
+#     (tail(custom_inner$train_set(i), 1) + 1) == custom_inner$test_set(i)[1]
+#   }, FUN.VALUE = logical(1L)))
+#
+#   # train set in outersample contains ids in innersample 1
+#   test2  = all(vapply(1:custom_inner$iters, function(i) {
+#     all(c(custom_inner$train_set(i),
+#           custom_inner$test_set(i)) == custom_outer$train_set(i))
+#   }, FUN.VALUE = logical(1L)))
+#   c(test1, test2)
+# })
 
 
 # ADD PIPELINES -----------------------------------------------------------
@@ -783,7 +783,6 @@ set_threads(graph_earth, n = threads)
 print("Benchmark")
 
 # nested for loop
-list.files(mlr3_save_path, full.names = TRUE)
 nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
 
   # debug
@@ -919,14 +918,14 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
     resamplings = customo_
   )
-  # bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
+  bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
 
   # save locally and to list
   print("Save")
   time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
   file_name = paste0(i, "-", task_$id, "-", cv_inner$iters, "-", time_, ".rds")
   print(file_name)
-  # saveRDS(bmr, file.path(mlr3_save_path, file_name))
+  saveRDS(bmr, file.path(mlr3_save_path, file_name))
   return(NULL)
 }
 
@@ -936,7 +935,7 @@ start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # cv_ = custom_cvs[[12]]
+  # cv_ = custom_cvs[[1]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner

commit 94d377fa191db0172002ff9a2dc3a15becabaa9e
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Fri Sep 15 15:04:09 2023 +0200

    make run_v2 functoin

diff --git a/run.R b/run.R
index 6835530..96201a9 100644
--- a/run.R
+++ b/run.R
@@ -172,22 +172,25 @@ print("Cross validations")
 nested_cv_split = function(task,
                            train_length = 12,
                            tune_length = 1,
-                           test_length = 1) {
-
-  # create cusom CV's for inner and outer sampling
-  custom_inner = rsmp("custom")
-  custom_outer = rsmp("custom")
+                           test_length = 1,
+                           gap_length = 3,
+                           id = task$id) {
 
   # get year month id data
-  # task = task_ret_week$clone()
+  # task = task_ret_quarter$clone()
   task_ = task$clone()
   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
                                     rows = 1:task_$nrow)
   stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
   groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
 
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom", id = task$id)
+  custom_outer = rsmp("custom", id = task$id)
+
+
   # util vars
-  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length)
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_length)
   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
 
   # create train data
@@ -214,7 +217,7 @@ nested_cv_split = function(task,
   stopifnot(all(test_2 == 1))
 
   # create test sets
-  insample_length = train_length + tune_length
+  insample_length = train_length + tune_length + gap_length
   test_groups <- lapply(start_folds,
                         function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
   test_sets <- lapply(test_groups, get_row_ids)
@@ -232,6 +235,11 @@ nested_cv_split = function(task,
   }, FUN.VALUE = numeric(1L))
   stopifnot(all(test_2 == 1))
 
+  # test
+  # as.Date(train_groups[[2]])
+  # as.Date(tune_groups[[2]])
+  # as.Date(test_groups[[2]])
+
   # create inner and outer resamplings
   custom_inner$instantiate(task, train_sets, tune_sets)
   inner_sets = lapply(seq_along(train_groups), function(i) {
@@ -244,15 +252,49 @@ nested_cv_split = function(task,
 # generate cv's
 train_sets = seq(12, 12 * 3, 12)
 validation_sets = train_sets / 12
+gap_sets = c(0:3)
+mat = cbind(train = train_sets, tune = validation_sets)
+expanded_list  = lapply(gap_sets, function(v) {
+  cbind.data.frame(mat, gap = v)
+})
+cv_param_grid = rbindlist(expanded_list)
 custom_cvs = list()
-for (i in seq_along(train_sets)) {
+for (i in 1:nrow(cv_param_grid)) {
   print(i)
-  custom_cvs[[i]] = nested_cv_split(task_ret_week,
-                                    train_sets[[i]],
-                                    validation_sets[[i]],
-                                    1)
+  param_ = cv_param_grid[i]
+  if (param_$gap == 0) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_week,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+  } else if (param_$gap == 1) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_month,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  } else if (param_$gap == 2) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  } else if (param_$gap == 3) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  }
 }
 
+# test
+length(custom_cvs) == nrow(cv_param_grid)
+
 # create expanding window function
 nested_cv_split_expanding = function(task,
                                      train_length_start = 6,
@@ -428,7 +470,7 @@ graph_template =
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
               po("filter", filter = flt("relief"), filter.frac = 0.05),
               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
-              # po("nop", id = "nop_filter"))) %>>%
+  # po("nop", id = "nop_filter"))) %>>%
   po("unbranch", id = "filter_unbranch") %>>%
   # modelmatrix
   po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
@@ -520,8 +562,8 @@ as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
 search_space_bart = search_space_template$clone()
 search_space_bart$add(
   ps(regr.bart.k = p_int(lower = 1, upper = 10))
-     # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
-     # regr.bart.n_trees = p_int(lower = 10, upper = 100))
+  # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
+  # regr.bart.n_trees = p_int(lower = 10, upper = 100))
 )
 # chatgpt returns this
 # n_trees = p_int(lower = 10, upper = 100),
@@ -748,9 +790,21 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   # i = 1
   print(i)
 
+  # choose task_
+  print(cv_inner$id)
+  if (cv_inner$id == "taskRetWeek") {
+    task_ = task_ret_week$clone()
+  } else if (cv_inner$id == "taskRetMonth") {
+    task_ = task_ret_month$clone()
+  } else if (cv_inner$id == "taskRetMonth2") {
+    task_ = task_ret_month2$clone()
+  } else if (cv_inner$id == "taskRetQuarter") {
+    task_ = task_ret_quarter$clone()
+  }
+
   # inner resampling
   custom_ = rsmp("custom")
-  custom_$instantiate(task_ret_week,
+  custom_$instantiate(task_,
                       list(cv_inner$train_set(i)),
                       list(cv_inner$test_set(i)))
 
@@ -856,39 +910,45 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
 
   # outer resampling
   customo_ = rsmp("custom")
-  customo_$instantiate(task_ret_week, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
+  customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
 
-  # nested CV for one round
+  # # nested CV for one round
   print("Benchmark!")
   design = benchmark_grid(
-    tasks = list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
+    tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
     resamplings = customo_
   )
-  bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
+  # bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
 
   # save locally and to list
   print("Save")
   time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
-  saveRDS(bmr, file.path(mlr3_save_path, paste0(i, "-", cv_inner$iters, "-", time_, ".rds")))
+  file_name = paste0(i, "-", task_$id, "-", cv_inner$iters, "-", time_, ".rds")
+  print(file_name)
+  # saveRDS(bmr, file.path(mlr3_save_path, file_name))
   return(NULL)
 }
 
+# main loop
 i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
 start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # i = 1
-  # cv_ = custom_cvs[[1]]
+  # cv_ = custom_cvs[[12]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner
   cv_outer = cv_$custom_outer
   cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
 
+  # one more test
+  # tail(cv_inner$train_set(1))
+  # head(cv_inner$test_set(1))
+  # tail(cv_inner$test_set(1))
+  # tail(cv_outer$train_set(1))
+  # head(cv_outer$test_set(1))
+
   nested_cv_benchmark(i, cv_inner, cv_outer)
 })
-
-end_time = Sys.time()
-end_time - start_time
diff --git a/run_v2.R b/run_archive.R
similarity index 91%
rename from run_v2.R
rename to run_archive.R
index 96201a9..6835530 100644
--- a/run_v2.R
+++ b/run_archive.R
@@ -172,25 +172,22 @@ print("Cross validations")
 nested_cv_split = function(task,
                            train_length = 12,
                            tune_length = 1,
-                           test_length = 1,
-                           gap_length = 3,
-                           id = task$id) {
+                           test_length = 1) {
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom")
+  custom_outer = rsmp("custom")
 
   # get year month id data
-  # task = task_ret_quarter$clone()
+  # task = task_ret_week$clone()
   task_ = task$clone()
   yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
                                     rows = 1:task_$nrow)
   stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
   groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
 
-  # create cusom CV's for inner and outer sampling
-  custom_inner = rsmp("custom", id = task$id)
-  custom_outer = rsmp("custom", id = task$id)
-
-
   # util vars
-  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_length)
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length)
   get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
 
   # create train data
@@ -217,7 +214,7 @@ nested_cv_split = function(task,
   stopifnot(all(test_2 == 1))
 
   # create test sets
-  insample_length = train_length + tune_length + gap_length
+  insample_length = train_length + tune_length
   test_groups <- lapply(start_folds,
                         function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
   test_sets <- lapply(test_groups, get_row_ids)
@@ -235,11 +232,6 @@ nested_cv_split = function(task,
   }, FUN.VALUE = numeric(1L))
   stopifnot(all(test_2 == 1))
 
-  # test
-  # as.Date(train_groups[[2]])
-  # as.Date(tune_groups[[2]])
-  # as.Date(test_groups[[2]])
-
   # create inner and outer resamplings
   custom_inner$instantiate(task, train_sets, tune_sets)
   inner_sets = lapply(seq_along(train_groups), function(i) {
@@ -252,49 +244,15 @@ nested_cv_split = function(task,
 # generate cv's
 train_sets = seq(12, 12 * 3, 12)
 validation_sets = train_sets / 12
-gap_sets = c(0:3)
-mat = cbind(train = train_sets, tune = validation_sets)
-expanded_list  = lapply(gap_sets, function(v) {
-  cbind.data.frame(mat, gap = v)
-})
-cv_param_grid = rbindlist(expanded_list)
 custom_cvs = list()
-for (i in 1:nrow(cv_param_grid)) {
+for (i in seq_along(train_sets)) {
   print(i)
-  param_ = cv_param_grid[i]
-  if (param_$gap == 0) {
-    custom_cvs[[i]] = nested_cv_split(task_ret_week,
-                                      param_$train,
-                                      param_$tune,
-                                      1,
-                                      param_$gap)
-  } else if (param_$gap == 1) {
-    custom_cvs[[i]] = nested_cv_split(task_ret_month,
-                                      param_$train,
-                                      param_$tune,
-                                      1,
-                                      param_$gap)
-
-  } else if (param_$gap == 2) {
-    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
-                                      param_$train,
-                                      param_$tune,
-                                      1,
-                                      param_$gap)
-
-  } else if (param_$gap == 3) {
-    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
-                                      param_$train,
-                                      param_$tune,
-                                      1,
-                                      param_$gap)
-
-  }
+  custom_cvs[[i]] = nested_cv_split(task_ret_week,
+                                    train_sets[[i]],
+                                    validation_sets[[i]],
+                                    1)
 }
 
-# test
-length(custom_cvs) == nrow(cv_param_grid)
-
 # create expanding window function
 nested_cv_split_expanding = function(task,
                                      train_length_start = 6,
@@ -470,7 +428,7 @@ graph_template =
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
               po("filter", filter = flt("relief"), filter.frac = 0.05),
               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
-  # po("nop", id = "nop_filter"))) %>>%
+              # po("nop", id = "nop_filter"))) %>>%
   po("unbranch", id = "filter_unbranch") %>>%
   # modelmatrix
   po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
@@ -562,8 +520,8 @@ as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
 search_space_bart = search_space_template$clone()
 search_space_bart$add(
   ps(regr.bart.k = p_int(lower = 1, upper = 10))
-  # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
-  # regr.bart.n_trees = p_int(lower = 10, upper = 100))
+     # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
+     # regr.bart.n_trees = p_int(lower = 10, upper = 100))
 )
 # chatgpt returns this
 # n_trees = p_int(lower = 10, upper = 100),
@@ -790,21 +748,9 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   # i = 1
   print(i)
 
-  # choose task_
-  print(cv_inner$id)
-  if (cv_inner$id == "taskRetWeek") {
-    task_ = task_ret_week$clone()
-  } else if (cv_inner$id == "taskRetMonth") {
-    task_ = task_ret_month$clone()
-  } else if (cv_inner$id == "taskRetMonth2") {
-    task_ = task_ret_month2$clone()
-  } else if (cv_inner$id == "taskRetQuarter") {
-    task_ = task_ret_quarter$clone()
-  }
-
   # inner resampling
   custom_ = rsmp("custom")
-  custom_$instantiate(task_,
+  custom_$instantiate(task_ret_week,
                       list(cv_inner$train_set(i)),
                       list(cv_inner$test_set(i)))
 
@@ -910,45 +856,39 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
 
   # outer resampling
   customo_ = rsmp("custom")
-  customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
+  customo_$instantiate(task_ret_week, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
 
-  # # nested CV for one round
+  # nested CV for one round
   print("Benchmark!")
   design = benchmark_grid(
-    tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
+    tasks = list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
     learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
     resamplings = customo_
   )
-  # bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
+  bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
 
   # save locally and to list
   print("Save")
   time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
-  file_name = paste0(i, "-", task_$id, "-", cv_inner$iters, "-", time_, ".rds")
-  print(file_name)
-  # saveRDS(bmr, file.path(mlr3_save_path, file_name))
+  saveRDS(bmr, file.path(mlr3_save_path, paste0(i, "-", cv_inner$iters, "-", time_, ".rds")))
   return(NULL)
 }
 
-# main loop
 i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
 start_time = Sys.time()
 lapply(custom_cvs, function(cv_) {
 
   # debug
-  # cv_ = custom_cvs[[12]]
+  # i = 1
+  # cv_ = custom_cvs[[1]]
 
   # get cv inner object
   cv_inner = cv_$custom_inner
   cv_outer = cv_$custom_outer
   cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
 
-  # one more test
-  # tail(cv_inner$train_set(1))
-  # head(cv_inner$test_set(1))
-  # tail(cv_inner$test_set(1))
-  # tail(cv_outer$train_set(1))
-  # head(cv_outer$test_set(1))
-
   nested_cv_benchmark(i, cv_inner, cv_outer)
 })
+
+end_time = Sys.time()
+end_time - start_time

commit 38cdb68edddf30433cec4cd5ce722b9cf2887190
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Fri Sep 15 15:02:14 2023 +0200

    make run_v2 functoin

diff --git a/H4-jobarray-/1-96-20230911120513.rds b/H4-jobarray-/1-96-20230911120513.rds
deleted file mode 100644
index 8ee0295..0000000
Binary files a/H4-jobarray-/1-96-20230911120513.rds and /dev/null differ

commit 1828f07d3b656688f72401bdbcf7cd45da8f41c1
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Fri Sep 15 15:01:45 2023 +0200

    make run_v2 functoin

diff --git a/postscriptum_light.R b/postscriptum_light.R
index f21afc7..2b8b74a 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -333,7 +333,7 @@ spy = na.omit(spy)
 
 # systemic risk
 library(PerformanceAnalytics)
-task_ = "taskRetQuarter"
+task_ = "taskRetMonth"
 sample_ = predictions_dt_ensemble[task == task_]
 sample_ = unique(sample_)
 setorder(sample_, date)
diff --git a/run_v2.R b/run_v2.R
new file mode 100644
index 0000000..96201a9
--- /dev/null
+++ b/run_v2.R
@@ -0,0 +1,954 @@
+library(data.table)
+library(gausscov)
+library(paradox)
+library(mlr3)
+library(mlr3pipelines)
+library(mlr3viz)
+library(mlr3tuning)
+library(mlr3misc)
+library(future)
+library(future.apply)
+
+
+# SETUP -------------------------------------------------------------------
+# create folder in which we will save results
+mlr3_save_path = paste0("./H4-jobarray-", Sys.getenv('PBS_ARRAY_ID'))
+if (!dir.exists(mlr3_save_path)) {
+  dir.create(mlr3_save_path)
+}
+
+# utils https://stackoverflow.com/questions/1995933/number-of-months-between-two-dates
+monnb <- function(d) {
+  lt <- as.POSIXlt(as.Date(d, origin="1900-01-01"))
+  lt$year*12 + lt$mon }
+mondf <- function(d1, d2) { monnb(d2) - monnb(d1) }
+
+# snake to camel
+snakeToCamel <- function(snake_str) {
+  # Replace underscores with spaces
+  spaced_str <- gsub("_", " ", snake_str)
+
+  # Convert to title case using tools::toTitleCase
+  title_case_str <- tools::toTitleCase(spaced_str)
+
+  # Remove spaces and make the first character lowercase
+  camel_case_str <- gsub(" ", "", title_case_str)
+  camel_case_str <- sub("^.", tolower(substr(camel_case_str, 1, 1)), camel_case_str)
+
+  # I haeve added this to remove dot
+  camel_case_str <- gsub("\\.", "", camel_case_str)
+
+  return(camel_case_str)
+}
+
+
+# PREPARE DATA ------------------------------------------------------------
+print("Prepare data")
+
+# read predictors
+data_tbl = fread("./pead-predictors-update.csv")
+# data_tbl <- fread("D:/features/pead-predictors-20230523202603.csv")
+
+# convert tibble to data.table
+DT = as.data.table(data_tbl)
+
+# create group variable
+DT[, date_rolling := as.IDate(date_rolling)]
+DT[, yearmonthid := round(date_rolling, digits = "month")]
+DT[, .(date, date_rolling, yearmonthid)]
+DT[, yearmonthid := as.integer(yearmonthid)]
+DT[, .(date, date_rolling, yearmonthid)]
+
+# define predictors
+cols_non_features <- c("symbol", "date", "time", "right_time",
+                       "bmo_return", "amc_return",
+                       "open", "high", "low", "close", "volume", "returns",
+                       "yearmonthid", "date_rolling"
+)
+targets <- c(colnames(DT)[grep("ret_excess", colnames(DT))])
+cols_features <- setdiff(colnames(DT), c(cols_non_features, targets))
+
+# change feature and targets columns names due to lighgbm
+cols_features_new = vapply(cols_features, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
+setnames(DT, cols_features, cols_features_new)
+cols_features = cols_features_new
+targets_new = vapply(targets, snakeToCamel, FUN.VALUE = character(1L), USE.NAMES = FALSE)
+setnames(DT, targets, targets_new)
+targets = targets_new
+
+# convert columns to numeric. This is important only if we import existing features
+chr_to_num_cols <- setdiff(colnames(DT[, .SD, .SDcols = is.character]), c("symbol", "time", "right_time"))
+print(chr_to_num_cols)
+DT <- DT[, (chr_to_num_cols) := lapply(.SD, as.numeric), .SDcols = chr_to_num_cols]
+
+# remove constant columns in set
+features_ <- DT[, ..cols_features]
+remove_cols <- colnames(features_)[apply(features_, 2, var, na.rm=TRUE) == 0]
+print(paste0("Removing feature with 0 standard deviation: ", remove_cols))
+cols_features <- setdiff(cols_features, remove_cols)
+
+# convert variables with low number of unique values to factors
+int_numbers = na.omit(DT[, ..cols_features])[, lapply(.SD, function(x) all(floor(x) == x))]
+int_cols = colnames(DT[, ..cols_features])[as.matrix(int_numbers)[1,]]
+factor_cols = DT[, ..int_cols][, lapply(.SD, function(x) length(unique(x)))]
+factor_cols = as.matrix(factor_cols)[1, ]
+factor_cols = factor_cols[factor_cols <= 100]
+DT = DT[, (names(factor_cols)) := lapply(.SD, as.factor), .SD = names(factor_cols)]
+
+# remove observations with missing target
+# if we want to keep as much data as possible an use only one predicitn horizont
+# we can skeep this step
+DT = na.omit(DT, cols = setdiff(targets, colnames(DT)[grep("xtreme", colnames(DT))]))
+
+# change IDate to date, because of error
+# Assertion on 'feature types' failed: Must be a subset of
+# {'logical','integer','numeric','character','factor','ordered','POSIXct'},
+# but has additional elements {'IDate'}.
+DT[, date := as.POSIXct(date, tz = "UTC")]
+# DT[, .(symbol,date, date_rolling, yearmonthid)]
+
+# sort
+# this returns error on HPC. Some problem with memory
+# setorder(DT, date)
+print("This was the problem")
+# DT = DT[order(date)] # DOESNT WORK TOO
+DT = DT[order(yearmonthid)]
+print("This was the problem. Solved.")
+
+
+
+# TASKS -------------------------------------------------------------------
+print("Tasks")
+
+# id coluns we always keep
+id_cols = c("symbol", "date", "yearmonthid")
+
+# convert date to PosixCt because it is requireed by mlr3
+DT[, date := as.POSIXct(date, tz = "UTC")]
+
+# task with future week returns as target
+target_ = colnames(DT)[grep("^ret.*xcess.*tand.*5", colnames(DT))]
+cols_ = c(id_cols, target_, cols_features)
+task_ret_week <- as_task_regr(DT[, ..cols_],
+                              id = "taskRetWeek",
+                              target = target_)
+
+# task with future month returns as target
+target_ = colnames(DT)[grep("^ret.*xcess.*tand.*22", colnames(DT))]
+cols_ = c(id_cols, target_, cols_features)
+task_ret_month <- as_task_regr(DT[, ..cols_],
+                               id = "taskRetMonth",
+                               target = target_)
+
+# task with future 2 months returns as target
+target_ = colnames(DT)[grep("^ret.*xcess.*tand.*44", colnames(DT))]
+cols_ = c(id_cols, target_, cols_features)
+task_ret_month2 <- as_task_regr(DT[, ..cols_],
+                                id = "taskRetMonth2",
+                                target = target_)
+
+# task with future 2 months returns as target
+target_ = colnames(DT)[grep("^ret.*xcess.*tand.*66", colnames(DT))]
+cols_ = c(id_cols, target_, cols_features)
+task_ret_quarter <- as_task_regr(DT[, ..cols_],
+                                 id = "taskRetQuarter",
+                                 target = target_)
+
+# set roles for symbol, date and yearmonth_id
+task_ret_week$col_roles$feature = setdiff(task_ret_week$col_roles$feature,
+                                          id_cols)
+task_ret_month$col_roles$feature = setdiff(task_ret_month$col_roles$feature,
+                                           id_cols)
+task_ret_month2$col_roles$feature = setdiff(task_ret_month2$col_roles$feature,
+                                            id_cols)
+task_ret_quarter$col_roles$feature = setdiff(task_ret_quarter$col_roles$feature,
+                                             id_cols)
+
+
+# CROSS VALIDATIONS -------------------------------------------------------
+print("Cross validations")
+
+# create train, tune and test set
+nested_cv_split = function(task,
+                           train_length = 12,
+                           tune_length = 1,
+                           test_length = 1,
+                           gap_length = 3,
+                           id = task$id) {
+
+  # get year month id data
+  # task = task_ret_quarter$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom", id = task$id)
+  custom_outer = rsmp("custom", id = task$id)
+
+
+  # util vars
+  start_folds = 1:(length(groups_v)-train_length-tune_length-test_length-gap_length)
+  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups <- lapply(start_folds,
+                         function(x) groups_v[x:(x+train_length-1)])
+  train_sets <- lapply(train_groups, get_row_ids)
+
+  # create tune set
+  tune_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+train_length):(x+train_length+tune_length-1)])
+  tune_sets <- lapply(tune_groups, get_row_ids)
+
+  # test train and tune
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_2 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create test sets
+  insample_length = train_length + tune_length + gap_length
+  test_groups <- lapply(start_folds,
+                        function(x) groups_v[(x+insample_length):(x+insample_length+test_length-1)])
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_4 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # test
+  # as.Date(train_groups[[2]])
+  # as.Date(tune_groups[[2]])
+  # as.Date(test_groups[[2]])
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# generate cv's
+train_sets = seq(12, 12 * 3, 12)
+validation_sets = train_sets / 12
+gap_sets = c(0:3)
+mat = cbind(train = train_sets, tune = validation_sets)
+expanded_list  = lapply(gap_sets, function(v) {
+  cbind.data.frame(mat, gap = v)
+})
+cv_param_grid = rbindlist(expanded_list)
+custom_cvs = list()
+for (i in 1:nrow(cv_param_grid)) {
+  print(i)
+  param_ = cv_param_grid[i]
+  if (param_$gap == 0) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_week,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+  } else if (param_$gap == 1) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_month,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  } else if (param_$gap == 2) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_month2,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  } else if (param_$gap == 3) {
+    custom_cvs[[i]] = nested_cv_split(task_ret_quarter,
+                                      param_$train,
+                                      param_$tune,
+                                      1,
+                                      param_$gap)
+
+  }
+}
+
+# test
+length(custom_cvs) == nrow(cv_param_grid)
+
+# create expanding window function
+nested_cv_split_expanding = function(task,
+                                     train_length_start = 6,
+                                     tune_length = 1,
+                                     test_length = 1) {
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom")
+  custom_outer = rsmp("custom")
+
+  # get year month id data
+  # task = task_ret_week$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+
+  # util vars
+  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
+
+  # create tune set
+  tune_groups <- lapply((train_length_start+1):length(groups_v), function(i) groups_v[i:(i+tune_length-1)])
+  index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  tune_groups = tune_groups[index_keep]
+
+  # equalize train and tune sets
+  train_groups = train_groups[1:length(tune_groups)]
+
+  # create test sets
+  insample_length = vapply(train_groups, function(x) as.integer(length(x) + tune_length), FUN.VALUE = integer(1))
+  test_groups <- lapply((insample_length+1):length(groups_v), function(i) groups_v[i:(i+test_length-1)])
+  index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  test_groups = test_groups[index_keep]
+
+  # equalize train, tune and test sets
+  train_groups = train_groups[1:length(test_groups)]
+  tune_groups = tune_groups[1:length(test_groups)]
+
+  # make sets
+  train_sets <- lapply(train_groups, get_row_ids)
+  tune_sets <- lapply(tune_groups, get_row_ids)
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_2 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_4 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# generate cv's for expanding windows
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
+                                                               train_length_start = 6,
+                                                               tune_length = 1,
+                                                               test_length = 1)
+
+# test if tain , validation and tst set follow logic
+lapply(seq_along(custom_cvs), function(i) {
+  # extract custyom cv
+  custom_cvs_ = custom_cvs[[i]]
+  custom_inner = custom_cvs_$custom_inner
+  custom_outer = custom_cvs_$custom_outer
+
+  # test set start after train set
+  test1 = all(vapply(1:custom_inner$iters, function(i) {
+    (tail(custom_inner$train_set(i), 1) + 1) == custom_inner$test_set(i)[1]
+  }, FUN.VALUE = logical(1L)))
+
+  # train set in outersample contains ids in innersample 1
+  test2  = all(vapply(1:custom_inner$iters, function(i) {
+    all(c(custom_inner$train_set(i),
+          custom_inner$test_set(i)) == custom_outer$train_set(i))
+  }, FUN.VALUE = logical(1L)))
+  c(test1, test2)
+})
+
+
+# ADD PIPELINES -----------------------------------------------------------
+print("Add pipelines")
+
+# source pipes, filters and other
+source("mlr3_winsorization.R")
+source("mlr3_uniformization.R")
+source("mlr3_gausscov_f1st.R")
+source("mlr3_gausscov_f3st.R")
+source("mlr3_dropna.R")
+source("mlr3_dropnacol.R")
+source("mlr3_filter_drop_corr.R")
+source("mlr3_winsorizationsimple.R")
+source("mlr3_winsorizationsimplegroup.R")
+source("PipeOpPCAExplained.R")
+# measures
+source("Linex.R")
+source("AdjLoss2.R")
+
+# add my pipes to mlr dictionary
+mlr_pipeops$add("uniformization", PipeOpUniform)
+mlr_pipeops$add("winsorize", PipeOpWinsorize)
+mlr_pipeops$add("winsorizesimple", PipeOpWinsorizeSimple)
+mlr_pipeops$add("winsorizesimplegroup", PipeOpWinsorizeSimpleGroup)
+mlr_pipeops$add("dropna", PipeOpDropNA)
+mlr_pipeops$add("dropnacol", PipeOpDropNACol)
+mlr_pipeops$add("dropcorr", PipeOpDropCorr)
+mlr_pipeops$add("pca_explained", PipeOpPCAExplained)
+mlr_filters$add("gausscov_f1st", FilterGausscovF1st)
+mlr_filters$add("gausscov_f3st", FilterGausscovF3st)
+mlr_measures$add("linex", Linex)
+mlr_measures$add("adjloss2", AdjLoss2)
+
+
+# GRAPH V2 ----------------------------------------------------------------
+# TODO: ADD abess
+# graph_template_abess =
+#   po("subsample") %>>% # uncomment this for hyperparameter tuning
+#   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+#   po("dropna", id = "dropna") %>>%
+#   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+#   po("fixfactors", id = "fixfactors") %>>%
+#   # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+#   po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+#   po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+#   po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+#   po("uniformization") %>>%
+#   po("dropna", id = "dropna_v2") %>>%
+#   po("learner", learner = lrn("regr.abess"))
+
+
+# graph template
+graph_template =
+  po("subsample") %>>% # uncomment this for hyperparameter tuning
+  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+  po("dropna", id = "dropna") %>>%
+  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+  po("fixfactors", id = "fixfactors") %>>%
+  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+  po("uniformization") %>>%
+  po("dropna", id = "dropna_v2") %>>%
+  # filters
+  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+              po("filter", filter = flt("relief"), filter.frac = 0.05),
+              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
+  # po("nop", id = "nop_filter"))) %>>%
+  po("unbranch", id = "filter_unbranch") %>>%
+  # modelmatrix
+  po("branch", options = c("nop_interaction", "modelmatrix"), id = "interaction_branch") %>>%
+  gunion(list(
+    po("nop", id = "nop_interaction"),
+    po("modelmatrix", formula = ~ . ^ 2))) %>>%
+  po("unbranch", id = "interaction_unbranch") %>>%
+  po("removeconstants", id = "removeconstants_3", ratio = 0)
+
+# hyperparameters template
+search_space_template = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+  # interaction
+  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix"))
+)
+
+# random forest graph
+graph_rf = graph_template %>>%
+  po("learner", learner = lrn("regr.ranger"))
+plot(graph_rf)
+graph_rf = as_learner(graph_rf)
+as.data.table(graph_rf$param_set)[, .(id, class, lower, upper, levels)]
+search_space_rf = search_space_template$clone()
+search_space_rf$add(
+  ps(regr.ranger.max.depth  = p_int(1, 40),
+     regr.ranger.replace    = p_lgl(),
+     regr.ranger.mtry.ratio = p_dbl(0.1, 1))
+)
+
+# xgboost graph
+graph_xgboost = graph_template %>>%
+  po("learner", learner = lrn("regr.xgboost"))
+plot(graph_xgboost)
+graph_xgboost = as_learner(graph_xgboost)
+as.data.table(graph_xgboost$param_set)[grep("depth", id), .(id, class, lower, upper, levels)]
+search_space_xgboost = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+  # interaction
+  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+  # learner
+  regr.xgboost.alpha     = p_dbl(0.001, 100, logscale = TRUE),
+  regr.xgboost.max_depth = p_int(1, 20),
+  regr.xgboost.eta       = p_dbl(0.0001, 1, logscale = TRUE),
+  regr.xgboost.nrounds   = p_int(1, 5000)
+)
+
+# BART graph
+# Error in makeModelMatrixFromDataFrame(x.test, if (!is.null(drop)) drop else TRUE) :
+#   when list, drop must have length equal to x
+# This happened PipeOp regr.bart's $predict()
+graph_bart = graph_template %>>%
+  po("learner", learner = lrn("regr.bart"))
+graph_bart = as_learner(graph_bart)
+as.data.table(graph_bart$param_set)[, .(id, class, lower, upper, levels)]
+search_space_bart = search_space_template$clone()
+search_space_bart$add(
+  ps(regr.bart.k = p_int(lower = 1, upper = 10))
+  # regr.bart.nu = p_dbl(lower = 0.1, upper = 10),
+  # regr.bart.n_trees = p_int(lower = 10, upper = 100))
+)
+# chatgpt returns this
+# n_trees = p_int(lower = 10, upper = 100),
+# n_chains = p_int(lower = 1, upper = 5),
+# k = p_int(lower = 1, upper = 10),
+# m_try = p_int(lower = 1, upper = 13),
+# nu = p_dbl(lower = 0.1, upper = 10),
+# alpha = p_dbl(lower = 0.01, upper = 1),
+# beta = p_dbl(lower = 0.01, upper = 1),
+# burn = p_int(lower = 10, upper = 100),
+# iter = p_int(lower = 100, upper = 1000)
+
+
+# # catboost graph
+# ### REMOVED FROM MLR3EXTRALEARNERS FROM VERSION 0.7.0.
+# graph_catboost = graph_template %>>%
+#   po("learner", learner = lrn("regr.catboost"))
+# graph_catboost = as_learner(graph_catboost)
+# as.data.table(graph_catboost$param_set)[, .(id, class, lower, upper, levels)]
+# search_space_catboost = search_space_template$clone()
+# # https://catboost.ai/en/docs/concepts/parameter-tuning#description10
+# search_space_catboost$add(
+#   ps(regr.catboost.learning_rate   = p_dbl(lower = 0.01, upper = 0.3),
+#      regr.catboost.depth           = p_int(lower = 4, upper = 10),
+#      regr.catboost.l2_leaf_reg     = p_int(lower = 1, upper = 5),
+#      regr.catboost.random_strength = p_int(lower = 0, upper = 3))
+# )
+
+# # gamboost graph
+# # Error in eval(predvars, data, env) : object 'adxDx14' not found
+# # This happened PipeOp regr.gamboost's $train()
+# # In addition: There were 50 or more warnings (use warnings() to see the first 50)
+# graph_gamboost = graph_template %>>%
+#   po("learner", learner = lrn("regr.gamboost"))
+# graph_gamboost = as_learner(graph_gamboost)
+# as.data.table(graph_gamboost$param_set)[, .(id, class, lower, upper, levels)]
+# search_space_gamboost = search_space_template$clone()
+# # https://catboost.ai/en/docs/concepts/parameter-tuning#description10
+# search_space_gamboost$add(
+#   ps(regr.gamboost.mstop       = p_int(lower = 10, upper = 100),
+#      regr.gamboost.nu          = p_dbl(lower = 0.01, upper = 0.5),
+#      regr.gamboost.baselearner = p_fct(levels = c("bbs", "bols", "btree")))
+# )
+
+# # gamboost graph
+# # Error in eval(predvars, data, env) : object 'adxDx14' not found
+# # This happened PipeOp regr.gamboost's $train()
+# # In addition: There were 50 or more warnings (use warnings() to see the first 50)
+# graph_gamboost = graph_template %>>%
+#   po("learner", learner = lrn("regr.gamboost"))
+# graph_gamboost = as_learner(graph_gamboost)
+# as.data.table(graph_gamboost$param_set)[, .(id, class, lower, upper, levels)]
+# search_space_gamboost = search_space_template$clone()
+# # https://catboost.ai/en/docs/concepts/parameter-tuning#description10
+# search_space_gamboost$add(
+#   ps(regr.gamboost.mstop       = p_int(lower = 10, upper = 100),
+#      regr.gamboost.nu          = p_dbl(lower = 0.01, upper = 0.5),
+#      regr.gamboost.baselearner = p_fct(levels = c("bbs", "bols", "btree")))
+# )
+
+# kknn graph
+graph_kknn = graph_template %>>%
+  po("learner", learner = lrn("regr.kknn"))
+graph_kknn = as_learner(graph_kknn)
+as.data.table(graph_kknn$param_set)[, .(id, class, lower, upper, levels)]
+search_space_kknn = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+  # interaction
+  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+  # learner
+  regr.kknn.k        = p_int(lower = 1, upper = 50, logscale = TRUE),
+  regr.kknn.distance = p_dbl(lower = 1, upper = 5),
+  regr.kknn.kernel   = p_fct(levels = c("rectangular", "optimal", "epanechnikov", "biweight", "triweight", "cos",  "inv",  "gaussian", "rank"))
+)
+
+# nnet graph
+graph_nnet = graph_template %>>%
+  po("learner", learner = lrn("regr.nnet", MaxNWts = 40000))
+graph_nnet = as_learner(graph_nnet)
+as.data.table(graph_nnet$param_set)[, .(id, class, lower, upper, levels)]
+search_space_nnet = search_space_template$clone()
+search_space_nnet$add(
+  ps(regr.nnet.size  = p_int(lower = 5, upper = 30),
+     regr.nnet.decay = p_dbl(lower = 0.0001, upper = 0.1),
+     regr.nnet.maxit = p_int(lower = 50, upper = 500))
+)
+
+# ksvm graph
+graph_ksvm = graph_template %>>%
+  po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
+graph_ksvm = as_learner(graph_ksvm)
+as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
+search_space_ksvm = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+  # interaction
+  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+  # learner
+  regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
+                                       "laplacedot", "besseldot", "anovadot")),
+  regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
+  regr.ksvm.degree  = p_int(lower = 1, upper = 5,
+                            depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
+  regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
+)
+
+# LAST
+# lightgbm graph
+# [LightGBM] [Fatal] Do not support special JSON characters in feature name.
+graph_template =
+  po("subsample") %>>% # uncomment this for hyperparameter tuning
+  po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+  po("dropna", id = "dropna") %>>%
+  po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+  po("fixfactors", id = "fixfactors") %>>%
+  # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+  po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+  po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+  po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+  po("uniformization") %>>%
+  po("dropna", id = "dropna_v2") %>>%
+  # filters
+  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
+  gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+              po("filter", filter = flt("relief"), filter.frac = 0.05),
+              po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
+  po("unbranch", id = "filter_unbranch")
+search_space_template = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov"))
+)
+graph_lightgbm = graph_template %>>%
+  po("learner", learner = lrn("regr.lightgbm"))
+graph_lightgbm = as_learner(graph_lightgbm)
+as.data.table(graph_lightgbm$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
+search_space_lightgbm = search_space_template$clone()
+search_space_lightgbm$add(
+  ps(regr.lightgbm.max_depth     = p_int(lower = 2, upper = 10),
+     regr.lightgbm.learning_rate = p_dbl(lower = 0.001, upper = 0.3),
+     regr.lightgbm.num_leaves    = p_int(lower = 10, upper = 100))
+)
+
+# earth graph
+graph_earth = graph_template %>>%
+  po("learner", learner = lrn("regr.earth"))
+graph_earth = as_learner(graph_earth)
+as.data.table(graph_earth$param_set)[grep("sample", id), .(id, class, lower, upper, levels)]
+search_space_earth = search_space_template$clone()
+search_space_earth$add(
+  ps(regr.earth.degree  = p_int(lower = 1, upper = 4),
+     # regr.earth.penalty = p_int(lower = 1, upper = 5),
+     regr.earth.nk      = p_int(lower = 50, upper = 250))
+)
+
+# threads
+threads = as.integer(Sys.getenv("NCPUS"))
+set_threads(graph_rf, n = threads)
+set_threads(graph_xgboost, n = threads)
+# set_threads(graph_bart, n = threads)
+set_threads(graph_ksvm, n = threads)
+set_threads(graph_nnet, n = threads)
+set_threads(graph_kknn, n = threads)
+set_threads(graph_lightgbm, n = threads)
+set_threads(graph_earth, n = threads)
+
+
+# NESTED CV BENCHMARK -----------------------------------------------------
+print("Benchmark")
+
+# nested for loop
+list.files(mlr3_save_path, full.names = TRUE)
+nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
+
+  # debug
+  # i = 1
+  print(i)
+
+  # choose task_
+  print(cv_inner$id)
+  if (cv_inner$id == "taskRetWeek") {
+    task_ = task_ret_week$clone()
+  } else if (cv_inner$id == "taskRetMonth") {
+    task_ = task_ret_month$clone()
+  } else if (cv_inner$id == "taskRetMonth2") {
+    task_ = task_ret_month2$clone()
+  } else if (cv_inner$id == "taskRetQuarter") {
+    task_ = task_ret_quarter$clone()
+  }
+
+  # inner resampling
+  custom_ = rsmp("custom")
+  custom_$instantiate(task_,
+                      list(cv_inner$train_set(i)),
+                      list(cv_inner$test_set(i)))
+
+  # auto tuner rf
+  at_rf = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_rf,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_rf,
+    terminator = trm("none")
+  )
+
+  # auto tuner xgboost
+  at_xgboost = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_xgboost,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_xgboost,
+    terminator = trm("none")
+  )
+
+  # auto tuner BART
+  at_bart = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_bart,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_bart,
+    terminator = trm("none")
+  )
+
+  # auto tuner ksvm
+  at_ksvm = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_ksvm,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_ksvm,
+    terminator = trm("none")
+  )
+
+  # auto tuner nnet
+  at_nnet = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_nnet,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_nnet,
+    terminator = trm("none")
+  )
+
+  # auto tuner lightgbm
+  at_lightgbm = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_lightgbm,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_lightgbm,
+    terminator = trm("none")
+  )
+
+  # auto tuner earth
+  at_earth = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_earth,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_earth,
+    terminator = trm("none")
+  )
+
+  # auto tuner kknn
+  at_kknn = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_kknn,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_kknn,
+    terminator = trm("none")
+  )
+
+  # # auto tuner mboost
+  # at_gamboost = auto_tuner(
+  #   tuner = tnr("hyperband", eta = 5),
+  #   learner = graph_gamboost,
+  #   resampling = custom_,
+  #   measure = msr("adjloss2"),
+  #   search_space = search_space_gamboost,
+  #   terminator = trm("none")
+  # )
+
+  # # auto tuner catboost
+  # at_catboost = auto_tuner(
+  #   tuner = tnr("hyperband", eta = 5),
+  #   learner = graph_catboost,
+  #   resampling = custom_,
+  #   measure = msr("adjloss2"),
+  #   search_space = search_space_catboost,
+  #   terminator = trm("none")
+  # )
+
+  # outer resampling
+  customo_ = rsmp("custom")
+  customo_$instantiate(task_, list(cv_outer$train_set(i)), list(cv_outer$test_set(i)))
+
+  # # nested CV for one round
+  print("Benchmark!")
+  design = benchmark_grid(
+    tasks = task_, # list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
+    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
+    resamplings = customo_
+  )
+  # bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
+
+  # save locally and to list
+  print("Save")
+  time_ = format.POSIXct(Sys.time(), format = "%Y%m%d%H%M%S")
+  file_name = paste0(i, "-", task_$id, "-", cv_inner$iters, "-", time_, ".rds")
+  print(file_name)
+  # saveRDS(bmr, file.path(mlr3_save_path, file_name))
+  return(NULL)
+}
+
+# main loop
+i = as.integer(Sys.getenv('PBS_ARRAY_INDEX'))
+start_time = Sys.time()
+lapply(custom_cvs, function(cv_) {
+
+  # debug
+  # cv_ = custom_cvs[[12]]
+
+  # get cv inner object
+  cv_inner = cv_$custom_inner
+  cv_outer = cv_$custom_outer
+  cat("Number of iterations fo cv inner is ", cv_inner$iters, "\n")
+
+  # one more test
+  # tail(cv_inner$train_set(1))
+  # head(cv_inner$test_set(1))
+  # tail(cv_inner$test_set(1))
+  # tail(cv_outer$train_set(1))
+  # head(cv_outer$test_set(1))
+
+  nested_cv_benchmark(i, cv_inner, cv_outer)
+})

commit c716c46f7aed68f0bf29b460d76becd20d49b073
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Fri Sep 15 00:27:12 2023 +0200

    remove ksvm learner because it is mazbe slow

diff --git a/run.R b/run.R
index 1a8abd5..6835530 100644
--- a/run.R
+++ b/run.R
@@ -862,7 +862,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn, at_ksvm),
+    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn), # at_ksvm
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)

commit 77b9e04a7ad4a9342bfd0d9ef2342f8f82f1b3fb
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Wed Sep 13 09:34:02 2023 +0200

    remove some optinons from regr.ksvm.kernel

diff --git a/postscriptum_light.R b/postscriptum_light.R
index 774e828..f21afc7 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -1,7 +1,8 @@
 library(data.table)
 library(mlr3verse)
 library(AzureStor)
-
+library(readr)
+# library(duckdb)
 
 
 
@@ -135,7 +136,7 @@ mlr_measures$add("adjloss2", AdjLoss2)
 id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 
 # set files with benchmarks
-bmr_files = list.files(list.files("F:", pattern = "^H4-v4", full.names = TRUE), full.names = TRUE)
+bmr_files = list.files(list.files("F:", pattern = "^H4-v5", full.names = TRUE), full.names = TRUE)
 
 # arrange files
 cv_ = as.integer(gsub("\\d+-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
@@ -230,6 +231,8 @@ sign_response_max = predictions_dt_ensemble[, max(sign_response)]
 sign_response_seq = seq(as.integer(sign_response_max / 2), sign_response_max - 1)
 cols_sign_response_pos = paste0("response_sign_sign_pos", sign_response_seq)
 predictions_dt_ensemble[, (cols_sign_response_pos) := lapply(sign_response_seq, function(x) sign_response > x)]
+cols_sign_response_neg = paste0("response_sign_sign_neg", sign_response_seq)
+predictions_dt_ensemble[, (cols_sign_response_neg) := lapply(sign_response_seq, function(x) sign_response < -x)]
 
 # check only sign ensamble performance
 lapply(cols_sign_response_pos, function(x) {
@@ -265,6 +268,7 @@ lapply(cols_sign_response_pos, function(x) {
 
 
 # save to azure for QC backtest
+############### CHANGE CODE ABOVE FILTER POSITIVE ##################
 cont = storage_container(BLOBENDPOINT, "qc-backtest")
 lapply(unique(predictions_dt_ensemble$task), function(x) {
   # debug
@@ -299,17 +303,86 @@ lapply(unique(predictions_dt_ensemble$task), function(x) {
   # save to azure blob
   print(colnames(y))
   file_name_ =  paste0("pead-", x, ".csv")
-  storage_write_csv(y, cont, file_name_)
+  storage_write_csv(y, cont, file_name_, col_names = FALSE)
   # universe = y[, .(date, symbol)]
   # storage_write_csv(universe, cont, "pead_task_ret_week_universe.csv", col_names = FALSE)
 })
 
+# save data for PEAD-SPY
+dt_sample = predictions_dt_ensemble[, .(task, date, mean_response)]
+dt_sample = unique(dt_sample)
+dt_sample = dt_sample[, .(resp = sum(mean_response)), by = date]
+setorder(dt_sample, date)
+dt_sample[, date := as.character(date)]
+cont = storage_container(BLOBENDPOINT, "qc-backtest")
+storage_write_csv(dt_sample, cont, paste0("pead-spy.csv"), col_names = FALSE)
+
+# import SPY data
+con <- dbConnect(duckdb::duckdb())
+query <- sprintf("
+    SELECT *
+    FROM 'F:/lean_root/data/all_stocks_daily.csv'
+    WHERE Symbol = 'spy'
+")
+spy <- dbGetQuery(con, query)
+dbDisconnect(con)
+spy = as.data.table(spy)
+spy = spy[, .(date = Date, close = `Adj Close`)]
+spy[, returns := close / shift(close) - 1]
+spy = na.omit(spy)
+
 # systemic risk
-predictors_pos = predictions_dt_ensemble[, .(response_sign_sign_pos_agg = sum(response_sign_sign_pos)), by = "date"]
-predictors_neg = predictions_dt_ensemble[, .(response_sign_sign_pos_agg = sum(response_sign_sign_neg)), by = "date"]
-setorder(predictors_pos, date)
-setorder(predictors_neg, date)
-predictors_diff = as.xts.data.table(predictors_pos) - as.xts.data.table(predictors_neg)
-plot(as.xts.data.table(na.omit(predictors_pos)))
-plot(as.xts.data.table(na.omit(predictors_neg)))
-plot(predictors_diff)
+library(PerformanceAnalytics)
+task_ = "taskRetQuarter"
+sample_ = predictions_dt_ensemble[task == task_]
+sample_ = unique(sample_)
+setorder(sample_, date)
+pos_cols = colnames(sample_)[grep("pos", colnames(sample_))]
+neg_cols = colnames(sample_)[grep("neg", colnames(sample_))]
+new_dt = sample_[, ..pos_cols] - sample_[, ..neg_cols]
+setnames(new_dt, gsub("pos", "net", pos_cols))
+sample_ = cbind(sample_, new_dt)
+sample_[, max(date)]
+plot(as.xts.data.table(sample_[, .N, by = date]))
+
+indicator = sample_[, .(response_sign_sign_net_agg = sum(response_sign_sign_net9),
+                        response_sign_sign_neg_agg = sum(response_sign_sign_neg9),
+                        mean_response_agg = sum(mean_response)),
+                    by = "date"]
+indicator[, `:=`(
+  response_sign_sign_net_agg = TTR::EMA(response_sign_sign_net_agg, 5, na.rm = TRUE),
+  response_sign_sign_neg_agg = TTR::EMA(response_sign_sign_neg_agg, 5, na.rm = TRUE),
+  mean_response_agg = TTR::EMA(mean_response_agg, 5, na.rm = TRUE)
+)
+]
+indicator = indicator[date > as.Date("2017-01-01") & date < as.Date("2023-01-01")]
+plot(as.xts.data.table(indicator)[, 1])
+plot(as.xts.data.table(indicator)[, 2])
+plot(as.xts.data.table(indicator)[, 3])
+
+backtest_data =  merge(spy, indicator, by = "date")
+backtest_data = na.omit(backtest_data)
+backtest_data[, signal := 1]
+backtest_data[shift(mean_response_agg) < 0, signal := 0]
+backtest_data_xts = as.xts.data.table(backtest_data[, .(date, benchmark = returns, strategy = ifelse(signal == 0, 0, returns * signal * 2))])
+PerformanceAnalytics::charts.PerformanceSummary(backtest_data_xts)
+# backtest performance
+Performance <- function(x) {
+  cumRetx = Return.cumulative(x)
+  annRetx = Return.annualized(x, scale=252)
+  sharpex = SharpeRatio.annualized(x, scale=252)
+  winpctx = length(x[x > 0])/length(x[x != 0])
+  annSDx = sd.annualized(x, scale=252)
+
+  DDs <- findDrawdowns(x)
+  maxDDx = min(DDs$return)
+  # maxLx = max(DDs$length)
+
+  Perf = c(cumRetx, annRetx, sharpex, winpctx, annSDx, maxDDx) # , maxLx)
+  names(Perf) = c("Cumulative Return", "Annual Return","Annualized Sharpe Ratio",
+                  "Win %", "Annualized Volatility", "Maximum Drawdown") # "Max Length Drawdown")
+  return(Perf)
+}
+Performance(backtest_data_xts[, 1])
+Performance(backtest_data_xts[, 2])
+
diff --git a/run.R b/run.R
index d60d66d..1a8abd5 100644
--- a/run.R
+++ b/run.R
@@ -654,8 +654,7 @@ search_space_ksvm = ps(
   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
   # learner
   regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
-                                       "tanhdot", "laplacedot", "besseldot",
-                                       "anovadot", "splinedot")),
+                                       "laplacedot", "besseldot", "anovadot")),
   regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
   regr.ksvm.degree  = p_int(lower = 1, upper = 5,
                             depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),

commit 3b3444026c2d3333d7a0a8df1bbaf800fdaa324a
Author: unknown <mislav.sagovac@contentio.biz>
Date:   Tue Sep 12 13:23:14 2023 +0200

    add ksvm learner and add expanding window

diff --git a/H4-jobarray-/1-96-20230911120513.rds b/H4-jobarray-/1-96-20230911120513.rds
new file mode 100644
index 0000000..8ee0295
Binary files /dev/null and b/H4-jobarray-/1-96-20230911120513.rds differ
diff --git a/image.def b/image.def
index 470d3e7..a196e72 100644
--- a/image.def
+++ b/image.def
@@ -15,6 +15,7 @@ From: r-base:4.3.0
   R --slave -e 'install.packages("future")'
   R --slave -e 'install.packages("future.apply")'
   R --slave -e 'install.packages("gausscov")'
+  R --slave -e 'install.packages("FSelectorRcpp")'
   R --slave -e 'install.packages("igraph")'
   R --slave -e 'install.packages("mlr3")'
   R --slave -e 'install.packages("mlr3misc")'
@@ -36,6 +37,7 @@ From: r-base:4.3.0
   R --slave -e 'install.packages("lightgbm")'
   R --slave -e 'install.packages("earth")'
   R --slave -e 'install.packages("kknn")'
+  R --slave -e 'install.packages("kernlab")'
   R --slave -e 'install.packages("torch")'
   R --slave -e 'remotes::install_github("mlr-org/mlr3torch")'
   # R --slave -e 'remotes::install_url("https://github.com/catboost/catboost/releases/download/v1.2.1/catboost-R-Linux-1.2.1.tgz", build_opts = c("--no-multiarch", "--no-test-load"))'
diff --git a/lukamislavtest.R b/lukamislavtest.R
new file mode 100644
index 0000000..2094535
--- /dev/null
+++ b/lukamislavtest.R
@@ -0,0 +1,38 @@
+library(data.table)
+library(mlr3)
+
+
+synology_path = "C:/Users/Mislav/SynologyDrive"
+dta_path = file.path(synology_path, "H2")
+
+# benchmark files ----
+files_ = list.files(dta_path, full.names = TRUE)
+files_info = file.info(files_)
+files_info = files_info[order(files_info$ctime), ]
+
+# read banchmark results
+bmr = readRDS(files_[1])
+bmr_dt = as.data.table(bmr)
+
+# aggreaget results
+aggregate = bmr$aggregate(msrs(c("regr.mse", "regr.mae", "regr.rmse")))
+
+# instances across folds
+rresults = bmr$resample_result(1)
+instance = rresults$learners[[1]]$state$model$tuning_instance
+
+# number of components
+bmr_dt$learner[[1]]$state$model$learner$state
+bmr_dt$learner[[1]]$state$model$learner$state$model$pca_explained
+bmr_dt$learner[[3]]$state$model$learner$state$model$pca_explained
+
+bmr_dt$learner[[1]]$state$model$learner$state$model$pca_explained
+x = bmr$resample_result(1)
+x$learners[[1]]$state$model$learner$state$model$pca_explained
+
+x$learners[[1]]$state$model$learner$model$pca_explained
+
+#
+bmr_dt$learner[[1]]$state$model$learner$state$model$ranger.ranger$model$prediction.error
+bmr_dt$learner[[1]]$state$model$learner$state$model$xgboost.xgboost
+
diff --git a/postscriptum_light.R b/postscriptum_light.R
index 8039061..774e828 100644
--- a/postscriptum_light.R
+++ b/postscriptum_light.R
@@ -36,7 +36,7 @@ snakeToCamel <- function(snake_str) {
 }
 
 # define backends
-data_tbl = fread("./pead-predictors.csv")
+data_tbl = fread("./pead-predictors-update.csv")
 DT = as.data.table(data_tbl)
 DT[, date_rolling := as.IDate(date_rolling)]
 DT[, yearmonthid := round(date_rolling, digits = "month")]
@@ -123,13 +123,19 @@ if (test) {
 }
 rm(list = c("task_ret_week", "task_ret_month", "task_ret_month2", "task_ret_quarter"))
 
+# measures
+source("Linex.R")
+source("AdjLoss2.R")
+mlr_measures$add("linex", Linex)
+mlr_measures$add("adjloss2", AdjLoss2)
+
 
 # RESULTS -----------------------------------------------------------------
 # utils
 id_cols = c("symbol", "date", "yearmonthid", "..row_id")
 
 # set files with benchmarks
-bmr_files = list.files(list.files("F:", pattern = "^H4-v3", full.names = TRUE), full.names = TRUE)
+bmr_files = list.files(list.files("F:", pattern = "^H4-v4", full.names = TRUE), full.names = TRUE)
 
 # arrange files
 cv_ = as.integer(gsub("\\d+-", "", gsub(".*/|-\\d+.rds", "", bmr_files)))
@@ -139,7 +145,7 @@ setorder(bmr_files, cv, i)
 
 # extract needed information from banchmark objects
 predictions_l = list()
-# imp_features_gausscov_l = list()
+aggs_l = list()
 # imp_features_corr_l = list()
 for (i in 1:nrow(bmr_files)) {
   # debug
@@ -150,6 +156,11 @@ for (i in 1:nrow(bmr_files)) {
   bmr = readRDS(bmr_files$bmr_files[i])
   bmr_dt = as.data.table(bmr)
 
+  # aggregate performances
+  agg_ = bmr$aggregate(msrs(c("regr.mse", "regr.mae", "adjloss2", "linex")))
+  cols = c("task_id", "learner_id", "iters", colnames(agg_)[7:length(colnames(agg_))])
+  agg_ = agg_[, learner_id := gsub(".*regr\\.|\\.tuned", "", learner_id)][, ..cols]
+
   # get predictions
   task_names = lapply(bmr_dt$task, `[[`, "id")
   learner_names = lapply(bmr_dt$learner, `[[`, "id")
@@ -175,11 +186,13 @@ for (i in 1:nrow(bmr_files)) {
 
   # predictions
   predictions_l[[i]] = predictions
+  aggs_l[[i]] = agg_
 }
 
-# important variables
-# sort(table(unlist(imp_features_gausscov_l)))
-# sort(table(unlist(imp_features_corr_l)))
+# aggregated results
+aggregate_results = rbindlist(aggs_l, fill = TRUE)
+cols = colnames(aggregate_results)[4:ncol(aggregate_results)]
+aggregate_results[, lapply(.SD, function(x) mean(x)), by = .(task_id, learner_id), .SDcols = cols]
 
 # hit ratio
 # predictions_dt = rbindlist(lapply(bmrs, function(x) x$predictions), idcol = "fold")
@@ -209,59 +222,94 @@ predictions_dt_ensemble = predictions_dt[, .(mean_response = mean(response),
 predictions_dt_ensemble[, `:=`(
   truth_sign = as.factor(sign(truth)),
   response_sign_median = as.factor(sign(median_response)),
-  response_sign_mean = as.factor(sign(mean_response)),
-  response_sign_sign_pos = sign_response > 10,
-  response_sign_sign_neg = sign_response < -10
+  response_sign_mean = as.factor(sign(mean_response))
+  # response_sign_sign_pos = sign_response > 15,
+  # response_sign_sign_neg = sign_response < -15
 )]
-predictions_dt_ensemble[, response_sign_sd_q := quantile(sd_response, probs = 0.05), by = "task"]
-predictions_dt_ensemble[, mfd := as.factor(ifelse(sd_response < response_sign_sd_q, 1, -1))] # machine forecast dissagreement
-
-ids_ = c("task")
-predictions_dt_ensemble[, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
-predictions_dt_ensemble[, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
-predictions_dt_ensemble[, mlr3measures::acc(truth_sign, mfd), by = ids_]
-predictions_dt_ensemble[response_sign_sign_pos == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
-predictions_dt_ensemble[response_sign_sign_neg == TRUE][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
-
-predictions_dt_ensemble[median_response > 0.1, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
-predictions_dt_ensemble[mean_response > 0.1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
-predictions_dt_ensemble[median_response > 0.5, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
-predictions_dt_ensemble[mean_response > 0.5, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
-predictions_dt_ensemble[median_response > 1, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
-predictions_dt_ensemble[mean_response > 1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
-predictions_dt_ensemble[sd_response > 1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
+sign_response_max = predictions_dt_ensemble[, max(sign_response)]
+sign_response_seq = seq(as.integer(sign_response_max / 2), sign_response_max - 1)
+cols_sign_response_pos = paste0("response_sign_sign_pos", sign_response_seq)
+predictions_dt_ensemble[, (cols_sign_response_pos) := lapply(sign_response_seq, function(x) sign_response > x)]
+
+# check only sign ensamble performance
+lapply(cols_sign_response_pos, function(x) {
+  predictions_dt_ensemble[get(x) == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(get(x)), levels = c(-1, 1))), by = "task"]
+})
+
+
+# predictions_dt_ensemble[, response_sign_sd_q := quantile(sd_response, probs = 0.05), by = "task"]
+# predictions_dt_ensemble[, mfd := as.factor(ifelse(sd_response < response_sign_sd_q, 1, -1))] # machine forecast dissagreement
+#
+# ids_ = c("task")
+# predictions_dt_ensemble[, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
+# predictions_dt_ensemble[, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
+# predictions_dt_ensemble[, mlr3measures::acc(truth_sign, mfd), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_pos == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_neg == TRUE][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
+#
+# predictions_dt_ensemble[median_response > 0.1, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
+# predictions_dt_ensemble[mean_response > 0.1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
+# predictions_dt_ensemble[median_response > 0.5, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
+# predictions_dt_ensemble[mean_response > 0.5, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
+# predictions_dt_ensemble[median_response > 1, mlr3measures::acc(truth_sign, response_sign_median), by = ids_]
+# predictions_dt_ensemble[mean_response > 1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
+# predictions_dt_ensemble[sd_response > 1, mlr3measures::acc(truth_sign, response_sign_mean), by = ids_]
 
 # best
-predictions_dt_ensemble[response_sign_sign_pos == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
-predictions_dt_ensemble[response_sign_sign_pos == TRUE & epsDiff > 0][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
-predictions_dt_ensemble[response_sign_sign_pos == TRUE & epsDiff > 0 & sd_response > 2][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_pos == TRUE][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_pos == TRUE & epsDiff > 0][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_pos == TRUE & epsDiff > 0 & sd_response > 2][, mlr3measures::acc(truth_sign, factor(as.integer(response_sign_sign_pos), levels = c(-1, 1))), by = ids_]
+#
+# predictions_dt_ensemble[response_sign_sign_neg == TRUE][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
+# predictions_dt_ensemble[response_sign_sign_neg == TRUE & epsDiff < 0][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
 
-predictions_dt_ensemble[response_sign_sign_neg == TRUE][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
-predictions_dt_ensemble[response_sign_sign_neg == TRUE & epsDiff < 0][, mlr3measures::acc(truth_sign, factor(-as.integer(response_sign_sign_neg), levels = c(-1, 1))), by = ids_]
 
 # save to azure for QC backtest
 cont = storage_container(BLOBENDPOINT, "qc-backtest")
-predictions_best = predictions_dt_ensemble[response_sign_sign_pos == TRUE]
-lapply(unique(predictions_best$task), function(x) {
+lapply(unique(predictions_dt_ensemble$task), function(x) {
   # debug
   # x = "taskRetQuarter"
 
   # prepare data
-  y = predictions_best[task == x]
-  y = y[, .(symbol, date, response = response_sign_mean, epsDiff)]
+  y = predictions_dt_ensemble[task == x]
+  cols = colnames(y)[grep("response_sign", colnames(y))]
+  cols = c("symbol", "date", "epsDiff", cols)
+  y = y[, ..cols]
   y = unique(y)
-  y = y[, .(
-    symbol = paste0(symbol, collapse = "|"),
-    response = paste0(response, collapse = "|"),
-    epsdiff = paste0(epsDiff, collapse = "|")
-  ), by = date]
-  y[, date := as.character(date)]
+
+  # remove where all false
+  y = y[response_sign_sign_pos9 == TRUE]
+
+  # by date
+  # cols_ = setdiff(cols, "date")
+  # y = y[, lapply(.SD, function(x) paste0(x, collapse = "|")), by = date]
+  # y[, date := as.character(date)]
+  # setorder(y, date)
+
+  # y = y[, .(
+  #   symbol = paste0(symbol, collapse = "|"),
+  #   response = paste0(response, collapse = "|"),
+  #   epsdiff = paste0(epsDiff, collapse = "|"),
+  #
+  # ), by = date]
+
+  # order
   setorder(y, date)
 
   # save to azure blob
-  print(y)
-  file_name_ =  paste0("pead-", x, "-v5.csv")
+  print(colnames(y))
+  file_name_ =  paste0("pead-", x, ".csv")
   storage_write_csv(y, cont, file_name_)
   # universe = y[, .(date, symbol)]
   # storage_write_csv(universe, cont, "pead_task_ret_week_universe.csv", col_names = FALSE)
 })
+
+# systemic risk
+predictors_pos = predictions_dt_ensemble[, .(response_sign_sign_pos_agg = sum(response_sign_sign_pos)), by = "date"]
+predictors_neg = predictions_dt_ensemble[, .(response_sign_sign_pos_agg = sum(response_sign_sign_neg)), by = "date"]
+setorder(predictors_pos, date)
+setorder(predictors_neg, date)
+predictors_diff = as.xts.data.table(predictors_pos) - as.xts.data.table(predictors_neg)
+plot(as.xts.data.table(na.omit(predictors_pos)))
+plot(as.xts.data.table(na.omit(predictors_neg)))
+plot(predictors_diff)
diff --git a/run.R b/run.R
index f7783dc..d60d66d 100644
--- a/run.R
+++ b/run.R
@@ -253,6 +253,92 @@ for (i in seq_along(train_sets)) {
                                     1)
 }
 
+# create expanding window function
+nested_cv_split_expanding = function(task,
+                                     train_length_start = 6,
+                                     tune_length = 1,
+                                     test_length = 1) {
+
+  # create cusom CV's for inner and outer sampling
+  custom_inner = rsmp("custom")
+  custom_outer = rsmp("custom")
+
+  # get year month id data
+  # task = task_ret_week$clone()
+  task_ = task$clone()
+  yearmonthid_ = task_$backend$data(cols = c("yearmonthid", "..row_id"),
+                                    rows = 1:task_$nrow)
+  stopifnot(all(task_$row_ids == yearmonthid_$`..row_id`))
+  groups_v = yearmonthid_[, unlist(unique(yearmonthid))]
+
+  # util vars
+  get_row_ids = function(mid) unlist(yearmonthid_[yearmonthid %in% mid, 2], use.names = FALSE)
+
+  # create train data
+  train_groups = lapply(train_length_start:length(groups_v), function(i) groups_v[1:i])
+
+  # create tune set
+  tune_groups <- lapply((train_length_start+1):length(groups_v), function(i) groups_v[i:(i+tune_length-1)])
+  index_keep = vapply(tune_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  tune_groups = tune_groups[index_keep]
+
+  # equalize train and tune sets
+  train_groups = train_groups[1:length(tune_groups)]
+
+  # create test sets
+  insample_length = vapply(train_groups, function(x) as.integer(length(x) + tune_length), FUN.VALUE = integer(1))
+  test_groups <- lapply((insample_length+1):length(groups_v), function(i) groups_v[i:(i+test_length-1)])
+  index_keep = vapply(test_groups, function(x) !any(is.na(x)), FUN.VALUE = logical(1L))
+  test_groups = test_groups[index_keep]
+
+  # equalize train, tune and test sets
+  train_groups = train_groups[1:length(test_groups)]
+  tune_groups = tune_groups[1:length(test_groups)]
+
+  # make sets
+  train_sets <- lapply(train_groups, get_row_ids)
+  tune_sets <- lapply(tune_groups, get_row_ids)
+  test_sets <- lapply(test_groups, get_row_ids)
+
+  # test tune and test
+  test_1 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(train_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_2 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(tune_sets[[i]], 1) - tail(train_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+  test_3 = vapply(seq_along(train_groups), function(i) {
+    mondf(
+      tail(as.Date(tune_groups[[i]], origin = "1970-01-01"), 1),
+      head(as.Date(test_groups[[i]], origin = "1970-01-01"), 1)
+    )
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_1 == 1))
+  test_4 = vapply(seq_along(train_groups), function(i) {
+    unlist(head(test_sets[[i]], 1) - tail(tune_sets[[i]], 1))
+  }, FUN.VALUE = numeric(1L))
+  stopifnot(all(test_2 == 1))
+
+  # create inner and outer resamplings
+  custom_inner$instantiate(task, train_sets, tune_sets)
+  inner_sets = lapply(seq_along(train_groups), function(i) {
+    c(train_sets[[i]], tune_sets[[i]])
+  })
+  custom_outer$instantiate(task, inner_sets, test_sets)
+  return(list(custom_inner = custom_inner, custom_outer = custom_outer))
+}
+
+# generate cv's for expanding windows
+custom_cvs[[length(custom_cvs)+1]] = nested_cv_split_expanding(task_ret_week,
+                                                               train_length_start = 6,
+                                                               tune_length = 1,
+                                                               test_length = 1)
+
 # test if tain , validation and tst set follow logic
 lapply(seq_along(custom_cvs), function(i) {
   # extract custyom cv
@@ -274,7 +360,6 @@ lapply(seq_along(custom_cvs), function(i) {
 })
 
 
-
 # ADD PIPELINES -----------------------------------------------------------
 print("Add pipelines")
 
@@ -309,6 +394,22 @@ mlr_measures$add("adjloss2", AdjLoss2)
 
 
 # GRAPH V2 ----------------------------------------------------------------
+# TODO: ADD abess
+# graph_template_abess =
+#   po("subsample") %>>% # uncomment this for hyperparameter tuning
+#   po("dropnacol", id = "dropnacol", cutoff = 0.05) %>>%
+#   po("dropna", id = "dropna") %>>%
+#   po("removeconstants", id = "removeconstants_1", ratio = 0)  %>>%
+#   po("fixfactors", id = "fixfactors") %>>%
+#   # po("winsorizesimple", id = "winsorizesimple", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+#   po("winsorizesimplegroup", group_var = "yearmonthid", id = "winsorizesimplegroup", probs_low = 0.01, probs_high = 0.99, na.rm = TRUE) %>>%
+#   po("removeconstants", id = "removeconstants_2", ratio = 0)  %>>%
+#   po("dropcorr", id = "dropcorr", cutoff = 0.99) %>>%
+#   po("uniformization") %>>%
+#   po("dropna", id = "dropna_v2") %>>%
+#   po("learner", learner = lrn("regr.abess"))
+
+
 # graph template
 graph_template =
   po("subsample") %>>% # uncomment this for hyperparameter tuning
@@ -323,8 +424,9 @@ graph_template =
   po("uniformization") %>>%
   po("dropna", id = "dropna_v2") %>>%
   # filters
-  po("branch", options = c("jmi", "gausscov"), id = "filter_branch") %>>%
+  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+              po("filter", filter = flt("relief"), filter.frac = 0.05),
               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
               # po("nop", id = "nop_filter"))) %>>%
   po("unbranch", id = "filter_unbranch") %>>%
@@ -355,7 +457,7 @@ search_space_template = ps(
   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
   # filters
-  filter_branch.selection = p_fct(levels = c("jmi", "gausscov")),
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
   # interaction
   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix"))
 )
@@ -397,7 +499,7 @@ search_space_xgboost = ps(
   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
   # filters
-  filter_branch.selection = p_fct(levels = c("jmi", "gausscov")),
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
   # interaction
   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
   # learner
@@ -503,7 +605,7 @@ search_space_kknn = ps(
   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
   # filters
-  filter_branch.selection = p_fct(levels = c("jmi", "gausscov")),
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
   # interaction
   interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
   # learner
@@ -524,6 +626,42 @@ search_space_nnet$add(
      regr.nnet.maxit = p_int(lower = 50, upper = 500))
 )
 
+# ksvm graph
+graph_ksvm = graph_template %>>%
+  po("learner", learner = lrn("regr.ksvm"), scaled = FALSE)
+graph_ksvm = as_learner(graph_ksvm)
+as.data.table(graph_ksvm$param_set)[, .(id, class, lower, upper, levels)]
+search_space_ksvm = ps(
+  # subsample for hyperband
+  subsample.frac = p_dbl(0.3, 1, tags = "budget"), # unccoment this if we want to use hyperband optimization
+  # preprocessing
+  dropcorr.cutoff = p_fct(
+    levels = c("0.80", "0.90", "0.95", "0.99"),
+    trafo = function(x, param_set) {
+      switch(x,
+             "0.80" = 0.80,
+             "0.90" = 0.90,
+             "0.95" = 0.95,
+             "0.99" = 0.99)
+    }
+  ),
+  # dropcorr.cutoff = p_fct(levels = c(0.8, 0.9, 0.95, 0.99)),
+  winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
+  winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
+  # filters
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov")),
+  # interaction
+  interaction_branch.selection = p_fct(levels = c("nop_interaction", "modelmatrix")),
+  # learner
+  regr.ksvm.kernel  = p_fct(levels = c("rbfdot", "polydot", "vanilladot",
+                                       "tanhdot", "laplacedot", "besseldot",
+                                       "anovadot", "splinedot")),
+  regr.ksvm.C       = p_dbl(lower = 0.0001, upper = 1000, logscale = TRUE),
+  regr.ksvm.degree  = p_int(lower = 1, upper = 5,
+                            depends = regr.ksvm.kernel %in% c("polydot", "besseldot", "anovadot")),
+  regr.ksvm.epsilon = p_dbl(lower = 0.01, upper = 1)
+)
+
 # LAST
 # lightgbm graph
 # [LightGBM] [Fatal] Do not support special JSON characters in feature name.
@@ -540,10 +678,10 @@ graph_template =
   po("uniformization") %>>%
   po("dropna", id = "dropna_v2") %>>%
   # filters
-  po("branch", options = c("jmi", "gausscov"), id = "filter_branch") %>>%
+  po("branch", options = c("jmi", "relief", "gausscov"), id = "filter_branch") %>>%
   gunion(list(po("filter", filter = flt("jmi"), filter.frac = 0.05),
+              po("filter", filter = flt("relief"), filter.frac = 0.05),
               po("filter", filter = flt("gausscov_f1st"), filter.cutoff = 0))) %>>%
-  # po("nop", id = "nop_filter"))) %>>%
   po("unbranch", id = "filter_unbranch")
 search_space_template = ps(
   # subsample for hyperband
@@ -563,7 +701,7 @@ search_space_template = ps(
   winsorizesimplegroup.probs_high = p_fct(levels = c(0.999, 0.99, 0.98, 0.97, 0.90, 0.8)),
   winsorizesimplegroup.probs_low = p_fct(levels = c(0.001, 0.01, 0.02, 0.03, 0.1, 0.2)),
   # filters
-  filter_branch.selection = p_fct(levels = c("jmi", "gausscov"))
+  filter_branch.selection = p_fct(levels = c("jmi", "relief", "gausscov"))
 )
 graph_lightgbm = graph_template %>>%
   po("learner", learner = lrn("regr.lightgbm"))
@@ -592,9 +730,12 @@ search_space_earth$add(
 threads = as.integer(Sys.getenv("NCPUS"))
 set_threads(graph_rf, n = threads)
 set_threads(graph_xgboost, n = threads)
-set_threads(graph_bart, n = threads)
+# set_threads(graph_bart, n = threads)
+set_threads(graph_ksvm, n = threads)
 set_threads(graph_nnet, n = threads)
+set_threads(graph_kknn, n = threads)
 set_threads(graph_lightgbm, n = threads)
+set_threads(graph_earth, n = threads)
 
 
 # NESTED CV BENCHMARK -----------------------------------------------------
@@ -644,6 +785,16 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     terminator = trm("none")
   )
 
+  # auto tuner ksvm
+  at_ksvm = auto_tuner(
+    tuner = tnr("hyperband", eta = 5),
+    learner = graph_ksvm,
+    resampling = custom_,
+    measure = msr("adjloss2"),
+    search_space = search_space_ksvm,
+    terminator = trm("none")
+  )
+
   # auto tuner nnet
   at_nnet = auto_tuner(
     tuner = tnr("hyperband", eta = 5),
@@ -674,7 +825,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
     terminator = trm("none")
   )
 
-  # auto tuner earth
+  # auto tuner kknn
   at_kknn = auto_tuner(
     tuner = tnr("hyperband", eta = 5),
     learner = graph_kknn,
@@ -712,7 +863,7 @@ nested_cv_benchmark <- function(i, cv_inner, cv_outer) {
   print("Benchmark!")
   design = benchmark_grid(
     tasks = list(task_ret_week, task_ret_month, task_ret_month2, task_ret_quarter),
-    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn),
+    learners = list(at_rf, at_xgboost, at_lightgbm, at_nnet, at_earth, at_kknn, at_ksvm),
     resamplings = customo_
   )
   bmr = benchmark(design, store_models = FALSE, store_backends = FALSE)
